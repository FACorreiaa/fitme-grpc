{
  "version": 4,
  "terraform_version": "1.10.2",
  "serial": 11,
  "lineage": "7d7b7845-b892-ff89-8f13-42e627cf219c",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "grafana",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "grafana",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "grafana",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "10.1.5",
                "chart": "grafana",
                "name": "grafana",
                "namespace": "monitoring",
                "revision": 2,
                "values": "{\"adminPassword\":\"deeznuts420\",\"adminUser\":\"admin\",\"datasources\":{\"datasources.yaml\":{\"apiVersion\":1,\"datasources\":[{\"access\":\"proxy\",\"apiVersion\":1,\"basicAuth\":false,\"editable\":false,\"isDefault\":true,\"name\":\"Tempo\",\"orgId\":1,\"type\":\"tempo\",\"uid\":\"tempo\",\"url\":\"http://tempo:3100\",\"version\":1},{\"access\":\"proxy\",\"basicAuth\":false,\"isDefault\":false,\"name\":\"Loki\",\"orgId\":1,\"type\":\"loki\",\"uid\":\"loki\",\"url\":\"http://loki:3100\"},{\"access\":\"proxy\",\"basicAuth\":false,\"isDefault\":false,\"name\":\"Prometheus\",\"orgId\":1,\"type\":\"prometheus\",\"uid\":\"prometheus\",\"url\":\"http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090\"},{\"access\":\"proxy\",\"basicAuth\":false,\"isDefault\":false,\"name\":\"Mimir\",\"orgId\":1,\"type\":\"mimir\",\"uid\":\"mimir\",\"url\":\"http://mimir-nginx.mimir-test.svc:80/api/v1/push\"}]}}}",
                "version": "6.60.6"
              }
            ],
            "name": "grafana",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "---\nadminUser: admin\nadminPassword: deeznuts420\n\ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n\n    datasources:\n      - name: Tempo\n        type: tempo\n        access: proxy\n        orgId: 1\n        url: http://tempo:3100\n        basicAuth: false\n        isDefault: true\n        version: 1\n        editable: false\n        apiVersion: 1\n        uid: tempo\n      - name: Loki\n        type: loki\n        uid: loki\n        access: proxy\n        orgId: 1\n        url: http://loki:3100\n        basicAuth: false\n        isDefault: false\n      - name: Prometheus\n        type: prometheus\n        uid: prometheus\n        access: proxy\n        orgId: 1\n        url: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090\n        basicAuth: false\n        isDefault: false\n      - name: Mimir\n        type: mimir\n        uid: mimir\n        access: proxy\n        orgId: 1\n        url: http://mimir-nginx.mimir-test.svc:80/api/v1/push\n        basicAuth: false\n        isDefault: false\n\n"
            ],
            "verify": false,
            "version": "6.60.6",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "loki",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "loki-stack",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "loki",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v2.9.3",
                "chart": "loki-stack",
                "name": "loki",
                "namespace": "monitoring",
                "revision": 1,
                "values": "{\"filebeat\":{\"enabled\":false,\"filebeatConfig\":{\"filebeat.yml\":\"# logging.level: debug\\nfilebeat.inputs:\\n- type: container\\n  paths:\\n    - /var/log/containers/*.log\\n  processors:\\n  - add_kubernetes_metadata:\\n      host: ${NODE_NAME}\\n      matchers:\\n      - logs_path:\\n          logs_path: \\\"/var/log/containers/\\\"\\noutput.logstash:\\n  hosts: [\\\"logstash-loki:5044\\\"]\\n\"}},\"fluent-bit\":{\"enabled\":false},\"grafana\":{\"enabled\":false,\"image\":{\"tag\":\"10.3.3\"},\"sidecar\":{\"datasources\":{\"enabled\":true,\"label\":\"\",\"labelValue\":\"\",\"maxLines\":1000}}},\"logstash\":{\"enabled\":false,\"filters\":{\"main\":\"filter {\\n  if [kubernetes] {\\n    mutate {\\n      add_field =\\u003e {\\n        \\\"container_name\\\" =\\u003e \\\"%{[kubernetes][container][name]}\\\"\\n        \\\"namespace\\\" =\\u003e \\\"%{[kubernetes][namespace]}\\\"\\n        \\\"pod\\\" =\\u003e \\\"%{[kubernetes][pod][name]}\\\"\\n      }\\n      replace =\\u003e { \\\"host\\\" =\\u003e \\\"%{[kubernetes][node][name]}\\\"}\\n    }\\n  }\\n  mutate {\\n    remove_field =\\u003e [\\\"tags\\\"]\\n  }\\n}\"},\"image\":\"grafana/logstash-output-loki\",\"imageTag\":\"1.0.1\",\"outputs\":{\"main\":\"output {\\n  loki {\\n    url =\\u003e \\\"http://loki:3100/loki/api/v1/push\\\"\\n    #username =\\u003e \\\"test\\\"\\n    #password =\\u003e \\\"test\\\"\\n  }\\n  # stdout { codec =\\u003e rubydebug }\\n}\"}},\"loki\":{\"datasource\":{\"jsonData\":\"{}\",\"uid\":\"\"},\"enabled\":true,\"isDefault\":true,\"livenessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"url\":\"http://loki.monitoring:3100\"},\"prometheus\":{\"datasource\":{\"jsonData\":\"{}\"},\"enabled\":false,\"isDefault\":false,\"url\":\"http://{{ include \\\"prometheus.fullname\\\" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}\"},\"promtail\":{\"config\":{\"clients\":[{\"url\":\"http://{{ .Release.Name }}:3100/loki/api/v1/push\"}],\"logLevel\":\"info\",\"serverPort\":3201},\"enabled\":true},\"proxy\":{\"http_proxy\":\"\",\"https_proxy\":\"\",\"no_proxy\":\"\"},\"test_pod\":{\"enabled\":true,\"image\":\"bats/bats:1.8.2\",\"pullPolicy\":\"IfNotPresent\"}}",
                "version": "2.10.2"
              }
            ],
            "name": "loki",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "test_pod:\n  enabled: true\n  image: bats/bats:1.8.2\n  pullPolicy: IfNotPresent\n\nloki:\n  enabled: true\n  isDefault: true\n  #url: http://{{(include \"loki.serviceName\" .)}}:{{ .Values.loki.service.port }}\n  url: http://loki.monitoring:3100\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  datasource:\n    jsonData: \"{}\"\n    uid: \"\"\n\n\npromtail:\n  enabled: true\n  config:\n    logLevel: info\n    serverPort: 3201\n    clients:\n      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push\n\nfluent-bit:\n  enabled: false\n\ngrafana:\n  enabled: false\n  sidecar:\n    datasources:\n      label: \"\"\n      labelValue: \"\"\n      enabled: true\n      maxLines: 1000\n  image:\n    tag: 10.3.3\n\nprometheus:\n  enabled: false\n  isDefault: false\n  url: http://{{ include \"prometheus.fullname\" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}\n  datasource:\n    jsonData: \"{}\"\n\nfilebeat:\n  enabled: false\n  filebeatConfig:\n    filebeat.yml: |\n      # logging.level: debug\n      filebeat.inputs:\n      - type: container\n        paths:\n          - /var/log/containers/*.log\n        processors:\n        - add_kubernetes_metadata:\n            host: ${NODE_NAME}\n            matchers:\n            - logs_path:\n                logs_path: \"/var/log/containers/\"\n      output.logstash:\n        hosts: [\"logstash-loki:5044\"]\n\nlogstash:\n  enabled: false\n  image: grafana/logstash-output-loki\n  imageTag: 1.0.1\n  filters:\n    main: |-\n      filter {\n        if [kubernetes] {\n          mutate {\n            add_field =\u003e {\n              \"container_name\" =\u003e \"%{[kubernetes][container][name]}\"\n              \"namespace\" =\u003e \"%{[kubernetes][namespace]}\"\n              \"pod\" =\u003e \"%{[kubernetes][pod][name]}\"\n            }\n            replace =\u003e { \"host\" =\u003e \"%{[kubernetes][node][name]}\"}\n          }\n        }\n        mutate {\n          remove_field =\u003e [\"tags\"]\n        }\n      }\n  outputs:\n    main: |-\n      output {\n        loki {\n          url =\u003e \"http://loki:3100/loki/api/v1/push\"\n          #username =\u003e \"test\"\n          #password =\u003e \"test\"\n        }\n        # stdout { codec =\u003e rubydebug }\n      }\n\n# proxy is currently only used by loki test pod\n# Note: If http_proxy/https_proxy are set, then no_proxy should include the\n# loki service name, so that tests are able to communicate with the loki\n# service.\nproxy:\n  http_proxy: \"\"\n  https_proxy: \"\"\n  no_proxy: \"\"\n\n"
            ],
            "verify": false,
            "version": "2.10.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "mimir",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "mimir-distributed",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "mimir",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.14.0",
                "chart": "mimir-distributed",
                "name": "mimir",
                "namespace": "monitoring",
                "revision": 1,
                "values": "{\"admin-cache\":{\"affinity\":{},\"allocatedMemory\":64,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"initContainers\":[],\"maxItemMemory\":1,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"tolerations\":[],\"topologySpreadConstraints\":{}},\"admin_api\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"10m\",\"memory\":\"32Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"alertmanager\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fallbackConfig\":\"receivers:\\n    - name: default-receiver\\nroute:\\n    receiver: default-receiver\\n\",\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableRetentionPolicy\":false,\"enabled\":true,\"size\":\"1Gi\",\"subPath\":\"\",\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"10m\",\"memory\":\"32Mi\"}},\"schedulerName\":\"\",\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"statefulSet\":{\"enabled\":true},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":0,\"maxUnavailable\":1},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":900,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"},\"zoneAwareReplication\":{\"enabled\":false,\"maxUnavailable\":2,\"migration\":{\"enabled\":false,\"writePath\":false},\"topologyKey\":null,\"zones\":[{\"extraAffinity\":{},\"name\":\"zone-a\",\"nodeSelector\":null,\"storageClass\":null},{\"extraAffinity\":{},\"name\":\"zone-b\",\"nodeSelector\":null,\"storageClass\":null},{\"extraAffinity\":{},\"name\":\"zone-c\",\"nodeSelector\":null,\"storageClass\":null}]}},\"chunks-cache\":{\"affinity\":{},\"allocatedMemory\":8192,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"maxItemMemory\":1,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{},\"volumeClaimTemplates\":[]},\"compactor\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableRetentionPolicy\":false,\"enabled\":true,\"size\":\"2Gi\",\"subPath\":\"\",\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"OrderedReady\",\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":60},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"512Mi\"}},\"schedulerName\":\"\",\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":900,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"configStorageType\":\"ConfigMap\",\"continuous_test\":{\"affinity\":{},\"annotations\":{},\"auth\":{\"bearerToken\":null,\"password\":null,\"tenant\":\"mimir-continuous-test\",\"type\":\"tenantId\"},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":false,\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"maxQueryAge\":\"48h\",\"nodeSelector\":{},\"numSeries\":1000,\"priorityClassName\":null,\"read\":null,\"replicas\":1,\"resources\":{\"limits\":{\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"1\",\"memory\":\"512Mi\"}},\"runInterval\":\"5m\",\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":0,\"maxUnavailable\":1},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"write\":null},\"distributor\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":1000,\"kedaAutoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":600,\"type\":\"Percent\",\"value\":10}]}},\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":1,\"preserveReplicas\":false,\"targetCPUUtilizationPercentage\":100,\"targetMemoryUtilizationPercentage\":100},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"512Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":100,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"enterprise\":{\"enabled\":false,\"image\":{\"repository\":\"grafana/enterprise-metrics\",\"tag\":\"v2.14.0\"},\"legacyLabels\":false},\"externalConfigSecretName\":\"{{ include \\\"mimir.resourceName\\\" (dict \\\"ctx\\\" . \\\"component\\\" \\\"config\\\") }}\",\"externalConfigVersion\":\"0\",\"extraObjects\":[],\"fullnameOverride\":null,\"gateway\":{\"affinity\":{},\"annotations\":{},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":70,\"targetMemoryUtilizationPercentage\":70},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabledNonEnterprise\":false,\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[{\"host\":\"{{ .Release.Name }}.mimir.example.com\",\"paths\":[{\"path\":\"/\"}]}],\"ingressClassName\":\"\",\"nameOverride\":\"\",\"tls\":[{\"hosts\":[\"{{ .Release.Name }}.mimir.example.com\"],\"secretName\":\"mimir-tls\"}]},\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"nginx\":{\"basicAuth\":{\"enabled\":false,\"existingSecret\":null,\"htpasswd\":\"{{ htpasswd (required \\\"'gateway.nginx.basicAuth.username' is required\\\" .Values.gateway.nginx.basicAuth.username) (required \\\"'gateway.nginx.basicAuth.password' is required\\\" .Values.gateway.nginx.basicAuth.password) }}\",\"password\":null,\"username\":null},\"config\":{\"accessLogEnabled\":true,\"clientMaxBodySize\":\"540M\",\"enableIPv6\":true,\"errorLogLevel\":\"error\",\"file\":\"worker_processes  5;  ## Default: 1\\nerror_log  /dev/stderr {{ .Values.gateway.nginx.config.errorLogLevel }};\\npid        /tmp/nginx.pid;\\nworker_rlimit_nofile 8192;\\n\\nevents {\\n  worker_connections  4096;  ## Default: 1024\\n}\\n\\nhttp {\\n  client_body_temp_path /tmp/client_temp;\\n  proxy_temp_path       /tmp/proxy_temp_path;\\n  fastcgi_temp_path     /tmp/fastcgi_temp;\\n  uwsgi_temp_path       /tmp/uwsgi_temp;\\n  scgi_temp_path        /tmp/scgi_temp;\\n\\n  default_type application/octet-stream;\\n  log_format   {{ .Values.gateway.nginx.config.logFormat }}\\n\\n  {{- if .Values.gateway.nginx.verboseLogging }}\\n  access_log   /dev/stderr  main;\\n  {{- else }}\\n\\n  map $status $loggable {\\n    ~^[23]  0;\\n    default 1;\\n  }\\n  access_log   {{ .Values.gateway.nginx.config.accessLogEnabled | ternary \\\"/dev/stderr  main  if=$loggable;\\\" \\\"off;\\\" }}\\n  {{- end }}\\n\\n  sendfile           on;\\n  tcp_nopush         on;\\n  proxy_http_version 1.1;\\n\\n  {{- if .Values.gateway.nginx.config.resolver }}\\n  resolver {{ .Values.gateway.nginx.config.resolver }};\\n  {{- else }}\\n  resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\\n  {{- end }}\\n\\n  {{- with .Values.gateway.nginx.config.httpSnippet }}\\n  {{ . | nindent 2 }}\\n  {{- end }}\\n\\n  # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.\\n  map $http_x_scope_orgid $ensured_x_scope_orgid {\\n    default $http_x_scope_orgid;\\n    \\\"\\\" \\\"{{ include \\\"mimir.noAuthTenant\\\" . }}\\\";\\n  }\\n\\n  map $http_x_scope_orgid $has_multiple_orgid_headers {\\n    default 0;\\n    \\\"~^.+,.+$\\\" 1;\\n  }\\n\\n  proxy_read_timeout 300;\\n  server {\\n    listen {{ include \\\"mimir.serverHttpListenPort\\\" . }};\\n    {{- if .Values.gateway.nginx.config.enableIPv6 }}\\n    listen [::]:{{ include \\\"mimir.serverHttpListenPort\\\" . }};\\n    {{- end }}\\n\\n    {{- if .Values.gateway.nginx.config.clientMaxBodySize }}\\n    client_max_body_size {{ .Values.gateway.nginx.config.clientMaxBodySize }};\\n    {{- end }}\\n\\n    {{- if .Values.gateway.nginx.basicAuth.enabled }}\\n    auth_basic           \\\"Mimir\\\";\\n    auth_basic_user_file /etc/nginx/secrets/.htpasswd;\\n    {{- end }}\\n\\n    if ($has_multiple_orgid_headers = 1) {\\n        return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';\\n    }\\n\\n    location = / {\\n      return 200 'OK';\\n      auth_basic off;\\n    }\\n\\n    location = /ready {\\n      return 200 'OK';\\n      auth_basic off;\\n    }\\n\\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\\n\\n    # Distributor endpoints\\n    location /distributor {\\n      set $distributor {{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$distributor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /api/v1/push {\\n      set $distributor {{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$distributor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location /otlp/v1/metrics {\\n      set $distributor {{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$distributor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Alertmanager endpoints\\n    location {{ template \\\"mimir.alertmanagerHttpPrefix\\\" . }} {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /multitenant_alertmanager/status {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /multitenant_alertmanager/configs {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /api/v1/alerts {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Ruler endpoints\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }}/config/v1/rules {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }}/api/v1/rules {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }}/api/v1/alerts {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /ruler/ring {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Rest of {{ template \\\"mimir.prometheusHttpPrefix\\\" . }} goes to the query frontend\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }} {\\n      set $query_frontend {{ template \\\"mimir.fullname\\\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$query_frontend:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Buildinfo endpoint can go to any component\\n    location = /api/v1/status/buildinfo {\\n      set $query_frontend {{ template \\\"mimir.fullname\\\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$query_frontend:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Compactor endpoint for uploading blocks\\n    location /api/v1/upload/block/ {\\n      set $compactor {{ template \\\"mimir.fullname\\\" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$compactor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    {{- with .Values.gateway.nginx.config.serverSnippet }}\\n    {{ . | nindent 4 }}\\n    {{- end }}\\n  }\\n}\\n\",\"httpSnippet\":\"\",\"logFormat\":\"main '$remote_addr - $remote_user [$time_local]  $status '\\n        '\\\"$request\\\" $body_bytes_sent \\\"$http_referer\\\" '\\n        '\\\"$http_user_agent\\\" \\\"$http_x_forwarded_for\\\"';\",\"resolver\":null,\"serverSnippet\":\"\"},\"image\":{\"registry\":\"docker.io\",\"repository\":\"nginxinc/nginx-unprivileged\",\"tag\":\"1.27-alpine\"},\"verboseLogging\":true},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":15,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"route\":{\"annotations\":{},\"enabled\":false,\"host\":\"{{ .Release.Name }}.mimir.example.com\",\"tls\":{\"termination\":\"edge\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"clusterIP\":null,\"labels\":{},\"legacyPort\":8080,\"loadBalancerIP\":null,\"nameOverride\":\"\",\"nodePort\":null,\"port\":80,\"type\":\"ClusterIP\"},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"global\":{\"clusterDomain\":\"cluster.local.\",\"dnsNamespace\":\"kube-system\",\"dnsService\":\"kube-dns\",\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"podAnnotations\":{},\"podLabels\":{}},\"gr-aggr-cache\":{\"affinity\":{},\"allocatedMemory\":8192,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":true,\"extraArgs\":{},\"extraContainers\":[],\"initContainers\":[],\"maxItemMemory\":1,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"tolerations\":[]},\"gr-metricname-cache\":{\"affinity\":{},\"allocatedMemory\":8192,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":true,\"extraArgs\":{},\"extraContainers\":[],\"initContainers\":[],\"maxItemMemory\":1,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"tolerations\":[]},\"grafana-agent-operator\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"graphite\":{\"enabled\":false,\"querier\":{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"labelSelector\":{\"matchExpressions\":[{\"key\":\"app.kubernetes.io/component\",\"operator\":\"In\",\"values\":[\"graphite-querier\"]}]},\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"livenessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":2,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"schemasConfiguration\":{\"storageAggregations\":\"[default]\\naggregationMethod = avg\\npattern = .*\\nxFilesFactor = 0.1\",\"storageSchemas\":\"[default]\\npattern = .*\\nintervals = 0:1s\\nretentions = 10s:8d,10min:1y\"},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":180,\"tolerations\":[]},\"write_proxy\":{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"labelSelector\":{\"matchExpressions\":[{\"key\":\"app.kubernetes.io/component\",\"operator\":\"In\",\"values\":[\"graphite-write-proxy\"]}]},\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"livenessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":2,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":180,\"tolerations\":[]}},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"grafana/mimir\",\"tag\":\"2.14.0\"},\"index-cache\":{\"affinity\":{},\"allocatedMemory\":2048,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"maxItemMemory\":5,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{},\"volumeClaimTemplates\":[]},\"ingester\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":1000,\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableRetentionPolicy\":false,\"enabled\":true,\"size\":\"2Gi\",\"subPath\":\"\",\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":60},\"replicas\":3,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"512Mi\"}},\"schedulerName\":\"\",\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"statefulSet\":{\"enabled\":true},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":1200,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"},\"zoneAwareReplication\":{\"enabled\":true,\"maxUnavailable\":50,\"migration\":{\"enabled\":false,\"excludeDefaultZone\":false,\"readPath\":false,\"replicas\":0,\"scaleDownDefaultZone\":false,\"writePath\":false},\"topologyKey\":null,\"zones\":[{\"extraAffinity\":{},\"name\":\"zone-a\",\"nodeSelector\":null,\"storageClass\":null},{\"extraAffinity\":{},\"name\":\"zone-b\",\"nodeSelector\":null,\"storageClass\":null},{\"extraAffinity\":{},\"name\":\"zone-c\",\"nodeSelector\":null,\"storageClass\":null}]}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[\"mimir.example.com\"],\"paths\":{\"alertmanager-headless\":[{\"path\":\"/alertmanager\"},{\"path\":\"/multitenant_alertmanager/status\"},{\"path\":\"/multitenant_alertmanager/configs\"},{\"path\":\"/api/v1/alerts\"}],\"compactor\":[{\"path\":\"/api/v1/upload/block/\"}],\"distributor-headless\":[{\"path\":\"/distributor\"},{\"path\":\"/api/v1/push\"},{\"path\":\"/otlp/v1/metrics\"}],\"query-frontend\":[{\"path\":\"/prometheus\"},{\"path\":\"/api/v1/status/buildinfo\"}],\"ruler\":[{\"path\":\"/prometheus/config/v1/rules\"},{\"path\":\"/prometheus/api/v1/rules\"},{\"path\":\"/prometheus/api/v1/alerts\"}]}},\"kedaAutoscaling\":{\"customHeaders\":{},\"pollingInterval\":10,\"prometheusAddress\":\"\"},\"kubeVersionOverride\":null,\"license\":{\"contents\":\"NOTAVALIDLICENSE\",\"external\":false,\"secretName\":\"{{ include \\\"mimir.resourceName\\\" (dict \\\"ctx\\\" . \\\"component\\\" \\\"license\\\") }}\"},\"memcached\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"memcached\",\"tag\":\"1.6.31-alpine\"},\"podSecurityContext\":{},\"priorityClassName\":null},\"memcachedExporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"extraArgs\":{},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"prom/memcached-exporter\",\"tag\":\"v0.14.4\"},\"resources\":{\"limits\":{},\"requests\":{}}},\"metaMonitoring\":{\"dashboards\":{\"annotations\":{\"k8s-sidecar-target-directory\":\"/tmp/dashboards/Mimir Dashboards\"},\"enabled\":false,\"labels\":{\"grafana_dashboard\":\"1\"},\"namespace\":null},\"grafanaAgent\":{\"annotations\":{},\"containerSecurityContext\":null,\"enabled\":true,\"imageRepo\":null,\"installOperator\":true,\"labels\":{},\"logs\":{\"additionalClientConfigs\":[],\"clusterLabel\":\"\",\"enabled\":true,\"remote\":{\"auth\":{\"passwordSecretKey\":\"\",\"passwordSecretName\":\"\",\"tenantId\":\"\",\"username\":\"\"},\"url\":\"\"}},\"metrics\":{\"additionalRemoteWriteConfigs\":[{\"url\":\"http://mimir-nginx.mimir-test.svc:80/api/v1/push\"}],\"enabled\":true,\"remote\":{\"auth\":{\"passwordSecretKey\":\"\",\"passwordSecretName\":\"\",\"username\":\"\"},\"headers\":{},\"sigv4\":{},\"url\":\"\"},\"scrapeInterval\":\"60s\",\"scrapeK8s\":{\"enabled\":true,\"kubeStateMetrics\":{\"labelSelectors\":{\"app.kubernetes.io/name\":\"kube-state-metrics\"},\"namespace\":\"kube-system\",\"service\":{\"port\":\"http-metrics\"}}}},\"namespace\":\"\",\"nodeSelector\":{},\"podSecurityContext\":null,\"resources\":null,\"tolerations\":[],\"topologySpreadConstraints\":[{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}]},\"prometheusRule\":{\"annotations\":{},\"enabled\":false,\"groups\":[],\"labels\":{},\"mimirAlerts\":false,\"mimirRules\":false,\"namespace\":null},\"serviceMonitor\":{\"annotations\":{},\"clusterLabel\":\"\",\"enabled\":true,\"interval\":null,\"labels\":{},\"metricRelabelings\":[],\"namespace\":null,\"namespaceSelector\":null,\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":null,\"tlsConfig\":null}},\"metadata-cache\":{\"affinity\":{},\"allocatedMemory\":512,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"maxItemMemory\":1,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{},\"volumeClaimTemplates\":[]},\"mimir\":{\"config\":\"usage_stats:\\n  installation_mode: helm\\n\\nactivity_tracker:\\n  filepath: /active-query-tracker/activity.log\\n\\n{{- if .Values.enterprise.enabled }}\\nadmin_api:\\n  leader_election:\\n    enabled: true\\n    ring:\\n      kvstore:\\n        store: \\\"memberlist\\\"\\n\\nadmin_client:\\n  storage:\\n  {{- if .Values.minio.enabled }}\\n    type: s3\\n    s3:\\n      access_key_id: {{ .Values.minio.rootUser }}\\n      bucket_name: enterprise-metrics-admin\\n      endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\\n      insecure: true\\n      secret_access_key: {{ .Values.minio.rootPassword }}\\n  {{- end }}\\n  {{- if (index .Values \\\"admin-cache\\\" \\\"enabled\\\") }}\\n    cache:\\n      backend: memcached\\n      memcached:\\n        addresses: {{ include \\\"mimir.adminCacheAddress\\\" . }}\\n        max_item_size: {{ mul (index .Values \\\"admin-cache\\\").maxItemMemory 1024 1024 }}\\n  {{- end }}\\n{{- end }}\\n\\nalertmanager:\\n  data_dir: /data\\n  enable_api: true\\n  external_url: /alertmanager\\n  {{- if .Values.alertmanager.zoneAwareReplication.enabled }}\\n  sharding_ring:\\n    zone_awareness_enabled: true\\n  {{- end }}\\n  {{- if .Values.alertmanager.fallbackConfig }}\\n  fallback_config_file: /configs/alertmanager_fallback_config.yaml\\n  {{- end }}\\n\\n{{- if .Values.minio.enabled }}\\nalertmanager_storage:\\n  backend: s3\\n  s3:\\n    access_key_id: {{ .Values.minio.rootUser }}\\n    bucket_name: {{ include \\\"mimir.minioBucketPrefix\\\" . }}-ruler\\n    endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\\n    insecure: true\\n    secret_access_key: {{ .Values.minio.rootPassword }}\\n{{- end }}\\n\\n{{- if .Values.enterprise.enabled }}\\nauth:\\n  type: enterprise\\n  admin:\\n    pass_access_policy_name: true\\n    pass_token_name: true\\n{{- end }}\\n\\n# This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.\\nblocks_storage:\\n  backend: s3\\n  bucket_store:\\n    {{- if index .Values \\\"chunks-cache\\\" \\\"enabled\\\" }}\\n    chunks_cache:\\n      backend: memcached\\n      memcached:\\n        addresses: {{ include \\\"mimir.chunksCacheAddress\\\" . }}\\n        max_item_size: {{ mul (index .Values \\\"chunks-cache\\\").maxItemMemory 1024 1024 }}\\n        timeout: 750ms\\n        max_idle_connections: 150\\n    {{- end }}\\n    {{- if index .Values \\\"index-cache\\\" \\\"enabled\\\" }}\\n    index_cache:\\n      backend: memcached\\n      memcached:\\n        addresses: {{ include \\\"mimir.indexCacheAddress\\\" . }}\\n        max_item_size: {{ mul (index .Values \\\"index-cache\\\").maxItemMemory 1024 1024 }}\\n        timeout: 750ms\\n        max_idle_connections: 150\\n    {{- end }}\\n    {{- if index .Values \\\"metadata-cache\\\" \\\"enabled\\\" }}\\n    metadata_cache:\\n      backend: memcached\\n      memcached:\\n        addresses: {{ include \\\"mimir.metadataCacheAddress\\\" . }}\\n        max_item_size: {{ mul (index .Values \\\"metadata-cache\\\").maxItemMemory 1024 1024 }}\\n        max_idle_connections: 150\\n    {{- end }}\\n    sync_dir: /data/tsdb-sync\\n  {{- if .Values.minio.enabled }}\\n  s3:\\n    access_key_id: {{ .Values.minio.rootUser }}\\n    bucket_name: {{ include \\\"mimir.minioBucketPrefix\\\" . }}-tsdb\\n    endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\\n    insecure: true\\n    secret_access_key: {{ .Values.minio.rootPassword }}\\n  {{- end }}\\n  tsdb:\\n    dir: /data/tsdb\\n    head_compaction_interval: 15m\\n    wal_replay_concurrency: 3\\n\\n{{- if .Values.enterprise.enabled }}\\ncluster_name: \\\"{{ .Release.Name }}\\\"\\n{{- end }}\\n\\ncompactor:\\n  compaction_interval: 30m\\n  deletion_delay: 2h\\n  max_closing_blocks_concurrency: 2\\n  max_opening_blocks_concurrency: 4\\n  symbols_flushers_concurrency: 4\\n  first_level_compaction_wait_period: 25m\\n  data_dir: \\\"/data\\\"\\n  sharding_ring:\\n    wait_stability_min_duration: 1m\\n    heartbeat_period: 1m\\n    heartbeat_timeout: 4m\\n\\ndistributor:\\n  ring:\\n    heartbeat_period: 1m\\n    heartbeat_timeout: 4m\\n\\nfrontend:\\n  parallelize_shardable_queries: true\\n  {{- if index .Values \\\"results-cache\\\" \\\"enabled\\\" }}\\n  results_cache:\\n    backend: memcached\\n    memcached:\\n      timeout: 500ms\\n      addresses: {{ include \\\"mimir.resultsCacheAddress\\\" . }}\\n      max_item_size: {{ mul (index .Values \\\"results-cache\\\").maxItemMemory 1024 1024 }}\\n  cache_results: true\\n  query_sharding_target_series_per_shard: 2500\\n  {{- end }}\\n  {{- if .Values.query_scheduler.enabled }}\\n  scheduler_address: {{ template \\\"mimir.fullname\\\" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverGrpcListenPort\\\" . }}\\n  {{- end }}\\n  {{- if .Values.enterprise.enabled }}\\n  log_query_request_headers: X-Access-Policy-Name,X-Token-Name\\n  {{- end }}\\n\\nfrontend_worker:\\n  grpc_client_config:\\n    max_send_msg_size: 419430400 # 400MiB\\n  {{- if .Values.query_scheduler.enabled }}\\n  scheduler_address: {{ template \\\"mimir.fullname\\\" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverGrpcListenPort\\\" . }}\\n  {{- else }}\\n  frontend_address: {{ template \\\"mimir.fullname\\\" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverGrpcListenPort\\\" . }}\\n  {{- end }}\\n\\n{{- if and .Values.enterprise.enabled }}\\ngateway:\\n  proxy:\\n    admin_api:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    alertmanager:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    compactor:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    default:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    distributor:\\n      url: dns:///{{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include \\\"mimir.serverGrpcListenPort\\\" . }}\\n    ingester:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-ingester-headless.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    query_frontend:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    ruler:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    store_gateway:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-store-gateway-headless.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}\\n    graphite_write_proxy:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-graphite-write-proxy.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    graphite_querier:\\n      url: http://{{ template \\\"mimir.fullname\\\" . }}-graphite-querier.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" . }}\\n    {{- end}}\\n{{- end }}\\n\\ningester:\\n  ring:\\n    final_sleep: 0s\\n    num_tokens: 512\\n    tokens_file_path: /data/tokens\\n    unregister_on_shutdown: false\\n    heartbeat_period: 2m\\n    heartbeat_timeout: 10m\\n    {{- if .Values.ingester.zoneAwareReplication.enabled }}\\n    zone_awareness_enabled: true\\n    {{- end }}\\n\\ningester_client:\\n  grpc_client_config:\\n    max_recv_msg_size: 104857600\\n    max_send_msg_size: 104857600\\n\\n{{- if .Values.enterprise.enabled }}\\ninstrumentation:\\n  enabled: true\\n  distributor_client:\\n    address: dns:///{{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include \\\"mimir.serverGrpcListenPort\\\" . }}\\n\\nlicense:\\n  path: \\\"/license/license.jwt\\\"\\n{{- end }}\\n\\nlimits:\\n  # Limit queries to 500 days. You can override this on a per-tenant basis.\\n  max_total_query_length: 12000h\\n  # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.\\n  # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.\\n  max_query_parallelism: 240\\n  # Avoid caching results newer than 10m because some samples can be delayed\\n  # This presents caching incomplete results\\n  max_cache_freshness: 10m\\n\\nmemberlist:\\n  abort_if_cluster_join_fails: false\\n  compression_enabled: false\\n  join_members:\\n  - dns+{{ include \\\"mimir.fullname\\\" . }}-gossip-ring.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include \\\"mimir.memberlistBindPort\\\" . }}\\n\\nquerier:\\n  # With query sharding we run more but smaller queries. We must strike a balance\\n  # which allows us to process more sharded queries in parallel when requested, but not overload\\n  # queriers during non-sharded queries.\\n  max_concurrent: 16\\n\\nquery_scheduler:\\n  # Increase from default of 100 to account for queries created by query sharding\\n  max_outstanding_requests_per_tenant: 800\\n\\nruler:\\n  alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager\\n  enable_api: true\\n  rule_path: /data\\n  {{- if .Values.ruler.remoteEvaluationDedicatedQueryPath }}\\n  query_frontend:\\n    address: dns:///{{ template \\\"mimir.fullname\\\" . }}-ruler-query-frontend.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverGrpcListenPort\\\" .  }}\\n  {{- end }}\\n\\n{{- if or (.Values.minio.enabled) (index .Values \\\"metadata-cache\\\" \\\"enabled\\\") }}\\nruler_storage:\\n  {{- if .Values.minio.enabled }}\\n  backend: s3\\n  s3:\\n    endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\\n    bucket_name: {{ include \\\"mimir.minioBucketPrefix\\\" . }}-ruler\\n    access_key_id: {{ .Values.minio.rootUser }}\\n    secret_access_key: {{ .Values.minio.rootPassword }}\\n    insecure: true\\n  {{- end }}\\n  {{- if index .Values \\\"metadata-cache\\\" \\\"enabled\\\" }}\\n  cache:\\n    backend: memcached\\n    memcached:\\n      addresses: {{ include \\\"mimir.metadataCacheAddress\\\" . }}\\n      max_item_size: {{ mul (index .Values \\\"metadata-cache\\\").maxItemMemory 1024 1024 }}\\n  {{- end }}\\n{{- end }}\\n\\nruntime_config:\\n  file: /var/{{ include \\\"mimir.name\\\" . }}/runtime.yaml\\n\\nstore_gateway:\\n  sharding_ring:\\n    heartbeat_period: 1m\\n    heartbeat_timeout: 4m\\n    wait_stability_min_duration: 1m\\n    {{- if .Values.store_gateway.zoneAwareReplication.enabled }}\\n    kvstore:\\n      prefix: multi-zone/\\n    {{- end }}\\n    tokens_file_path: /data/tokens\\n    unregister_on_shutdown: false\\n    {{- if .Values.store_gateway.zoneAwareReplication.enabled }}\\n    zone_awareness_enabled: true\\n    {{- end }}\\n\\n{{- if and .Values.enterprise.enabled .Values.graphite.enabled }}\\ngraphite:\\n  enabled: true\\n\\n  write_proxy:\\n    distributor_client:\\n      address: dns:///{{ template \\\"mimir.fullname\\\" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverGrpcListenPort\\\" .  }}\\n\\n  querier:\\n    remote_read:\\n      query_address: http://{{ template \\\"mimir.fullname\\\" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include \\\"mimir.serverHttpListenPort\\\" .  }}/prometheus\\n    proxy_bad_requests: false\\n\\n    schemas:\\n      default_storage_schemas_file: /etc/graphite-proxy/storage-schemas.conf\\n      default_storage_aggregations_file: /etc/graphite-proxy/storage-aggregations.conf\\n    aggregation_cache:\\n      memcached:\\n        addresses: dnssrvnoa+{{ template \\\"mimir.fullname\\\" . }}-gr-aggr-cache.{{ .Release.Namespace}}.svc:11211\\n        timeout: 1s\\n    metric_name_cache:\\n      memcached:\\n        addresses: dnssrvnoa+{{ template \\\"mimir.fullname\\\" . }}-gr-metricname-cache.{{ .Release.Namespace}}.svc:11211\\n        timeout: 1s\\n{{- end}}\\n\",\"structuredConfig\":{}},\"minio\":{\"buckets\":[{\"name\":\"mimir-tsdb\",\"policy\":\"none\",\"purge\":false},{\"name\":\"mimir-ruler\",\"policy\":\"none\",\"purge\":false},{\"name\":\"enterprise-metrics-tsdb\",\"policy\":\"none\",\"purge\":false},{\"name\":\"enterprise-metrics-admin\",\"policy\":\"none\",\"purge\":false},{\"name\":\"enterprise-metrics-ruler\",\"policy\":\"none\",\"purge\":false}],\"configPathmc\":\"/tmp/minio/mc/\",\"enabled\":true,\"mode\":\"standalone\",\"persistence\":{\"size\":\"5Gi\"},\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"rootPassword\":\"supersecret\",\"rootUser\":\"grafana-mimir\"},\"nameOverride\":null,\"nginx\":{\"affinity\":\"\",\"annotations\":{},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"basicAuth\":{\"enabled\":false,\"existingSecret\":null,\"htpasswd\":\"{{ htpasswd (required \\\"'nginx.basicAuth.username' is required\\\" .Values.nginx.basicAuth.username) (required \\\"'nginx.basicAuth.password' is required\\\" .Values.nginx.basicAuth.password) }}\",\"password\":null,\"username\":null},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"deploymentStrategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"enabled\":true,\"extraArgs\":{},\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"nginxinc/nginx-unprivileged\",\"tag\":\"1.27-alpine\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[{\"host\":\"nginx.mimir.example.com\",\"paths\":[{\"path\":\"/\"}]}],\"tls\":[{\"hosts\":[\"nginx.mimir.example.com\"],\"secretName\":\"mimir-nginx-tls\"}]},\"nginxConfig\":{\"accessLogEnabled\":true,\"errorLogLevel\":\"error\",\"file\":\"worker_processes  5;  ## Default: 1\\nerror_log  /dev/stderr {{ .Values.nginx.nginxConfig.errorLogLevel }};\\npid        /tmp/nginx.pid;\\nworker_rlimit_nofile 8192;\\n\\nevents {\\n  worker_connections  4096;  ## Default: 1024\\n}\\n\\nhttp {\\n  client_body_temp_path /tmp/client_temp;\\n  proxy_temp_path       /tmp/proxy_temp_path;\\n  fastcgi_temp_path     /tmp/fastcgi_temp;\\n  uwsgi_temp_path       /tmp/uwsgi_temp;\\n  scgi_temp_path        /tmp/scgi_temp;\\n\\n  default_type application/octet-stream;\\n  log_format   {{ .Values.nginx.nginxConfig.logFormat }}\\n\\n  {{- if .Values.nginx.verboseLogging }}\\n  access_log   /dev/stderr  main;\\n  {{- else }}\\n\\n  map $status $loggable {\\n    ~^[23]  0;\\n    default 1;\\n  }\\n  access_log   {{ .Values.nginx.nginxConfig.accessLogEnabled | ternary \\\"/dev/stderr  main  if=$loggable;\\\" \\\"off;\\\" }}\\n  {{- end }}\\n\\n  sendfile           on;\\n  tcp_nopush         on;\\n  proxy_http_version 1.1;\\n\\n  {{- if .Values.nginx.nginxConfig.resolver }}\\n  resolver {{ .Values.nginx.nginxConfig.resolver }};\\n  {{- else }}\\n  resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\\n  {{- end }}\\n\\n  {{- with .Values.nginx.nginxConfig.httpSnippet }}\\n  {{ . | nindent 2 }}\\n  {{- end }}\\n\\n  # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.\\n  map $http_x_scope_orgid $ensured_x_scope_orgid {\\n    default $http_x_scope_orgid;\\n    \\\"\\\" \\\"{{ include \\\"mimir.noAuthTenant\\\" . }}\\\";\\n  }\\n\\n  map $http_x_scope_orgid $has_multiple_orgid_headers {\\n    default 0;\\n    \\\"~^.+,.+$\\\" 1;\\n  }\\n\\n  proxy_read_timeout 300;\\n  server {\\n    listen 8080;\\n    listen [::]:8080;\\n\\n    {{- if .Values.nginx.basicAuth.enabled }}\\n    auth_basic           \\\"Mimir\\\";\\n    auth_basic_user_file /etc/nginx/secrets/.htpasswd;\\n    {{- end }}\\n\\n    if ($has_multiple_orgid_headers = 1) {\\n        return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';\\n    }\\n\\n    location = / {\\n      return 200 'OK';\\n      auth_basic off;\\n    }\\n\\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\\n\\n    # Distributor endpoints\\n    location /distributor {\\n      set $distributor {{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$distributor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /api/v1/push {\\n      set $distributor {{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$distributor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location /otlp/v1/metrics {\\n      set $distributor {{ template \\\"mimir.fullname\\\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$distributor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Alertmanager endpoints\\n    location {{ template \\\"mimir.alertmanagerHttpPrefix\\\" . }} {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /multitenant_alertmanager/status {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /multitenant_alertmanager/configs {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /api/v1/alerts {\\n      set $alertmanager {{ template \\\"mimir.fullname\\\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$alertmanager:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Ruler endpoints\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }}/config/v1/rules {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }}/api/v1/rules {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }}/api/v1/alerts {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n    location = /ruler/ring {\\n      set $ruler {{ template \\\"mimir.fullname\\\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$ruler:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Rest of {{ template \\\"mimir.prometheusHttpPrefix\\\" . }} goes to the query frontend\\n    location {{ template \\\"mimir.prometheusHttpPrefix\\\" . }} {\\n      set $query_frontend {{ template \\\"mimir.fullname\\\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$query_frontend:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Buildinfo endpoint can go to any component\\n    location = /api/v1/status/buildinfo {\\n      set $query_frontend {{ template \\\"mimir.fullname\\\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$query_frontend:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    # Compactor endpoint for uploading blocks\\n    location /api/v1/upload/block/ {\\n      set $compactor {{ template \\\"mimir.fullname\\\" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\\n      proxy_pass      http://$compactor:{{ include \\\"mimir.serverHttpListenPort\\\" . }}$request_uri;\\n    }\\n\\n    {{- with .Values.nginx.nginxConfig.serverSnippet }}\\n    {{ . | nindent 4 }}\\n    {{- end }}\\n  }\\n}\\n\",\"httpSnippet\":\"\",\"logFormat\":\"main '$remote_addr - $remote_user [$time_local]  $status '\\n        '\\\"$request\\\" $body_bytes_sent \\\"$http_referer\\\" '\\n        '\\\"$http_user_agent\\\" \\\"$http_x_forwarded_for\\\"';\",\"resolver\":null,\"serverSnippet\":\"\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podSecurityContext\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/\",\"port\":\"http-metric\"},\"initialDelaySeconds\":15,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"route\":{\"annotations\":{},\"enabled\":false,\"host\":\"nginx.mimir.example.com\",\"tls\":{\"termination\":\"edge\"}},\"service\":{\"annotations\":{},\"clusterIP\":null,\"labels\":{},\"loadBalancerIP\":null,\"nodePort\":null,\"port\":80,\"type\":\"ClusterIP\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"},\"verboseLogging\":true},\"overrides_exporter\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"livenessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{}},\"querier\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":5000,\"kedaAutoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":120,\"type\":\"Percent\",\"value\":10}],\"stabilizationWindowSeconds\":600},\"scaleUp\":{\"policies\":[{\"periodSeconds\":120,\"type\":\"Percent\",\"value\":50},{\"periodSeconds\":120,\"type\":\"Pods\",\"value\":15}],\"stabilizationWindowSeconds\":60}},\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":1,\"predictiveScalingEnabled\":false,\"predictiveScalingLookback\":\"30m\",\"predictiveScalingPeriod\":\"6d23h30m\",\"preserveReplicas\":false,\"querySchedulerInflightRequestsThreshold\":12},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":2,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":180,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"query_frontend\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":5000,\"kedaAutoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":60,\"type\":\"Percent\",\"value\":10}]}},\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":1,\"preserveReplicas\":false,\"targetCPUUtilizationPercentage\":75,\"targetMemoryUtilizationPercentage\":100},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":390,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"query_scheduler\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":2,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":1,\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":180,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"rbac\":{\"create\":true,\"forcePSPOnKubernetes124\":false,\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"podSecurityPolicy\":{\"additionalVolumes\":[],\"allowPrivilegeEscalation\":false,\"fsGroup\":{\"ranges\":[{\"max\":65535,\"min\":1}],\"rule\":\"MustRunAs\"},\"hostIPC\":false,\"hostNetwork\":false,\"hostPID\":false,\"privileged\":false,\"readOnlyRootFilesystem\":true,\"runAsUser\":{\"rule\":\"MustRunAsNonRoot\"},\"seLinux\":{\"rule\":\"RunAsAny\"},\"seccompProfile\":\"runtime/default\",\"supplementalGroups\":{\"ranges\":[{\"max\":65535,\"min\":1}],\"rule\":\"MustRunAs\"}},\"type\":\"psp\"},\"results-cache\":{\"affinity\":{},\"allocatedMemory\":512,\"annotations\":{},\"connectionLimit\":16384,\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"maxItemMemory\":5,\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":{},\"volumeClaimTemplates\":[]},\"rollout_operator\":{\"enabled\":true,\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"ruler\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":1000,\"kedaAutoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":600,\"type\":\"Percent\",\"value\":10}]}},\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":1,\"preserveReplicas\":false,\"targetCPUUtilizationPercentage\":100,\"targetMemoryUtilizationPercentage\":100},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"remoteEvaluationDedicatedQueryPath\":false,\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"serviceAccount\":{\"annotations\":{},\"create\":false,\"labels\":{},\"name\":\"\"},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"50%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":600,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"ruler_querier\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":5000,\"kedaAutoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":120,\"type\":\"Percent\",\"value\":10}],\"stabilizationWindowSeconds\":600},\"scaleUp\":{\"policies\":[{\"periodSeconds\":120,\"type\":\"Percent\",\"value\":50},{\"periodSeconds\":120,\"type\":\"Pods\",\"value\":15}],\"stabilizationWindowSeconds\":60}},\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":1,\"preserveReplicas\":false,\"querySchedulerInflightRequestsThreshold\":12},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":2,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":180,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"ruler_query_frontend\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":5000,\"kedaAutoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":60,\"type\":\"Percent\",\"value\":10}]}},\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":1,\"preserveReplicas\":false,\"targetCPUUtilizationPercentage\":75,\"targetMemoryUtilizationPercentage\":100},\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"15%\",\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":390,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"ruler_query_scheduler\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":null,\"nodeSelector\":{},\"persistence\":{\"subPath\":null},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":2,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":1,\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":180,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}},\"runtimeConfig\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"labels\":{},\"name\":null},\"smoke_test\":{\"annotations\":{},\"backoffLimit\":5,\"env\":[],\"extraArgs\":{},\"extraEnvFrom\":[],\"initContainers\":[],\"priorityClassName\":null,\"tenantId\":\"\"},\"store_gateway\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"jaegerReporterMaxQueueSize\":1000,\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableRetentionPolicy\":false,\"enabled\":true,\"size\":\"2Gi\",\"subPath\":\"\",\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"maxUnavailable\":1},\"podLabels\":{},\"podManagementPolicy\":\"OrderedReady\",\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":60},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"512Mi\"}},\"schedulerName\":\"\",\"securityContext\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"strategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":120,\"tolerations\":[],\"topologySpreadConstraints\":{\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"},\"zoneAwareReplication\":{\"enabled\":true,\"maxUnavailable\":50,\"migration\":{\"enabled\":false,\"readPath\":false},\"topologyKey\":null,\"zones\":[{\"extraAffinity\":{},\"name\":\"zone-a\",\"nodeSelector\":null,\"storageClass\":null},{\"extraAffinity\":{},\"name\":\"zone-b\",\"nodeSelector\":null,\"storageClass\":null},{\"extraAffinity\":{},\"name\":\"zone-c\",\"nodeSelector\":null,\"storageClass\":null}]}},\"tokengenJob\":{\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enable\":true,\"env\":[],\"extraArgs\":{},\"extraEnvFrom\":[],\"initContainers\":[],\"priorityClassName\":null,\"securityContext\":{}},\"useExternalConfig\":false,\"vaultAgent\":{\"caCertPath\":\"\",\"clientCertPath\":\"\",\"clientKeyPath\":\"\",\"enabled\":false,\"roleName\":\"\",\"serverCertPath\":\"\",\"serverKeyPath\":\"\"}}",
                "version": "5.5.1"
              }
            ],
            "name": "mimir",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Grafana Enterprise Metrics specific values and features are found after the `enterprise:` key\n#\n# The default values specified in this file are enough to deploy all of the\n# Grafana Mimir or Grafana Enterprise Metrics microservices but are not suitable for\n# production load.\n# To configure the resources for production load, refer to the small.yaml or\n# large.yaml values files.\n\n# Note: The values in this file are not backward compatible. Copying and pasting the values is discouraged, but if you do so,\n# make sure to use the values.yaml from the branch or tag that matches the mimir-distributed Helm chart version that you\n# want to install. You also can see values.yaml for a specific version by running\n# `helm show values grafana/mimir-distributed --version \u003cversion\u003e`\n\n# If you want to get the values file from Github, build the URL as follows because we git tag every release:\n# `https://github.com/grafana/mimir/blob/mimir-distributed-\u003cchart-version\u003e/operations/helm/charts/mimir-distributed/values.yaml`.\n# For example, https://github.com/grafana/mimir/blob/mimir-distributed-3.1.0/operations/helm/charts/mimir-distributed/values.yaml.\n\n# -- Overrides the version used to determine compatibility of resources with the target Kubernetes cluster.\n# This is useful when using `helm template`, because then helm will use the client version of kubectl as the Kubernetes version,\n# which may or may not match your cluster's server version. Example: 'v1.24.4'. Set to null to use the version that helm\n# devises.\nkubeVersionOverride: null\n\n# -- Overrides the chart's name. Used to change mimir/enterprise-metrics infix in the resource names. E.g. myRelease-mimir-ingester-1 to myRelease-nameOverride-ingester-1.\n# This option is used to align resource names with Cortex, when doing a migration from Cortex to Grafana Mimir.\n# Note: Grafana provided dashboards rely on the default naming and will need changes.\nnameOverride: null\n\n# -- Overrides the chart's computed fullname. Used to change the full prefix of resource names. E.g. myRelease-mimir-ingester-1 to fullnameOverride-ingester-1.\n# Note: Grafana provided dashboards rely on the default naming and will need changes.\nfullnameOverride: null\n\nimage:\n  # -- Grafana Mimir container image repository. Note: for Grafana Enterprise Metrics use the value 'enterprise.image.repository'\n  repository: grafana/mimir\n  # -- Grafana Mimir container image tag. Note: for Grafana Enterprise Metrics use the value 'enterprise.image.tag'\n  tag: 2.14.0\n  # -- Container pull policy - shared between Grafana Mimir and Grafana Enterprise Metrics\n  pullPolicy: IfNotPresent\n  # -- Optionally specify an array of imagePullSecrets - shared between Grafana Mimir and Grafana Enterprise Metrics\n  # Secrets must be manually created in the namespace.\n  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  # pullSecrets:\n  #   - myRegistryKeySecretName\n\nglobal:\n  # -- Definitions to set up nginx resolver\n  dnsService: kube-dns\n  dnsNamespace: kube-system\n  clusterDomain: cluster.local.\n\n  # -- Common environment variables to add to all pods directly managed by this chart.\n  # scope: admin-api, alertmanager, compactor, continuous-test, distributor, gateway, graphite-proxy, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, smoke-test, tokengen\n  extraEnv: []\n\n  # -- Common source of environment injections to add to all pods directly managed by this chart.\n  # scope: admin-api, alertmanager, compactor, continuous-test, distributor, gateway, graphite-proxy, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, smoke-test, tokengen\n  # For example to inject values from a Secret, use:\n  # extraEnvFrom:\n  #   - secretRef:\n  #       name: mysecret\n  extraEnvFrom: []\n\n  # -- Common volumes to add to all pods directly managed by this chart.\n  # scope: admin-api, alertmanager, compactor, continuous-test, distributor, gateway, graphite-proxy, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, smoke-test, tokengen\n  extraVolumes: []\n\n  # -- Common mount points to add to all pods directly managed by this chart.\n  # scope: admin-api, alertmanager, compactor, continuous-test, distributor, gateway, graphite-proxy, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, smoke-test, tokengen\n  extraVolumeMounts: []\n\n  # -- Pod annotations for all pods directly managed by this chart. Usable for example to associate a version to 'global.extraEnv' and 'global.extraEnvFrom' and trigger a restart of the affected services.\n  # scope: admin-api, alertmanager, compactor, distributor, gateway, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, tokengen\n  podAnnotations: {}\n\n  # -- Pod labels for all pods directly managed by this chart.\n  # scope: admin-api, alertmanager, compactor, distributor, gateway, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, tokengen\n  podLabels: {}\n\nserviceAccount:\n  # -- Whether to create a service account or not. In case 'create' is false, do set 'name' to an existing service account name.\n  create: true\n  # -- Override for the generated service account name.\n  name:\n  annotations: {}\n  labels: {}\n\n# -- Configuration is loaded from the secret called 'externalConfigSecretName'. If 'useExternalConfig' is true, then the configuration is not generated, just consumed.\nuseExternalConfig: false\n\n# -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.\n# In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/mimir/latest/reference-configuration-parameters/#use-environment-variables-in-the-configuration).\n# Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).\nconfigStorageType: ConfigMap\n\n# -- Name of the Secret or ConfigMap that contains the configuration (used for naming even if config is internal).\nexternalConfigSecretName: '{{ include \"mimir.resourceName\" (dict \"ctx\" . \"component\" \"config\") }}'\n\n# -- When 'useExternalConfig' is true, then changing 'externalConfigVersion' triggers restart of services - otherwise changes to the configuration cause a restart.\nexternalConfigVersion: \"0\"\n\n# --Vault Agent config to mount secrets to TLS configurable components. This requires Vault and Vault Agent to already be running.\nvaultAgent:\n  enabled: false\n  # -- Vault Kubernetes Authentication role\n  roleName: \"\"\n  # -- Path to client certificate in Vault\n  clientCertPath: \"\"\n  # -- Path to client key in Vault\n  clientKeyPath: \"\"\n  # -- Path to server certificate in Vault\n  serverCertPath: \"\"\n  # -- Path to server key in Vault\n  serverKeyPath: \"\"\n  # -- Path to client CA certificate in Vault\n  caCertPath: \"\"\n\nmimir:\n  # -- Base config file for Grafana Mimir and Grafana Enterprise Metrics. Contains Helm templates that are evaulated at install/upgrade.\n  # To modify the resulting configuration, either copy and alter 'mimir.config' as a whole or use the 'mimir.structuredConfig' to add and modify certain YAML elements.\n  config: |\n    usage_stats:\n      installation_mode: helm\n\n    activity_tracker:\n      filepath: /active-query-tracker/activity.log\n\n    {{- if .Values.enterprise.enabled }}\n    admin_api:\n      leader_election:\n        enabled: true\n        ring:\n          kvstore:\n            store: \"memberlist\"\n\n    admin_client:\n      storage:\n      {{- if .Values.minio.enabled }}\n        type: s3\n        s3:\n          access_key_id: {{ .Values.minio.rootUser }}\n          bucket_name: enterprise-metrics-admin\n          endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\n          insecure: true\n          secret_access_key: {{ .Values.minio.rootPassword }}\n      {{- end }}\n      {{- if (index .Values \"admin-cache\" \"enabled\") }}\n        cache:\n          backend: memcached\n          memcached:\n            addresses: {{ include \"mimir.adminCacheAddress\" . }}\n            max_item_size: {{ mul (index .Values \"admin-cache\").maxItemMemory 1024 1024 }}\n      {{- end }}\n    {{- end }}\n\n    alertmanager:\n      data_dir: /data\n      enable_api: true\n      external_url: /alertmanager\n      {{- if .Values.alertmanager.zoneAwareReplication.enabled }}\n      sharding_ring:\n        zone_awareness_enabled: true\n      {{- end }}\n      {{- if .Values.alertmanager.fallbackConfig }}\n      fallback_config_file: /configs/alertmanager_fallback_config.yaml\n      {{- end }}\n\n    {{- if .Values.minio.enabled }}\n    alertmanager_storage:\n      backend: s3\n      s3:\n        access_key_id: {{ .Values.minio.rootUser }}\n        bucket_name: {{ include \"mimir.minioBucketPrefix\" . }}-ruler\n        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\n        insecure: true\n        secret_access_key: {{ .Values.minio.rootPassword }}\n    {{- end }}\n\n    {{- if .Values.enterprise.enabled }}\n    auth:\n      type: enterprise\n      admin:\n        pass_access_policy_name: true\n        pass_token_name: true\n    {{- end }}\n\n    # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.\n    blocks_storage:\n      backend: s3\n      bucket_store:\n        {{- if index .Values \"chunks-cache\" \"enabled\" }}\n        chunks_cache:\n          backend: memcached\n          memcached:\n            addresses: {{ include \"mimir.chunksCacheAddress\" . }}\n            max_item_size: {{ mul (index .Values \"chunks-cache\").maxItemMemory 1024 1024 }}\n            timeout: 750ms\n            max_idle_connections: 150\n        {{- end }}\n        {{- if index .Values \"index-cache\" \"enabled\" }}\n        index_cache:\n          backend: memcached\n          memcached:\n            addresses: {{ include \"mimir.indexCacheAddress\" . }}\n            max_item_size: {{ mul (index .Values \"index-cache\").maxItemMemory 1024 1024 }}\n            timeout: 750ms\n            max_idle_connections: 150\n        {{- end }}\n        {{- if index .Values \"metadata-cache\" \"enabled\" }}\n        metadata_cache:\n          backend: memcached\n          memcached:\n            addresses: {{ include \"mimir.metadataCacheAddress\" . }}\n            max_item_size: {{ mul (index .Values \"metadata-cache\").maxItemMemory 1024 1024 }}\n            max_idle_connections: 150\n        {{- end }}\n        sync_dir: /data/tsdb-sync\n      {{- if .Values.minio.enabled }}\n      s3:\n        access_key_id: {{ .Values.minio.rootUser }}\n        bucket_name: {{ include \"mimir.minioBucketPrefix\" . }}-tsdb\n        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\n        insecure: true\n        secret_access_key: {{ .Values.minio.rootPassword }}\n      {{- end }}\n      tsdb:\n        dir: /data/tsdb\n        head_compaction_interval: 15m\n        wal_replay_concurrency: 3\n\n    {{- if .Values.enterprise.enabled }}\n    cluster_name: \"{{ .Release.Name }}\"\n    {{- end }}\n\n    compactor:\n      compaction_interval: 30m\n      deletion_delay: 2h\n      max_closing_blocks_concurrency: 2\n      max_opening_blocks_concurrency: 4\n      symbols_flushers_concurrency: 4\n      first_level_compaction_wait_period: 25m\n      data_dir: \"/data\"\n      sharding_ring:\n        wait_stability_min_duration: 1m\n        heartbeat_period: 1m\n        heartbeat_timeout: 4m\n\n    distributor:\n      ring:\n        heartbeat_period: 1m\n        heartbeat_timeout: 4m\n\n    frontend:\n      parallelize_shardable_queries: true\n      {{- if index .Values \"results-cache\" \"enabled\" }}\n      results_cache:\n        backend: memcached\n        memcached:\n          timeout: 500ms\n          addresses: {{ include \"mimir.resultsCacheAddress\" . }}\n          max_item_size: {{ mul (index .Values \"results-cache\").maxItemMemory 1024 1024 }}\n      cache_results: true\n      query_sharding_target_series_per_shard: 2500\n      {{- end }}\n      {{- if .Values.query_scheduler.enabled }}\n      scheduler_address: {{ template \"mimir.fullname\" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverGrpcListenPort\" . }}\n      {{- end }}\n      {{- if .Values.enterprise.enabled }}\n      log_query_request_headers: X-Access-Policy-Name,X-Token-Name\n      {{- end }}\n\n    frontend_worker:\n      grpc_client_config:\n        max_send_msg_size: 419430400 # 400MiB\n      {{- if .Values.query_scheduler.enabled }}\n      scheduler_address: {{ template \"mimir.fullname\" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverGrpcListenPort\" . }}\n      {{- else }}\n      frontend_address: {{ template \"mimir.fullname\" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverGrpcListenPort\" . }}\n      {{- end }}\n\n    {{- if and .Values.enterprise.enabled }}\n    gateway:\n      proxy:\n        admin_api:\n          url: http://{{ template \"mimir.fullname\" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        alertmanager:\n          url: http://{{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        compactor:\n          url: http://{{ template \"mimir.fullname\" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        default:\n          url: http://{{ template \"mimir.fullname\" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        distributor:\n          url: dns:///{{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include \"mimir.serverGrpcListenPort\" . }}\n        ingester:\n          url: http://{{ template \"mimir.fullname\" . }}-ingester-headless.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        query_frontend:\n          url: http://{{ template \"mimir.fullname\" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        ruler:\n          url: http://{{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        store_gateway:\n          url: http://{{ template \"mimir.fullname\" . }}-store-gateway-headless.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}\n        graphite_write_proxy:\n          url: http://{{ template \"mimir.fullname\" . }}-graphite-write-proxy.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        graphite_querier:\n          url: http://{{ template \"mimir.fullname\" . }}-graphite-querier.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" . }}\n        {{- end}}\n    {{- end }}\n\n    ingester:\n      ring:\n        final_sleep: 0s\n        num_tokens: 512\n        tokens_file_path: /data/tokens\n        unregister_on_shutdown: false\n        heartbeat_period: 2m\n        heartbeat_timeout: 10m\n        {{- if .Values.ingester.zoneAwareReplication.enabled }}\n        zone_awareness_enabled: true\n        {{- end }}\n\n    ingester_client:\n      grpc_client_config:\n        max_recv_msg_size: 104857600\n        max_send_msg_size: 104857600\n\n    {{- if .Values.enterprise.enabled }}\n    instrumentation:\n      enabled: true\n      distributor_client:\n        address: dns:///{{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include \"mimir.serverGrpcListenPort\" . }}\n\n    license:\n      path: \"/license/license.jwt\"\n    {{- end }}\n\n    limits:\n      # Limit queries to 500 days. You can override this on a per-tenant basis.\n      max_total_query_length: 12000h\n      # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.\n      # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.\n      max_query_parallelism: 240\n      # Avoid caching results newer than 10m because some samples can be delayed\n      # This presents caching incomplete results\n      max_cache_freshness: 10m\n\n    memberlist:\n      abort_if_cluster_join_fails: false\n      compression_enabled: false\n      join_members:\n      - dns+{{ include \"mimir.fullname\" . }}-gossip-ring.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include \"mimir.memberlistBindPort\" . }}\n\n    querier:\n      # With query sharding we run more but smaller queries. We must strike a balance\n      # which allows us to process more sharded queries in parallel when requested, but not overload\n      # queriers during non-sharded queries.\n      max_concurrent: 16\n\n    query_scheduler:\n      # Increase from default of 100 to account for queries created by query sharding\n      max_outstanding_requests_per_tenant: 800\n\n    ruler:\n      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager\n      enable_api: true\n      rule_path: /data\n      {{- if .Values.ruler.remoteEvaluationDedicatedQueryPath }}\n      query_frontend:\n        address: dns:///{{ template \"mimir.fullname\" . }}-ruler-query-frontend.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverGrpcListenPort\" .  }}\n      {{- end }}\n\n    {{- if or (.Values.minio.enabled) (index .Values \"metadata-cache\" \"enabled\") }}\n    ruler_storage:\n      {{- if .Values.minio.enabled }}\n      backend: s3\n      s3:\n        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000\n        bucket_name: {{ include \"mimir.minioBucketPrefix\" . }}-ruler\n        access_key_id: {{ .Values.minio.rootUser }}\n        secret_access_key: {{ .Values.minio.rootPassword }}\n        insecure: true\n      {{- end }}\n      {{- if index .Values \"metadata-cache\" \"enabled\" }}\n      cache:\n        backend: memcached\n        memcached:\n          addresses: {{ include \"mimir.metadataCacheAddress\" . }}\n          max_item_size: {{ mul (index .Values \"metadata-cache\").maxItemMemory 1024 1024 }}\n      {{- end }}\n    {{- end }}\n\n    runtime_config:\n      file: /var/{{ include \"mimir.name\" . }}/runtime.yaml\n\n    store_gateway:\n      sharding_ring:\n        heartbeat_period: 1m\n        heartbeat_timeout: 4m\n        wait_stability_min_duration: 1m\n        {{- if .Values.store_gateway.zoneAwareReplication.enabled }}\n        kvstore:\n          prefix: multi-zone/\n        {{- end }}\n        tokens_file_path: /data/tokens\n        unregister_on_shutdown: false\n        {{- if .Values.store_gateway.zoneAwareReplication.enabled }}\n        zone_awareness_enabled: true\n        {{- end }}\n\n    {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}\n    graphite:\n      enabled: true\n\n      write_proxy:\n        distributor_client:\n          address: dns:///{{ template \"mimir.fullname\" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverGrpcListenPort\" .  }}\n\n      querier:\n        remote_read:\n          query_address: http://{{ template \"mimir.fullname\" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include \"mimir.serverHttpListenPort\" .  }}/prometheus\n        proxy_bad_requests: false\n\n        schemas:\n          default_storage_schemas_file: /etc/graphite-proxy/storage-schemas.conf\n          default_storage_aggregations_file: /etc/graphite-proxy/storage-aggregations.conf\n        aggregation_cache:\n          memcached:\n            addresses: dnssrvnoa+{{ template \"mimir.fullname\" . }}-gr-aggr-cache.{{ .Release.Namespace}}.svc:11211\n            timeout: 1s\n        metric_name_cache:\n          memcached:\n            addresses: dnssrvnoa+{{ template \"mimir.fullname\" . }}-gr-metricname-cache.{{ .Release.Namespace}}.svc:11211\n            timeout: 1s\n    {{- end}}\n\n  # -- Additional structured values on top of the text based 'mimir.config'. Applied after the text based config is evaluated for templates. Enables adding and modifying YAML elements in the evaulated 'mimir.config'.\n  #\n  # Additionally, consider the optional \"insecure_skip_verify\" key below, it allows you to skip_verify_false in case the s3_endpoint certificate is not trusted.\n  # For more information see https://grafana.com/docs/mimir/latest/references/configuration-parameters/\n  #\n  # Example:\n  #\n  # structuredConfig:\n  #   common:\n  #     storage:\n  #       backend: s3\n  #       s3:\n  #         bucket_name: \"${BUCKET_NAME}\"\n  #         endpoint: \"${BUCKET_HOST}:${BUCKET_PORT}\"\n  #         access_key_id: \"${AWS_ACCESS_KEY_ID}\" # This is a secret injected via an environment variable\n  #         secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\" # This is a secret injected via an environment variable\n  #         http:\n  #           insecure_skip_verify: true\n  structuredConfig: {}\n\n# -- runtimeConfig provides a reloadable runtime configuration. Changing the runtimeConfig doesn't require a restart of all components.\n# For more infromation see https://grafana.com/docs/mimir/latest/configure/about-runtime-configuration/\n#\n# Example:\n#\n# runtimeConfig:\n#   ingester_limits: # limits that each ingester replica enforces\n#     max_ingestion_rate: 20000\n#     max_series: 1500000\n#     max_tenants: 1000\n#     max_inflight_push_requests: 30000\n#   distributor_limits: # limits that each distributor replica enforces\n#     max_ingestion_rate: 20000\n#     max_inflight_push_requests: 30000\n#     max_inflight_push_requests_bytes: 50000000\n#   overrides:\n#     tenant-1: # limits for tenant-1 that the whole cluster enforces\n#       ingestion_tenant_shard_size: 9\n#       max_global_series_per_user: 1500000\n#       max_fetched_series_per_query: 100000\nruntimeConfig: {}\n\n# RBAC configuration\nrbac:\n  # -- If true, PodSecurityPolicy will be rendered by the chart on Kuberentes 1.24.\n  # By default the PodSecurityPolicy is not rendered on version 1.24.\n  create: true\n  # -- PSP configuration\n  podSecurityPolicy:\n    seccompProfile: runtime/default\n    privileged: false\n    allowPrivilegeEscalation: false\n    hostNetwork: false\n    hostIPC: false\n    hostPID: false\n    readOnlyRootFilesystem: true\n    runAsUser:\n      rule: \"MustRunAsNonRoot\"\n    seLinux:\n      rule: \"RunAsAny\"\n    supplementalGroups:\n      rule: \"MustRunAs\"\n      ranges:\n        - min: 1\n          max: 65535\n    fsGroup:\n      rule: \"MustRunAs\"\n      ranges:\n        - min: 1\n          max: 65535\n    additionalVolumes: []\n  forcePSPOnKubernetes124: false\n  # -- For GKE/EKS/AKS use 'type: psp'. For OpenShift use 'type: scc'\n  type: psp\n  # -- podSecurityContext is the default pod security context for Mimir, GEM, gateway, and cache components.\n  # When installing on OpenShift, override podSecurityContext settings with\n  #\n  # rbac:\n  #   podSecurityContext:\n  #     fsGroup: null\n  #     runAsGroup: null\n  #     runAsUser: null\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n    seccompProfile:\n      type: RuntimeDefault\n\n# -- KEDA Autoscaling configuration\nkedaAutoscaling:\n  # -- A Prometheus-compatible URL. Cadvisor and Mimir metrics for the Mimir pods are expected in this server.\n  # For more information on the required metrics see [Monitor system health](https://grafana.com/docs/helm-charts/mimir-distributed/latest/run-production-environment-with-helm/monitor-system-health/).\n  # If empty, the helm chart uses the metamonitoring URL from metaMonitoring.grafanaAgent.metrics.remote.url.\n  # If that is empty, then the Mimir cluster is used.\n  prometheusAddress: \"\"\n  customHeaders: {}\n  pollingInterval: 10\n\nalertmanager:\n  enabled: true\n  # -- Total number of replicas for the alertmanager across all availability zones\n  # If alertmanager.zoneAwareReplication.enabled=false, this number is taken as is.\n  # Otherwise each zone starts `ceil(replicas / number_of_zones)` number of pods.\n  #   E.g. if 'replicas' is set to 4 and there are 3 zones, then 4/3=1.33 and after rounding up it means 2 pods per zone are started.\n  replicas: 1\n\n  statefulSet:\n    enabled: true\n\n  service:\n    annotations: {}\n    labels: {}\n\n  # -- Optionally set the scheduler for pods of the alertmanager\n  schedulerName: \"\"\n\n  resources:\n    requests:\n      cpu: 10m\n      memory: 32Mi\n\n  # -- Fallback config for alertmanager.\n  # When a tenant doesn't have an Alertmanager configuration, the Grafana Mimir Alertmanager uses the fallback configuration.\n  fallbackConfig: |\n    receivers:\n        - name: default-receiver\n    route:\n        receiver: default-receiver\n\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # -- Pod Disruption Budget for alertmanager, this will be applied across availability zones to prevent losing redundancy\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for alertmanager pods\n  priorityClassName: null\n\n  # -- NodeSelector to pin alertmanager pods to certain set of nodes. This is ignored when alertmanager.zoneAwareReplication.enabled=true.\n  nodeSelector: {}\n  # -- Pod affinity settings for the alertmanager. This is ignored when alertmanager.zoneAwareReplication.enabled=true.\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n  annotations: {}\n  persistence:\n    # SubPath in emptyDir for persistence, only enabled if alertmanager.statefulSet.enabled is false\n    subPath:\n\n  persistentVolume:\n    # If true and alertmanager.statefulSet.enabled is true,\n    # Alertmanager will create/use a Persistent Volume Claim\n    # If false, use emptyDir\n    enabled: true\n\n    # Alertmanager data Persistent Volume Claim annotations\n    #\n    annotations: {}\n\n    # Alertmanager data Persistent Volume access modes\n    # Must match those of existing PV or dynamic provisioner\n    # Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    #\n    accessModes:\n      - ReadWriteOnce\n\n    # Alertmanager data Persistent Volume size\n    #\n    size: 1Gi\n\n    # Subdirectory of Alertmanager data Persistent Volume to mount\n    # Useful if the volume's root directory is not empty\n    #\n    subPath: \"\"\n\n    # Alertmanager data Persistent Volume Storage Class\n    # If defined, storageClassName: \u003cstorageClass\u003e\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    # If undefined (the default) or set to null, no storageClassName spec is\n    #   set, choosing the default provisioner.\n    #\n    # A per-zone storageClass configuration in `alertmanager.zoneAwareReplication.zones[*].storageClass` takes precedence over this field.\n    # storageClass: \"-\"\n\n    # -- Enable StatefulSetAutoDeletePVC feature\n    # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    enableRetentionPolicy: false\n    whenDeleted: Retain\n    whenScaled: Retain\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for alermeneger pods\n  securityContext: {}\n\n  # -- The SecurityContext for alertmanager containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  # Tolerations for pod assignment\n  # ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  tolerations: []\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n  # -- updateStrategy of the alertmanager statefulset. This is ignored when alertmanager.zoneAwareReplication.enabled=true.\n  statefulStrategy:\n    type: RollingUpdate\n\n  terminationGracePeriodSeconds: 900\n\n  initContainers: []\n  # Init containers to be added to the alertmanager pod.\n  # - name: my-init-container\n  #   image: busybox:latest\n  #   command: ['sh', '-c', 'echo hello']\n\n  extraContainers: []\n  # Additional containers to be added to the alertmanager pod.\n  # - name: reverse-proxy\n  #   image: angelbarrera92/basic-auth-reverse-proxy:dev\n  #   args:\n  #     - \"serve\"\n  #     - \"--upstream=http://localhost:3100\"\n  #     - \"--auth-config=/etc/reverse-proxy-conf/authn.yaml\"\n  #   ports:\n  #     - name: http\n  #       containerPort: 11811\n  #       protocol: TCP\n  #   volumeMounts:\n  #     - name: reverse-proxy-auth-config\n  #       mountPath: /etc/reverse-proxy-conf\n\n  extraVolumes: []\n  # Additional volumes to the alertmanager pod.\n  # - name: reverse-proxy-auth-config\n  #   secret:\n  #     secretName: reverse-proxy-auth-config\n\n  # Extra volume mounts that will be added to the alertmanager container\n  extraVolumeMounts: []\n\n  # Extra env variables to pass to the alertmanager container\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n\n  # -- Options to configure zone-aware replication for alertmanager\n  # Example configuration with full geographical redundancy:\n  # rollout_operator:\n  #   enabled: true\n  # alertmanager:\n  #   zoneAwareReplication:\n  #     enabled: true\n  #     topologyKey: 'kubernetes.io/hostname'  # This generates default anti-affinity rules\n  #     zones:  # Zone list has to be fully redefined for modification. Update with you actual zones or skip to use logical zones only.\n  #     - name: zone-a\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-a\n  #     - name: zone-b\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-b\n  #     - name: zone-c\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-c\n  #\n  zoneAwareReplication:\n    # -- Enable zone-aware replication for alertmanager\n    enabled: false\n    # -- Maximum number of alertmanagers that can be unavailable per zone during rollout\n    maxUnavailable: 2\n    # -- topologyKey to use in pod anti-affinity. If unset, no anti-affinity rules are generated. If set, the generated anti-affinity rule makes sure that pods from different zones do not mix.\n    # E.g.: topologyKey: 'kubernetes.io/hostname'\n    topologyKey: null\n    # -- Auxiliary values for migration, see https://grafana.com/docs/helm-charts/mimir-distributed/latest/migration-guides/migrate-from-single-zone-with-helm/\n    migration:\n      # -- Indicate if migration is ongoing for multi zone alertmanager\n      enabled: false\n      # -- Start zone-aware alertmanagers\n      writePath: false\n    # -- Zone definitions for alertmanager zones. Note: you have to redefine the whole list to change parts as YAML does not allow to modify parts of a list.\n    zones:\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-a\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-a\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- Alertmanager data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `alertmanager.persistentVolume.storageClass`.\n        storageClass: null\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-b\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-b\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- Alertmanager data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `alertmanager.persistentVolume.storageClass`.\n        storageClass: null\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-c\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-c\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- Alertmanager data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `alertmanager.persistentVolume.storageClass`.\n        storageClass: null\n\ndistributor:\n  # Setting it to null will produce a deployment without replicas set, allowing you to use autoscaling with the deployment\n  replicas: 1\n\n  # -- [Experimental] Configure autoscaling via KEDA (https://keda.sh). This requires having\n  # KEDA already installed in the Kubernetes cluster. The metrics for scaling are read\n  # according to top-level kedaAutoscaling.prometheusAddress (defaulting to metamonitoring remote-write destination).\n  # Basic auth and extra HTTP headers from metaMonitoring are ignored, please use customHeaders.\n  # The remote URL is used even if metamonitoring is disabled.\n  # See https://github.com/grafana/mimir/issues/7367 for more details on how to migrate to autoscaled resources without disruptions.\n  kedaAutoscaling:\n    enabled: false\n    # -- preserveReplicas gives you the option to migrate from non-autoscaled to autoscaled deployments without losing replicas. When set to true, the replica fields in the component will be left intact.\n    # For futher details see [helm: autoscaling migration procedure](https://github.com/grafana/mimir/issues/7367)\n    preserveReplicas: false\n    minReplicaCount: 1\n    maxReplicaCount: 10\n    targetCPUUtilizationPercentage: 100\n    targetMemoryUtilizationPercentage: 100\n    behavior:\n      scaleDown:\n        policies:\n          - periodSeconds: 600\n            type: Percent\n            value: 10\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 512Mi\n\n  # Additional distributor container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for distributor pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for distributor pods\n  securityContext: {}\n\n  # -- The SecurityContext for distributor containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  # Keep the termination grace period higher than the -shutdown-delay configured on the distributor.\n  terminationGracePeriodSeconds: 100\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 1000\n\ningester:\n  # -- Total number of replicas for the ingester across all availability zones\n  # If ingester.zoneAwareReplication.enabled=false, this number is taken as is.\n  # Otherwise each zone starts `ceil(replicas / number_of_zones)` number of pods.\n  #   E.g. if 'replicas' is set to 4 and there are 3 zones, then 4/3=1.33 and after rounding up it means 2 pods per zone are started.\n  replicas: 3\n\n  statefulSet:\n    enabled: true\n\n  service:\n    annotations: {}\n    labels: {}\n\n  # -- Optionally set the scheduler for pods of the ingester\n  schedulerName: \"\"\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 512Mi\n\n  # Additional ingester container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # -- The name of the PriorityClass for ingester pods\n  priorityClassName: null\n\n  # -- Pod Disruption Budget for ingester, this will be applied across availability zones to prevent losing redundancy\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  podManagementPolicy: Parallel\n\n  # -- NodeSelector to pin ingester pods to certain set of nodes. This is ignored when ingester.zoneAwareReplication.enabled=true.\n  nodeSelector: {}\n  # -- Pod affinity settings for the ingester. This is ignored when ingester.zoneAwareReplication.enabled=true.\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  # It is recommended to replace this with requiredDuringSchedulingIgnoredDuringExecution podAntiAffinity rules when\n  # deploying to production.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n\n  persistentVolume:\n    # If true and ingester.statefulSet.enabled is true,\n    # Ingester will create/use a Persistent Volume Claim\n    # If false, use emptyDir\n    # It is advisable to enable volume persistence in ingester to avoid losing metrics.\n    #\n    enabled: true\n\n    # Ingester data Persistent Volume Claim annotations\n    #\n    annotations: {}\n\n    # Ingester data Persistent Volume access modes\n    # Must match those of existing PV or dynamic provisioner\n    # Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    accessModes:\n      - ReadWriteOnce\n\n    # Ingester data Persistent Volume size\n    size: 2Gi\n\n    # Subdirectory of Ingester data Persistent Volume to mount\n    # Useful if the volume's root directory is not empty\n    subPath: \"\"\n\n    # -- Enable StatefulSetAutoDeletePVC feature\n    # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    enableRetentionPolicy: false\n    whenDeleted: Retain\n    whenScaled: Retain\n\n    # Ingester data Persistent Volume Storage Class\n    # If defined, storageClassName: \u003cstorageClass\u003e\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    # If undefined (the default) or set to null, no storageClassName spec is\n    #   set, choosing the default provisioner.\n    #\n    # A per-zone storageClass configuration in `ingester.zoneAwareReplication.zones[*].storageClass` takes precedence over this field.\n    #\n    # storageClass: \"-\"\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 60\n\n  # -- SecurityContext override for ingester pods\n  securityContext: {}\n\n  # -- The SecurityContext for ingester containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  # -- updateStrategy of the ingester statefulset. This is ignored when ingester.zoneAwareReplication.enabled=true.\n  statefulStrategy:\n    type: RollingUpdate\n\n  terminationGracePeriodSeconds: 1200\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 1000\n\n  # -- Options to configure zone-aware replication for ingester\n  # Example configuration with full geographical redundancy:\n  # rollout_operator:\n  #   enabled: true\n  # ingester:\n  #   zoneAwareReplication:\n  #     enabled: true\n  #     topologyKey: 'kubernetes.io/hostname'  # This generates default anti-affinity rules\n  #     zones:  # Zone list has to be fully redefined for modification. Update with you actual zones or skip to use logical zones only.\n  #     - name: zone-a\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-a\n  #       storageClass: storage-class-us-central1-a\n  #     - name: zone-a\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-b\n  #       storageClass: storage-class-us-central1-b\n  #     - name: zone-c\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-c\n  #       storageClass: storage-class-us-central1-c\n  #\n  zoneAwareReplication:\n    # -- Enable zone-aware replication for ingester\n    enabled: true\n    # -- Maximum number of ingesters that can be unavailable per zone during rollout\n    maxUnavailable: 50\n    # -- topologyKey to use in pod anti-affinity. If unset, no anti-affinity rules are generated. If set, the generated anti-affinity rule makes sure that pods from different zones do not mix.\n    # E.g.: topologyKey: 'kubernetes.io/hostname'\n    topologyKey: null\n    # -- Auxiliary values for migration, see https://grafana.com/docs/helm-charts/mimir-distributed/latest/migration-guides/migrate-from-single-zone-with-helm/\n    migration:\n      # -- Indicate if migration is ongoing for multi zone ingester\n      enabled: false\n      # -- Exclude default zone on write path\n      excludeDefaultZone: false\n      # -- Enable zone-awareness, read path only\n      readPath: false\n      # -- Total number of replicas to start in availability zones when migration is enabled\n      replicas: 0\n      # -- Scale default zone ingesters to 0\n      scaleDownDefaultZone: false\n      # -- Enable zone-awareness, write path only\n      writePath: false\n    # -- Zone definitions for ingester zones. Note: you have to redefine the whole list to change parts as YAML does not allow to modify parts of a list.\n    zones:\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-a\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-a\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- Ingester data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `ingester.persistentVolume.storageClass`.\n        storageClass: null\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-b\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-b\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- Ingester data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `ingester.persistentVolume.storageClass`.\n        storageClass: null\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-c\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-c\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- Ingester data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `ingester.persistentVolume.storageClass`.\n        storageClass: null\n\noverrides_exporter:\n  enabled: true\n  replicas: 1\n\n  annotations: {}\n\n  initContainers: []\n\n  service:\n    annotations: {}\n    labels: {}\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  podLabels: {}\n  podAnnotations: {}\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for overrides-exporter pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: {}\n    #  maxSkew: 1\n    #  topologyKey: kubernetes.io/hostname\n    #  whenUnsatisfiable: ScheduleAnyway\n    #  minDomains: 1\n    #  nodeAffinityPolicy: Honor\n    #  nodeTaintsPolicy: Honor\n    #  matchLabelKeys:\n  #    - pod-template-hash\n\n  # -- SecurityContext override for overrides-exporter pods\n  securityContext: {}\n\n  # -- The SecurityContext for overrides-exporter containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  extraArgs: {}\n\n  persistence:\n    subPath:\n\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  terminationGracePeriodSeconds: 30\n\n  tolerations: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n\nruler:\n  enabled: true\n  replicas: 1\n\n  # -- [Experimental] Configure autoscaling via KEDA (https://keda.sh). This requires having\n  # KEDA already installed in the Kubernetes cluster. The metrics for scaling are read\n  # according to top-level kedaAutoscaling.prometheusAddress (defaulting to metamonitoring remote-write destination).\n  # Basic auth and extra HTTP headers from metaMonitoring are ignored, please use customHeaders.\n  # The remote URL is used even if metamonitoring is disabled.\n  # See https://github.com/grafana/mimir/issues/7367 for more details on how to migrate to autoscaled resources without disruptions.\n  kedaAutoscaling:\n    enabled: false\n    # -- preserveReplicas gives you the option to migrate from non-autoscaled to autoscaled deployments without losing replicas. When set to true, the replica fields in the component will be left intact.\n    # For futher details see [helm: autoscaling migration procedure](https://github.com/grafana/mimir/issues/7367)\n    preserveReplicas: false\n    minReplicaCount: 1\n    maxReplicaCount: 10\n    targetCPUUtilizationPercentage: 100\n    targetMemoryUtilizationPercentage: 100\n    behavior:\n      scaleDown:\n        policies:\n          - periodSeconds: 600\n            type: Percent\n            value: 10\n\n  service:\n    annotations: {}\n    labels: {}\n\n  # -- Dedicated service account for ruler pods.\n  # If not set, the default service account defined at the begining of this file will be used.\n  # This service account can be used even if the default one is not set.\n  serviceAccount:\n    create: false\n    # -- Ruler specific service account name. If not set and create is set to true, the default\n    # name will be the default mimir service account's name with the \"-ruler\" suffix.\n    name: \"\"\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional ruler container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n  # log.level: debug\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for ruler pods\n  securityContext: {}\n\n  # -- The SecurityContext for ruler containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 50%\n      maxUnavailable: 0\n\n  terminationGracePeriodSeconds: 600\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 1000\n\n  # -- If set to true, a dedicated query path will be deployed for the ruler and operational mode will be set to use remote evaluation. https://grafana.com/docs/mimir/latest/references/architecture/components/ruler/#remote\n  # -- This is useful for isolating the ruler queries from other queriers (api/grafana).\n  remoteEvaluationDedicatedQueryPath: false\n\n# -- Only deployed if .Values.ruler.remoteEvaluationDedicatedQueryPath\nruler_querier:\n  replicas: 2\n\n  # -- [Experimental] Configure autoscaling via KEDA (https://keda.sh). This requires having\n  # KEDA already installed in the Kubernetes cluster. The metrics for scaling are read\n  # according to top-level kedaAutoscaling.prometheusAddress (defaulting to metamonitoring remote-write destination).\n  # Basic auth and extra HTTP headers from metaMonitoring are ignored, please use customHeaders.\n  # The remote URL is used even if metamonitoring is disabled.\n  # See https://github.com/grafana/mimir/issues/7367 for more details on how to migrate to autoscaled resources without disruptions.\n  kedaAutoscaling:\n    enabled: false\n    # -- preserveReplicas gives you the option to migrate from non-autoscaled to autoscaled deployments without losing replicas. When set to true, the replica fields in the component will be left intact.\n    # For futher details see [helm: autoscaling migration procedure](https://github.com/grafana/mimir/issues/7367)\n    preserveReplicas: false\n    minReplicaCount: 1\n    maxReplicaCount: 10\n    querySchedulerInflightRequestsThreshold: 12\n    behavior:\n      scaleDown:\n        policies:\n          - periodSeconds: 120\n            type: Percent\n            value: 10\n        stabilizationWindowSeconds: 600\n      scaleUp:\n        policies:\n          - periodSeconds: 120\n            type: Percent\n            value: 50\n          - periodSeconds: 120\n            type: Pods\n            value: 15\n        stabilizationWindowSeconds: 60\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional ruler-querier container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for ruler-querier pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for ruler-querier pods\n  securityContext: {}\n\n  # -- The SecurityContext for ruler-querier containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  terminationGracePeriodSeconds: 180\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 5000\n\n# -- Only deployed if .Values.ruler.remoteEvaluationDedicatedQueryPath\nruler_query_frontend:\n  # Setting it to null will produce a deployment without replicas set, allowing you to use autoscaling with the deployment\n  replicas: 1\n\n  # -- [Experimental] Configure autoscaling via KEDA (https://keda.sh). This requires having\n  # KEDA already installed in the Kubernetes cluster. The metrics for scaling are read\n  # according to top-level kedaAutoscaling.prometheusAddress (defaulting to metamonitoring remote-write destination).\n  # Basic auth and extra HTTP headers from metaMonitoring are ignored, please use customHeaders.\n  # The remote URL is used even if metamonitoring is disabled.\n  # See https://github.com/grafana/mimir/issues/7367 for more details on how to migrate to autoscaled resources without disruptions.\n  kedaAutoscaling:\n    enabled: false\n    # -- preserveReplicas gives you the option to migrate from non-autoscaled to autoscaled deployments without losing replicas. When set to true, the replica fields in the component will be left intact.\n    # For futher details see [helm: autoscaling migration procedure](https://github.com/grafana/mimir/issues/7367)\n    preserveReplicas: false\n    minReplicaCount: 1\n    maxReplicaCount: 10\n    targetCPUUtilizationPercentage: 75\n    targetMemoryUtilizationPercentage: 100\n    behavior:\n      scaleDown:\n        policies:\n          - periodSeconds: 60\n            type: Percent\n            value: 10\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional ruler-query-frontend container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for ruler-query-frontend pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for query-fronted pods\n  securityContext: {}\n\n  # -- The SecurityContext for ruler-query-frontend containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  terminationGracePeriodSeconds: 390\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 5000\n\n# -- Only deployed if .Values.ruler.remoteEvaluationDedicatedQueryPath\nruler_query_scheduler:\n  replicas: 2\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional ruler-query-scheduler container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for ruler-query-scheduler pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for ruler-query-scheduler pods\n  securityContext: {}\n\n  # -- The SecurityContext for ruler-query-scheduler containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n\n  terminationGracePeriodSeconds: 180\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n\nquerier:\n  replicas: 2\n\n  # -- [Experimental] Configure autoscaling via KEDA (https://keda.sh). This requires having\n  # KEDA already installed in the Kubernetes cluster. The metrics for scaling are read\n  # according to top-level kedaAutoscaling.prometheusAddress (defaulting to metamonitoring remote-write destination).\n  # Basic auth and extra HTTP headers from metaMonitoring are ignored, please use customHeaders.\n  # The remote URL is used even if metamonitoring is disabled.\n  # See https://github.com/grafana/mimir/issues/7367 for more details on how to migrate to autoscaled resources without disruptions.\n  kedaAutoscaling:\n    enabled: false\n    # -- preserveReplicas gives you the option to migrate from non-autoscaled to autoscaled deployments without losing replicas. When set to true, the replica fields in the component will be left intact.\n    # For futher details see [helm: autoscaling migration procedure](https://github.com/grafana/mimir/issues/7367)\n    preserveReplicas: false\n    minReplicaCount: 1\n    maxReplicaCount: 10\n    querySchedulerInflightRequestsThreshold: 12\n    # -- predictiveScalingEnabled scales up the querier based on the inflight requests in the past.\n    # This helps with scaling up for predictable traffic patterns and minimizing HTTP 429 responses due to filled query queues.\n    # Due to false positive items it can increase the querier TCO.\n    predictiveScalingEnabled: false\n    # -- The period to consider when considering scheduler metrics for predictive scaling.\n    # This is usually slightly lower than the period of the repeating query events to give scaling up lead time.\n    predictiveScalingPeriod: 6d23h30m\n    # -- The time range to consider when considering scheduler metrics for predictive scaling.\n    # For example: if lookback is 30m and period is 6d23h30m,\n    # the querier will scale based on the maximum inflight queries between 6d23h30m and 7d ago.\n    predictiveScalingLookback: 30m\n    behavior:\n      scaleDown:\n        policies:\n          - periodSeconds: 120\n            type: Percent\n            value: 10\n        stabilizationWindowSeconds: 600\n      scaleUp:\n        policies:\n          - periodSeconds: 120\n            type: Percent\n            value: 50\n          - periodSeconds: 120\n            type: Pods\n            value: 15\n        stabilizationWindowSeconds: 60\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional querier container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for querier pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for querier pods\n  securityContext: {}\n\n  # -- The SecurityContext for querier containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  terminationGracePeriodSeconds: 180\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 5000\n\nquery_frontend:\n  # Setting it to null will produce a deployment without replicas set, allowing you to use autoscaling with the deployment\n  replicas: 1\n\n  # -- [Experimental] Configure autoscaling via KEDA (https://keda.sh). This requires having\n  # KEDA already installed in the Kubernetes cluster. The metrics for scaling are read\n  # according to top-level kedaAutoscaling.prometheusAddress (defaulting to metamonitoring remote-write destination).\n  # Basic auth and extra HTTP headers from metaMonitoring are ignored, please use customHeaders.\n  # The remote URL is used even if metamonitoring is disabled.\n  # See https://github.com/grafana/mimir/issues/7367 for more details on how to migrate to autoscaled resources without disruptions.\n  kedaAutoscaling:\n    enabled: false\n    # -- preserveReplicas gives you the option to migrate from non-autoscaled to autoscaled deployments without losing replicas. When set to true, the replica fields in the component will be left intact.\n    # For futher details see [helm: autoscaling migration procedure](https://github.com/grafana/mimir/issues/7367)\n    preserveReplicas: false\n    minReplicaCount: 1\n    maxReplicaCount: 10\n    targetCPUUtilizationPercentage: 75\n    targetMemoryUtilizationPercentage: 100\n    behavior:\n      scaleDown:\n        policies:\n          - periodSeconds: 60\n            type: Percent\n            value: 10\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional query-frontend container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for query-frontend pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for query-fronted pods\n  securityContext: {}\n\n  # -- The SecurityContext for query-frontend containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  terminationGracePeriodSeconds: 390\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 5000\n\nquery_scheduler:\n  enabled: true\n  replicas: 2\n\n  service:\n    annotations: {}\n    labels: {}\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Additional query-scheduler container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for query-scheduler pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  # -- SecurityContext override for query-scheduler pods\n  securityContext: {}\n\n  # -- The SecurityContext for query-scheduler containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n\n  terminationGracePeriodSeconds: 180\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n\nstore_gateway:\n  # -- Total number of replicas for the store-gateway across all availability zones\n  # If store_gateway.zoneAwareReplication.enabled=false, this number is taken as is.\n  # Otherwise each zone starts `ceil(replicas / number_of_zones)` number of pods.\n  #   E.g. if 'replicas' is set to 4 and there are 3 zones, then 4/3=1.33 and after rounding up it means 2 pods per zone are started.\n  replicas: 1\n\n  service:\n    annotations: {}\n    labels: {}\n\n  # -- Optionally set the scheduler for pods of the store-gateway\n  schedulerName: \"\"\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 512Mi\n\n  # Additional store-gateway container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # -- Management policy for store-gateway pods\n  # New variable introduced with Helm chart version 5.1.0. For backwards compatibility it is set to `OrderedReady`\n  # On new deployments it is highly recommended to switch it to `Parallel` as this will be the new default from 6.0.0\n  podManagementPolicy: OrderedReady\n\n  # -- Pod Disruption Budget for store-gateway, this will be applied across availability zones to prevent losing redundancy\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for store-gateway pods\n  priorityClassName: null\n\n  # -- NodeSelector to pin store-gateway pods to certain set of nodes. This is ignored when store_gateway.zoneAwareReplication.enabled=true.\n  nodeSelector: {}\n  # -- Pod affinity settings for the store_gateway. This is ignored when store_gateway.zoneAwareReplication.enabled=true.\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  # It is recommended to replace this with requiredDuringSchedulingIgnoredDuringExecution podAntiAffinity rules when\n  # deploying to production.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n\n  persistentVolume:\n    # If true Store-gateway will create/use a Persistent Volume Claim\n    # If false, use emptyDir\n    #\n    enabled: true\n\n    # Store-gateway data Persistent Volume Claim annotations\n    #\n    annotations: {}\n\n    # Store-gateway data Persistent Volume access modes\n    # Must match those of existing PV or dynamic provisioner\n    # Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    #\n    accessModes:\n      - ReadWriteOnce\n\n    # Store-gateway data Persistent Volume size\n    #\n    size: 2Gi\n\n    # Subdirectory of Store-gateway data Persistent Volume to mount\n    # Useful if the volume's root directory is not empty\n    #\n    subPath: \"\"\n\n    # Store-gateway data Persistent Volume Storage Class\n    # If defined, storageClassName: \u003cstorageClass\u003e\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    # If undefined (the default) or set to null, no storageClassName spec is\n    #   set, choosing the default provisioner.\n    #\n    # A per-zone storageClass configuration in `store_gateway.zoneAwareReplication.zones[*].storageClass` takes precedence over this field.\n    # storageClass: \"-\"\n\n    # -- Enable StatefulSetAutoDeletePVC feature\n    # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    enableRetentionPolicy: false\n    whenDeleted: Retain\n    whenScaled: Retain\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 60\n\n  # -- SecurityContext override for store-gateway pods\n  securityContext: {}\n\n  # -- The SecurityContext for store-gateway containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  # -- updateStrategy of the store-gateway statefulset. This is ignored when store_gateway.zoneAwareReplication.enabled=true.\n  strategy:\n    type: RollingUpdate\n\n  terminationGracePeriodSeconds: 120\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: 1000\n\n  # -- Options to configure zone-aware replication for store-gateway\n  # Example configuration with full geographical redundancy:\n  # rollout_operator:\n  #   enabled: true\n  # store_gateway:\n  #   zoneAwareReplication:\n  #     enabled: true\n  #     topologyKey: 'kubernetes.io/hostname'  # This generates default anti-affinity rules\n  #     zones:  # Zone list has to be fully redefined for modification. Update with you actual zones or skip to use logical zones only.\n  #     - name: zone-a\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-a\n  #     - name: zone-a\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-b\n  #     - name: zone-c\n  #       nodeSelector:\n  #         topology.kubernetes.io/zone: us-central1-c\n  #\n  zoneAwareReplication:\n    # -- Enable zone-aware replication for store-gateway\n    enabled: true\n    # -- Maximum number of store-gateways that can be unavailable per zone during rollout\n    maxUnavailable: 50\n    # -- topologyKey to use in pod anti-affinity. If unset, no anti-affinity rules are generated. If set, the generated anti-affinity rule makes sure that pods from different zones do not mix.\n    # E.g.: topologyKey: 'kubernetes.io/hostname'\n    topologyKey: null\n    # -- Auxiliary values for migration, see https://grafana.com/docs/helm-charts/mimir-distributed/latest/migration-guides/migrate-from-single-zone-with-helm/\n    migration:\n      # -- Indicate if migration is ongoing for multi zone store-gateway\n      enabled: false\n      # -- Enable zone-awareness on the readPath\n      readPath: false\n    # -- Zone definitions for store-gateway zones. Note: you have to redefine the whole list to change parts as YAML does not allow to modify parts of a list.\n    zones:\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-a\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-a\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- StoreGateway data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `store_gateway.persistentVolume.storageClass`.\n        storageClass: null\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-b\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-b\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- StoreGateway data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `store_gateway.persistentVolume.storageClass`.\n        storageClass: null\n      # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      - name: zone-c\n        # -- nodeselector to restrict where pods of this zone can be placed. E.g.:\n        # nodeSelector:\n        #   topology.kubernetes.io/zone: zone-c\n        nodeSelector: null\n        # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)\n        extraAffinity: {}\n        # -- StoreGateway data Persistent Volume Storage Class\n        # If defined, storageClassName: \u003cstorageClass\u003e\n        # If set to \"-\", then use `storageClassName: \"\"`, which disables dynamic provisioning\n        # If undefined or set to null (the default), then fall back to the value of `store_gateway.persistentVolume.storageClass`.\n        storageClass: null\n\ncompactor:\n  replicas: 1\n\n  service:\n    annotations: {}\n    labels: {}\n\n  # -- Optionally set the scheduler for pods of the compactor\n  schedulerName: \"\"\n\n  resources:\n    requests:\n      cpu: 100m\n      memory: 512Mi\n\n  # Additional compactor container arguments, e.g. log level (debug, info, warn, error)\n  extraArgs: {}\n\n  # Pod Labels\n  podLabels: {}\n\n  # Pod Annotations\n  podAnnotations: {}\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  podManagementPolicy: OrderedReady\n\n  # -- The name of the PriorityClass for compactor pods\n  priorityClassName: null\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n\n  persistentVolume:\n    # If true compactor will create/use a Persistent Volume Claim\n    # If false, use emptyDir\n    #\n    enabled: true\n\n    # compactor data Persistent Volume Claim annotations\n    #\n    annotations: {}\n\n    # compactor data Persistent Volume access modes\n    # Must match those of existing PV or dynamic provisioner\n    # Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    #\n    accessModes:\n      - ReadWriteOnce\n\n    # compactor data Persistent Volume size\n    #\n    size: 2Gi\n\n    # Subdirectory of compactor data Persistent Volume to mount\n    # Useful if the volume's root directory is not empty\n    #\n    subPath: \"\"\n\n    # compactor data Persistent Volume Storage Class\n    # If defined, storageClassName: \u003cstorageClass\u003e\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    # If undefined (the default) or set to null, no storageClassName spec is\n    #   set, choosing the default provisioner.\n    #\n    # storageClass: \"-\"\n\n    # -- Enable StatefulSetAutoDeletePVC feature\n    # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    enableRetentionPolicy: false\n    whenDeleted: Retain\n    whenScaled: Retain\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 60\n\n  # -- SecurityContext override for compactor pods\n  securityContext: {}\n\n  # -- The SecurityContext for compactor containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  strategy:\n    type: RollingUpdate\n\n  terminationGracePeriodSeconds: 900\n\n  tolerations: []\n  initContainers: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n\nmemcached:\n  image:\n    # -- Memcached Docker image repository\n    repository: memcached\n    # -- Memcached Docker image tag\n    tag: 1.6.31-alpine\n    # -- Memcached Docker image pull policy\n    pullPolicy: IfNotPresent\n\n  # -- The SecurityContext override for memcached pods\n  podSecurityContext: {}\n\n  # -- The name of the PriorityClass for memcached pods\n  priorityClassName: null\n\n  # -- The SecurityContext for memcached containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n\nmemcachedExporter:\n  # -- Whether memcached metrics should be exported\n  enabled: true\n\n  image:\n    repository: prom/memcached-exporter\n    tag: v0.14.4\n    pullPolicy: IfNotPresent\n\n  resources:\n    requests: {}\n    limits: {}\n\n  # -- The SecurityContext for memcached exporter containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n\n  # -- Extra args to add to the exporter container.\n  # Example:\n  # extraArgs:\n  #   memcached.tls.enable: true\n  #   memcached.tls.cert-file: /certs/cert.crt\n  #   memcached.tls.key-file: /certs/cert.key\n  #   memcached.tls.ca-file: /certs/ca.crt\n  #   memcached.tls.insecure-skip-verify: false\n  #   memcached.tls.server-name: memcached\n  extraArgs: {}\n\nchunks-cache:\n  # -- Specifies whether memcached based chunks-cache should be enabled\n  enabled: false\n\n  # -- Total number of chunks-cache replicas\n  replicas: 1\n\n  # -- Port of the chunks-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to chunks-cache for object storage (in MB).\n  allocatedMemory: 8192\n\n  # -- Maximum item memory for chunks-cache (in MB).\n  maxItemMemory: 1\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for chunks-cache pods\n  initContainers: []\n\n  # -- Annotations for the chunks-cache pods\n  annotations: {}\n  # -- Node selector for chunks-cache pods\n  nodeSelector: {}\n  # -- Affinity for chunks-cache pods\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: {}\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  #  minDomains: 1\n  #  nodeAffinityPolicy: Honor\n  #  nodeTaintsPolicy: Honor\n  #  matchLabelKeys:\n  #    - pod-template-hash\n\n  # -- Tolerations for chunks-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for chunks-cache pods\n  priorityClassName: null\n  # -- Labels for chunks-cache pods\n  podLabels: {}\n  # -- Annotations for chunks-cache pods\n  podAnnotations: {}\n  # -- Management policy for chunks-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the chunks-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n\n  # -- Stateful chunks-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Add extended options for chunks-cache memcached container. The format is the same as for the memcached -o/--extend flag.\n  # Example:\n  # extraExtendedOptions: 'tls,no_hashexpand'\n  extraExtendedOptions: \"\"\n\n  # -- Additional CLI args for chunks-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the chunks-cache pod.\n  extraContainers: []\n\n  # -- Additional volumes to be added to the chunks-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumes:\n  # - name: extra-volume\n  #   secret:\n  #    secretName: extra-volume-secret\n  extraVolumes: []\n\n  # -- Additional volume mounts to be added to the chunks-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumeMounts:\n  # - name: extra-volume\n  #   mountPath: /etc/extra-volume\n  #   readOnly: true\n  extraVolumeMounts: []\n\n  # -- List of additional PVCs to be created for the chunks-cache statefulset\n  volumeClaimTemplates: []\n\n  # -- Resource requests and limits for the chunks-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\nindex-cache:\n  # -- Specifies whether memcached based index-cache should be enabled\n  enabled: false\n\n  # -- Total number of index-cache replicas\n  replicas: 1\n\n  # -- Port of the index-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to index-cache for object storage (in MB).\n  allocatedMemory: 2048\n\n  # -- Maximum item index-cache for memcached (in MB).\n  maxItemMemory: 5\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for index-cache pods\n  initContainers: []\n\n  # -- Annotations for the index-cache pods\n  annotations: {}\n  # -- Node selector for index-cache pods\n  nodeSelector: {}\n  # -- Affinity for index-cache pods\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: {}\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  #  minDomains: 1\n  #  nodeAffinityPolicy: Honor\n  #  nodeTaintsPolicy: Honor\n  #  matchLabelKeys:\n  #    - pod-template-hash\n\n  # -- Tolerations for index-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for index-cache pods\n  priorityClassName: null\n  # -- Labels for index-cache pods\n  podLabels: {}\n  # -- Annotations for index-cache pods\n  podAnnotations: {}\n  # -- Management policy for index-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the index-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n\n  # -- Stateful index-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Add extended options for index-cache memcached container. The format is the same as for the memcached -o/--extend flag.\n  # Example:\n  # extraExtendedOptions: 'tls,modern,track_sizes'\n  extraExtendedOptions: \"\"\n\n  # -- Additional CLI args for index-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the index-cache pod.\n  extraContainers: []\n\n  # -- Additional volumes to be added to the index-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumes:\n  # - name: extra-volume\n  #   secret:\n  #    secretName: extra-volume-secret\n  extraVolumes: []\n\n  # -- Additional volume mounts to be added to the index-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumeMounts:\n  # - name: extra-volume\n  #   mountPath: /etc/extra-volume\n  #   readOnly: true\n  extraVolumeMounts: []\n\n  # -- List of additional PVCs to be created for the index-cache statefulset\n  volumeClaimTemplates: []\n\n  # -- Resource requests and limits for the index-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\nmetadata-cache:\n  # -- Specifies whether memcached based metadata-cache should be enabled\n  enabled: false\n\n  # -- Total number of metadata-cache replicas\n  replicas: 1\n\n  # -- Port of the metadata-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to metadata-cache for object storage (in MB).\n  allocatedMemory: 512\n\n  # -- Maximum item metadata-cache for memcached (in MB).\n  maxItemMemory: 1\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for metadata-cache pods\n  initContainers: []\n\n  # -- Annotations for the metadata-cache pods\n  annotations: {}\n  # -- Node selector for metadata-cache pods\n  nodeSelector: {}\n  # -- Affinity for metadata-cache pods\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: {}\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  #  minDomains: 1\n  #  nodeAffinityPolicy: Honor\n  #  nodeTaintsPolicy: Honor\n  #  matchLabelKeys:\n  #    - pod-template-hash\n\n  # -- Tolerations for metadata-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for metadata-cache pods\n  priorityClassName: null\n  # -- Labels for metadata-cache pods\n  podLabels: {}\n  # -- Annotations for metadata-cache pods\n  podAnnotations: {}\n  # -- Management policy for metadata-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the metadata-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n\n  # -- Stateful metadata-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Add extended options for metadata-cache memcached container. The format is the same as for the memcached -o/--extend flag.\n  # Example:\n  # extraExtendedOptions: 'tls,modern,track_sizes'\n  extraExtendedOptions: \"\"\n\n  # -- Additional CLI args for metadata-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the metadata-cache pod.\n  extraContainers: []\n\n  # -- Additional volumes to be added to the metadata-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumes:\n  # - name: extra-volume\n  #   secret:\n  #    secretName: extra-volume-secret\n  extraVolumes: []\n\n  # -- Additional volume mounts to be added to the metadata-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumeMounts:\n  # - name: extra-volume\n  #   mountPath: /etc/extra-volume\n  #   readOnly: true\n  extraVolumeMounts: []\n\n  # -- List of additional PVCs to be created for the metadata-cache statefulset\n  volumeClaimTemplates: []\n\n  # -- Resource requests and limits for the metadata-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\nresults-cache:\n  # -- Specifies whether memcached based results-cache should be enabled\n  enabled: false\n\n  # -- Total number of results-cache replicas\n  replicas: 1\n\n  # -- Port of the results-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to results-cache for object storage (in MB).\n  allocatedMemory: 512\n\n  # -- Maximum item results-cache for memcached (in MB).\n  maxItemMemory: 5\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for results-cache pods\n  initContainers: []\n\n  # -- Annotations for the results-cache pods\n  annotations: {}\n  # -- Node selector for results-cache pods\n  nodeSelector: {}\n  # -- Affinity for results-cache pods\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: {}\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  #  minDomains: 1\n  #  nodeAffinityPolicy: Honor\n  #  nodeTaintsPolicy: Honor\n  #  matchLabelKeys:\n  #    - pod-template-hash\n\n  # -- Tolerations for results-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for results-cache pods\n  priorityClassName: null\n  # -- Labels for results-cache pods\n  podLabels: {}\n  # -- Annotations for results-cache pods\n  podAnnotations: {}\n  # -- Management policy for results-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the results-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n\n  # -- Stateful results-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Add extended options for results-cache memcached container. The format is the same as for the memcached -o/--extend flag.\n  # Example:\n  # extraExtendedOptions: 'tls,modern,track_sizes'\n  extraExtendedOptions: \"\"\n\n  # -- Additional CLI args for results-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the results-cache pod.\n  extraContainers: []\n\n  # -- Additional volumes to be added to the results-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumes:\n  # - name: extra-volume\n  #   secret:\n  #    secretName: extra-volume-secret\n  extraVolumes: []\n\n  # -- Additional volume mounts to be added to the results-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumeMounts:\n  # - name: extra-volume\n  #   mountPath: /etc/extra-volume\n  #   readOnly: true\n  extraVolumeMounts: []\n\n  # -- List of additional PVCs to be created for the results-cache statefulset\n  volumeClaimTemplates: []\n\n  # -- Resource requests and limits for the results-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\n# -- Setting for the Grafana Rollout Operator https://github.com/grafana/helm-charts/tree/main/charts/rollout-operator\nrollout_operator:\n  enabled: true\n\n  # -- podSecurityContext is the pod security context for the rollout operator.\n  # When installing on OpenShift, override podSecurityContext settings with\n  #\n  # rollout_operator:\n  #   podSecurityContext:\n  #     fsGroup: null\n  #     runAsGroup: null\n  #     runAsUser: null\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n    seccompProfile:\n      type: RuntimeDefault\n\n  # Set the container security context\n  securityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n\nminio:\n  enabled: true\n  mode: standalone\n  rootUser: grafana-mimir\n  buckets:\n    - name: mimir-tsdb\n      policy: none\n      purge: false\n    - name: mimir-ruler\n      policy: none\n      purge: false\n    - name: enterprise-metrics-tsdb\n      policy: none\n      purge: false\n    - name: enterprise-metrics-admin\n      policy: none\n      purge: false\n    - name: enterprise-metrics-ruler\n      policy: none\n      purge: false\n  persistence:\n    size: 5Gi\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n  rootPassword: supersecret\n  # Changed the mc config path to '/tmp' from '/etc' as '/etc' is only writable by root and OpenShift will not permit this.\n  configPathmc: \"/tmp/minio/mc/\"\n\n# -- DEPRECATED: use the 'gateway' section instead. For a migration guide refer to\n# https://grafana.com/docs/helm-charts/mimir-distributed/latest/migration-guides/migrate-to-unified-proxy-deployment/\n#\n# Configuration for nginx gateway.\n# Can only be enabled when 'enterprise.enabled' is false.\nnginx:\n  # -- Specifies whether nginx should be enabled\n  enabled: true\n  # -- Number of replicas for nginx\n  replicas: 1\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  autoscaling:\n    # -- Enable autoscaling for nginx\n    enabled: false\n    # -- Minimum autoscaling replicas for nginx\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for nginx\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for nginx\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for nginx\n    targetMemoryUtilizationPercentage:\n  # -- See `kubectl explain deployment.spec.strategy` for more,\n  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n  deploymentStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n  image:\n    # -- The Docker registry for nginx image\n    registry: docker.io\n    # -- The nginx image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The nginx image tag\n    tag: 1.27-alpine\n    # -- The nginx image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for nginx pods\n  priorityClassName: null\n  # -- Labels for nginx pods\n  podLabels: {}\n  # -- Annotations for nginx pods\n  podAnnotations: {}\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- Additional CLI args for nginx\n  extraArgs: {}\n  # -- Environment variables to add to the nginx pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the nginx pods\n  extraEnvFrom: []\n  # -- Volumes to add to the nginx pods\n  extraVolumes: []\n  # -- Volume mounts to add to the nginx pods\n  extraVolumeMounts: []\n  # -- The SecurityContext override for nginx containers\n  podSecurityContext: {}\n\n  # -- The SecurityContext for nginx containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n  # -- Resource requests and limits for the nginx\n  resources: {}\n  # -- Grace period to allow the nginx to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for nginx pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: \"\"\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  annotations: {}\n\n  # -- Node selector for nginx pods\n  nodeSelector: {}\n  # -- Tolerations for nginx pods\n  tolerations: []\n  # Nginx service configuration\n  service:\n    # -- Port of the nginx service\n    port: 80\n    # -- Type of the nginx service\n    type: ClusterIP\n    # -- ClusterIP of the nginx service\n    clusterIP: null\n    # -- Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IP address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the nginx service\n    annotations: {}\n    # -- Labels for nginx service\n    labels: {}\n  # Ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the nginx should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the nginx ingress\n    annotations: {}\n    # -- Hosts configuration for the nginx ingress\n    hosts:\n      - host: nginx.mimir.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the nginx ingress\n    tls:\n      - secretName: mimir-nginx-tls\n        hosts:\n          - nginx.mimir.example.com\n  # -- Route configuration (for OpenShift only)\n  route:\n    # -- Specifies whether an OpenShift route for the nginx should be created\n    enabled: false\n    # -- Annotations for the nginx route\n    annotations: {}\n    # -- Hostname configuration\n    host: nginx.mimir.example.com\n    # -- TLS configuration for OpenShift Route\n    tls:\n      # -- More details about TLS configuration and termination types: https://docs.openshift.com/container-platform/3.11/architecture/networking/routes.html#secured-routes\n      # For OpenShift 4: https://docs.openshift.com/container-platform/4.11/networking/routes/secured-routes.html\n      termination: edge\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for nginx\n    enabled: false\n    # -- The basic auth username for nginx\n    username: null\n    # -- The basic auth password for nginx\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'nginx.basicAuth.username' is required\" .Values.nginx.basicAuth.username) (required \"'nginx.basicAuth.password' is required\" .Values.nginx.basicAuth.password) }}\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for nginx\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http-metric\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n\n  # -- Additional containers to be added to the nginx pod.\n  extraContainers: []\n  # - name: dnsmasq\n  #   image: \"janeczku/go-dnsmasq:release-1.0.7\"\n  #   imagePullPolicy: IfNotPresent\n  #   args:\n  #     - --listen\n  #     - \"127.0.0.1:8053\"\n  #     - --hostsfile=/etc/hosts\n  #     - --enable-search\n  #     - --verbose\n\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Sets the log level of the NGINX error log. One of `debug`, `info`, `notice`, `warn`, `error`, `crit`, `alert`, or `emerg`\n    errorLogLevel: error\n    # -- Enables NGINX access logs\n    accessLogEnabled: true\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Allows to set a custom resolver\n    resolver: null\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr {{ .Values.nginx.nginxConfig.errorLogLevel }};\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.nginx.nginxConfig.logFormat }}\n\n        {{- if .Values.nginx.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   {{ .Values.nginx.nginxConfig.accessLogEnabled | ternary \"/dev/stderr  main  if=$loggable;\" \"off;\" }}\n        {{- end }}\n\n        sendfile           on;\n        tcp_nopush         on;\n        proxy_http_version 1.1;\n\n        {{- if .Values.nginx.nginxConfig.resolver }}\n        resolver {{ .Values.nginx.nginxConfig.resolver }};\n        {{- else }}\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n        {{- end }}\n\n        {{- with .Values.nginx.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.\n        map $http_x_scope_orgid $ensured_x_scope_orgid {\n          default $http_x_scope_orgid;\n          \"\" \"{{ include \"mimir.noAuthTenant\" . }}\";\n        }\n\n        map $http_x_scope_orgid $has_multiple_orgid_headers {\n          default 0;\n          \"~^.+,.+$\" 1;\n        }\n\n        proxy_read_timeout 300;\n        server {\n          listen 8080;\n          listen [::]:8080;\n\n          {{- if .Values.nginx.basicAuth.enabled }}\n          auth_basic           \"Mimir\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          if ($has_multiple_orgid_headers = 1) {\n              return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';\n          }\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n          # Distributor endpoints\n          location /distributor {\n            set $distributor {{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$distributor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location = /api/v1/push {\n            set $distributor {{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$distributor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location /otlp/v1/metrics {\n            set $distributor {{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$distributor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          # Alertmanager endpoints\n          location {{ template \"mimir.alertmanagerHttpPrefix\" . }} {\n            set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location = /multitenant_alertmanager/status {\n            set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location = /multitenant_alertmanager/configs {\n            set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location = /api/v1/alerts {\n            set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          # Ruler endpoints\n          location {{ template \"mimir.prometheusHttpPrefix\" . }}/config/v1/rules {\n            set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location {{ template \"mimir.prometheusHttpPrefix\" . }}/api/v1/rules {\n            set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          location {{ template \"mimir.prometheusHttpPrefix\" . }}/api/v1/alerts {\n            set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n          location = /ruler/ring {\n            set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          # Rest of {{ template \"mimir.prometheusHttpPrefix\" . }} goes to the query frontend\n          location {{ template \"mimir.prometheusHttpPrefix\" . }} {\n            set $query_frontend {{ template \"mimir.fullname\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$query_frontend:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          # Buildinfo endpoint can go to any component\n          location = /api/v1/status/buildinfo {\n            set $query_frontend {{ template \"mimir.fullname\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$query_frontend:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          # Compactor endpoint for uploading blocks\n          location /api/v1/upload/block/ {\n            set $compactor {{ template \"mimir.fullname\" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n            proxy_pass      http://$compactor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n          }\n\n          {{- with .Values.nginx.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\n# -- Use either this ingress or the gateway, but not both at once.\n# If you enable this, make sure to disable the gateway's ingress.\ningress:\n  enabled: false\n  # ingressClassName: nginx\n  annotations: {}\n  paths:\n    distributor-headless:\n      - path: /distributor\n        # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n        # pathType: Prefix\n      - path: /api/v1/push\n      - path: /otlp/v1/metrics\n    alertmanager-headless:\n      - path: /alertmanager\n      - path: /multitenant_alertmanager/status\n      - path: /multitenant_alertmanager/configs\n      - path: /api/v1/alerts\n    ruler:\n      - path: /prometheus/config/v1/rules\n      - path: /prometheus/api/v1/rules\n      - path: /prometheus/api/v1/alerts\n    query-frontend:\n      - path: /prometheus\n      - path: /api/v1/status/buildinfo\n    compactor:\n      - path: /api/v1/upload/block/\n  hosts:\n    - mimir.example.com\n  # tls:\n  #   - secretName: mimir-distributed-tls\n  #     hosts:\n  #       - mimir.example.com\n\n# -- A reverse proxy deployment that is meant to receive traffic for Mimir or GEM.\n# When enterprise.enabled is true the GEM gateway is deployed. Otherwise, it is an nginx.\n# Options except those under gateway.nginx apply to both versions - nginx and GEM gateway.\ngateway:\n  # -- The gateway is deployed by default for enterprise installations (enterprise.enabled=true).\n  # Toggle this to have it deployed for non-enterprise installations too.\n  enabledNonEnterprise: false\n\n  # -- Number of replicas for the Deployment\n  replicas: 1\n\n  # -- HorizontalPodAutoscaler\n  autoscaling:\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 3\n    targetCPUUtilizationPercentage: 70\n    targetMemoryUtilizationPercentage: 70\n\n  # -- Deployment strategy. See `kubectl explain deployment.spec.strategy` for more,\n  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  # -- The name of the PriorityClass\n  priorityClassName: null\n  # -- Labels for Deployment Pods\n  podLabels: {}\n  # -- Annotations Deployment Pods\n  podAnnotations: {}\n  # -- PodDisruptionBudget https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- Additional CLI args for the container\n  extraArgs: {}\n  # -- Environment variables to add to the Pods. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/\n  env: []\n  # -- Environment variables from secrets or configmaps to add to the Pods.\n  extraEnvFrom: []\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n  # -- Volumes to add to the Pods\n  extraVolumes: []\n  # -- Volume mounts to add to the Pods\n  extraVolumeMounts: []\n  # -- Additional containers to be added to the Pods.\n  extraContainers: []\n  # - name: dnsmasq\n  #   image: \"janeczku/go-dnsmasq:release-1.0.7\"\n  #   imagePullPolicy: IfNotPresent\n  #   args:\n  #     - --listen\n  #     - \"127.0.0.1:8053\"\n  #     - --hostsfile=/etc/hosts\n  #     - --enable-search\n  #     - --verbose\n\n  # -- Init containers https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\n  initContainers: []\n\n  # -- SecurityContext override for gateway pods\n  securityContext: {}\n  # -- The SecurityContext for gateway containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  # -- Resource requests and limits for the container\n  resources: {}\n  # -- Grace period to allow the gateway container to shut down before it is killed\n  terminationGracePeriodSeconds: 30\n\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  # Annotations for the Deployment\n  annotations: {}\n\n  # -- Node selector for Deployment Pods\n  nodeSelector: {}\n  # -- Tolerations for Deployment Pods\n  tolerations: []\n  # -- Gateway Service configuration\n  service:\n    # -- Port on which the Service listens\n    port: 80\n    # -- Type of the Service\n    type: ClusterIP\n    # -- ClusterIP of the Service\n    clusterIP: null\n    # -- Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IP address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the Service\n    annotations: {}\n    # -- Labels for the Service\n    labels: {}\n    # -- DEPRECATED Legacy compatibility port the GEM gateway service listens on, set to 'null' to disable\n    legacyPort: 8080\n    # -- Overrides the name of the Service. Useful if you are switching from the deprecated nginx or\n    # GEM gateway configuration and want to use the same in-cluster address for Mimir/GEM.\n    # By using the same name as the nginx/GEM gateway Service, Helm will not delete the Service Resource.\n    # Instead, it will update the existing one in place.\n    # If left as an empty string, a name is generated.\n    nameOverride: \"\"\n\n  ingress:\n    enabled: false\n    # -- Overrides the name of the Ingress. Useful if you are switching from the deprecated nginx or\n    # GEM gateway configuration and you Ingress Controller needs time to reconcile a new Ingress resource.\n    # By using the same name as the nginx/GEM gateway Ingress, Helm will not delete the Ingress Resource.\n    # Instead, it will update the existing one in place.\n    # If left as an empty string, a name is generated.\n    nameOverride: \"\"\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    ingressClassName: \"\"\n    # -- Annotations for the Ingress\n    annotations: {}\n    # -- Hosts configuration for the Ingress\n    hosts:\n      # -- Passed through the `tpl` function to allow templating.\n      - host: \"{{ .Release.Name }}.mimir.example.com\"\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the nginx ingress\n    tls:\n      - secretName: mimir-tls\n        # --  Hosts included in the tls certificate. Passed through the `tpl` function to allow templating.\n        hosts:\n          - \"{{ .Release.Name }}.mimir.example.com\"\n\n  # -- OpenShift Route configuration\n  route:\n    enabled: false\n    # -- Annotations for the Route\n    annotations: {}\n    # -- Passed through the `tpl` function to allow templating.\n    host: \"{{ .Release.Name }}.mimir.example.com\"\n\n    tls:\n      # -- More details about TLS configuration and termination types: https://docs.openshift.com/container-platform/3.11/architecture/networking/routes.html#secured-routes\n      # For OpenShift 4: https://docs.openshift.com/container-platform/4.11/networking/routes/secured-routes.html\n      termination: edge\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n\n  nginx:\n    # -- Enable logging of 2xx and 3xx HTTP requests\n    verboseLogging: true\n\n    # -- Image for the nginx. pullPolicy and optional pullSecrets are set in toplevel 'image' section, not here.\n    image:\n      # -- The Docker registry for nginx image\n      registry: docker.io\n      # -- The nginx image repository\n      repository: nginxinc/nginx-unprivileged\n      # -- The nginx image tag\n      tag: 1.27-alpine\n\n    # -- Basic auth configuration\n    basicAuth:\n      # -- Enables basic authentication for nginx\n      enabled: false\n      # -- The basic auth username for nginx\n      username: null\n      # -- The basic auth password for nginx\n      password: null\n      # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n      # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n      # high CPU load.\n      htpasswd: \u003e-\n        {{ htpasswd (required \"'gateway.nginx.basicAuth.username' is required\" .Values.gateway.nginx.basicAuth.username) (required \"'gateway.nginx.basicAuth.password' is required\" .Values.gateway.nginx.basicAuth.password) }}\n      # -- Name of an existing basic auth secret to use instead of gateway.nginx.basicAuth.htpasswd. Must contain '.htpasswd' key\n      existingSecret: null\n\n    config:\n      # -- NGINX log format\n      logFormat: |-\n        main '$remote_addr - $remote_user [$time_local]  $status '\n                '\"$request\" $body_bytes_sent \"$http_referer\" '\n                '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n      # -- Sets the log level of the NGINX error log. One of `debug`, `info`, `notice`, `warn`, `error`, `crit`, `alert`, or `emerg`\n      errorLogLevel: error\n      # -- Enables NGINX access logs\n      accessLogEnabled: true\n      # -- Allows appending custom configuration to the server block\n      serverSnippet: \"\"\n      # -- Allows appending custom configuration to the http block\n      httpSnippet: \"\"\n      # -- Allow to set client_max_body_size in the nginx configuration\n      clientMaxBodySize: 540M\n      # -- Allows to set a custom resolver\n      resolver: null\n      # -- Configures whether or not NGINX bind IPv6\n      enableIPv6: true\n      # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating.\n      file: |\n        worker_processes  5;  ## Default: 1\n        error_log  /dev/stderr {{ .Values.gateway.nginx.config.errorLogLevel }};\n        pid        /tmp/nginx.pid;\n        worker_rlimit_nofile 8192;\n\n        events {\n          worker_connections  4096;  ## Default: 1024\n        }\n\n        http {\n          client_body_temp_path /tmp/client_temp;\n          proxy_temp_path       /tmp/proxy_temp_path;\n          fastcgi_temp_path     /tmp/fastcgi_temp;\n          uwsgi_temp_path       /tmp/uwsgi_temp;\n          scgi_temp_path        /tmp/scgi_temp;\n\n          default_type application/octet-stream;\n          log_format   {{ .Values.gateway.nginx.config.logFormat }}\n\n          {{- if .Values.gateway.nginx.verboseLogging }}\n          access_log   /dev/stderr  main;\n          {{- else }}\n\n          map $status $loggable {\n            ~^[23]  0;\n            default 1;\n          }\n          access_log   {{ .Values.gateway.nginx.config.accessLogEnabled | ternary \"/dev/stderr  main  if=$loggable;\" \"off;\" }}\n          {{- end }}\n\n          sendfile           on;\n          tcp_nopush         on;\n          proxy_http_version 1.1;\n\n          {{- if .Values.gateway.nginx.config.resolver }}\n          resolver {{ .Values.gateway.nginx.config.resolver }};\n          {{- else }}\n          resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n          {{- end }}\n\n          {{- with .Values.gateway.nginx.config.httpSnippet }}\n          {{ . | nindent 2 }}\n          {{- end }}\n\n          # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.\n          map $http_x_scope_orgid $ensured_x_scope_orgid {\n            default $http_x_scope_orgid;\n            \"\" \"{{ include \"mimir.noAuthTenant\" . }}\";\n          }\n\n          map $http_x_scope_orgid $has_multiple_orgid_headers {\n            default 0;\n            \"~^.+,.+$\" 1;\n          }\n\n          proxy_read_timeout 300;\n          server {\n            listen {{ include \"mimir.serverHttpListenPort\" . }};\n            {{- if .Values.gateway.nginx.config.enableIPv6 }}\n            listen [::]:{{ include \"mimir.serverHttpListenPort\" . }};\n            {{- end }}\n\n            {{- if .Values.gateway.nginx.config.clientMaxBodySize }}\n            client_max_body_size {{ .Values.gateway.nginx.config.clientMaxBodySize }};\n            {{- end }}\n\n            {{- if .Values.gateway.nginx.basicAuth.enabled }}\n            auth_basic           \"Mimir\";\n            auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n            {{- end }}\n\n            if ($has_multiple_orgid_headers = 1) {\n                return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';\n            }\n\n            location = / {\n              return 200 'OK';\n              auth_basic off;\n            }\n\n            location = /ready {\n              return 200 'OK';\n              auth_basic off;\n            }\n\n            proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n            # Distributor endpoints\n            location /distributor {\n              set $distributor {{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$distributor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location = /api/v1/push {\n              set $distributor {{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$distributor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location /otlp/v1/metrics {\n              set $distributor {{ template \"mimir.fullname\" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$distributor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            # Alertmanager endpoints\n            location {{ template \"mimir.alertmanagerHttpPrefix\" . }} {\n              set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location = /multitenant_alertmanager/status {\n              set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location = /multitenant_alertmanager/configs {\n              set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location = /api/v1/alerts {\n              set $alertmanager {{ template \"mimir.fullname\" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$alertmanager:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            # Ruler endpoints\n            location {{ template \"mimir.prometheusHttpPrefix\" . }}/config/v1/rules {\n              set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location {{ template \"mimir.prometheusHttpPrefix\" . }}/api/v1/rules {\n              set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            location {{ template \"mimir.prometheusHttpPrefix\" . }}/api/v1/alerts {\n              set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n            location = /ruler/ring {\n              set $ruler {{ template \"mimir.fullname\" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$ruler:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            # Rest of {{ template \"mimir.prometheusHttpPrefix\" . }} goes to the query frontend\n            location {{ template \"mimir.prometheusHttpPrefix\" . }} {\n              set $query_frontend {{ template \"mimir.fullname\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$query_frontend:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            # Buildinfo endpoint can go to any component\n            location = /api/v1/status/buildinfo {\n              set $query_frontend {{ template \"mimir.fullname\" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$query_frontend:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            # Compactor endpoint for uploading blocks\n            location /api/v1/upload/block/ {\n              set $compactor {{ template \"mimir.fullname\" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};\n              proxy_pass      http://$compactor:{{ include \"mimir.serverHttpListenPort\" . }}$request_uri;\n            }\n\n            {{- with .Values.gateway.nginx.config.serverSnippet }}\n            {{ . | nindent 4 }}\n            {{- end }}\n          }\n        }\n\nmetaMonitoring:\n  # Dashboard configuration for deploying Grafana dashboards for Mimir\n  dashboards:\n    # -- If enabled, Grafana dashboards are deployed\n    enabled: false\n    # -- Alternative namespace to create dashboards ConfigMaps in. They are created in the Helm release namespace by default.\n    namespace: null\n    # -- Annotations to add to the Grafana dashboard ConfigMap\n    annotations:\n      k8s-sidecar-target-directory: /tmp/dashboards/Mimir Dashboards\n    # -- Labels to add to the Grafana dashboard ConfigMap\n    labels:\n      grafana_dashboard: \"1\"\n\n  # ServiceMonitor configuration for monitoring Kubernetes Services with Prometheus Operator and/or Grafana Agent\n  serviceMonitor:\n    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n    enabled: true\n    # -- To disable setting a 'cluster' label in metrics, set to 'null'.\n    # To overwrite the 'cluster' label with your own value, set to a non-empty string.\n    # Keep empty string \"\" to have the default value in the 'cluster' label, which is the helm release name for Mimir and the actual cluster name for Enterprise Metrics.\n    clusterLabel: \"\"\n    # -- Alternative namespace for ServiceMonitor resources\n    # If left unset, the default is to install the ServiceMonitor resources in the namespace where the chart is installed, i.e. the namespace specified for the helm command.\n    namespace: null\n    # -- Namespace selector for ServiceMonitor resources\n    # If left unset, the default is to select the namespace where the chart is installed, i.e. the namespace specified for the helm command.\n    namespaceSelector: null\n    # -- ServiceMonitor annotations\n    annotations: {}\n    # -- Additional ServiceMonitor labels\n    labels: {}\n    # -- ServiceMonitor scrape interval\n    interval: null\n    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n    scrapeTimeout: null\n    # -- ServiceMonitor relabel configs to apply to targets before scraping\n    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig\n    relabelings: []\n    # -- ServiceMonitor metric relabel configs to apply to samples before ingestion\n    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig\n    metricRelabelings: []\n    # -- ServiceMonitor will use http by default, but you can pick https as well\n    scheme: http\n    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n    tlsConfig: null\n\n  # Rules for the Prometheus Operator\n  prometheusRule:\n    # -- If enabled, a PrometheusRule resource for Prometheus Operator is created\n    enabled: false\n    # -- Create standard Mimir alerts in Prometheus Operator via a PrometheusRule CRD\n    mimirAlerts: false\n    # -- Create standard Mimir recording rules in Prometheus Operator via a PrometheusRule CRD\n    mimirRules: false\n    # -- PrometheusRule annotations\n    annotations: {}\n    # -- Additional PrometheusRule labels. To find out what your Prometheus operator expects,\n    # see the Prometheus object and field spec.ruleSelector\n    labels: {}\n    # -- prometheusRule namespace. This should be the namespace where the Prometheus Operator is installed,\n    # unless the Prometheus Operator is set up to look for rules outside its namespace\n    namespace: null\n    # -- Contents of Prometheus rules file\n    groups: []\n  #  - name: mimir_api_1\n  #    rules:\n  #    - expr: histogram_quantile(0.99, sum(rate(cortex_request_duration_seconds_bucket[1m]))\n  #        by (le, cluster, job))\n  #      record: cluster_job:cortex_request_duration_seconds:99quantile\n  #    - expr: histogram_quantile(0.50, sum(rate(cortex_request_duration_seconds_bucket[1m]))\n  #        by (le, cluster, job))\n  #      record: cluster_job:cortex_request_duration_seconds:50quantile\n  #    - expr: sum(rate(cortex_request_duration_seconds_sum[1m])) by (cluster, job) / sum(rate(cortex_request_duration_seconds_count[1m]))\n  #        by (cluster, job)\n  #      record: cluster_job:cortex_request_duration_seconds:avg\n  #    - expr: sum(rate(cortex_request_duration_seconds_bucket[1m])) by (le, cluster, job)\n  #      record: cluster_job:cortex_request_duration_seconds_bucket:sum_rate\n  #    - expr: sum(rate(cortex_request_duration_seconds_sum[1m])) by (cluster, job)\n  #      record: cluster_job:cortex_request_duration_seconds_sum:sum_rate\n  #    - expr: sum(rate(cortex_request_duration_seconds_count[1m])) by (cluster, job)\n  #      record: cluster_job:cortex_request_duration_seconds_count:sum_rate\n\n  # metaMonitoringAgent configures the built in Grafana Agent that can scrape metrics and logs and send them to a local or remote destination\n  grafanaAgent:\n    # -- Controls whether to create PodLogs, MetricsInstance, LogsInstance, and GrafanaAgent CRs to scrape the\n    # ServiceMonitors of the chart and ship metrics and logs to the remote endpoints below.\n    # Note that you need to configure serviceMonitor in order to have some metrics available.\n    enabled: true\n\n    # -- Controls the image repository and tag for config-reloader and grafana-agent containers in the meta-monitoring\n    # StatefulSet and DaemonSet created by the grafana-agent-operator. You can define one or both sections under imageRepo.\n    # If a section is defined, you must pass repo, image and tag keys.\n    imageRepo:\n    #  configReloader:\n    #    repo: quay.io\n    #    image: prometheus-operator/prometheus-config-reloader\n    #    tag: v0.47.0\n    #  grafanaAgent:\n    #    repo: docker.io\n    #    image: grafana/agent\n    #    tag: v0.29.0\n\n    # -- Resource requests and limits for the grafana-agent and config-reloader containers in the meta-monitoring\n    # StatefulSet and DaemonSet created by the grafana-agent-operator. You can define one or both sections under resources.\n    resources:\n    #   configReloader:\n    #     requests:\n    #       cpu: 5m\n    #       memory: 10Mi\n    #     limits:\n    #       memory: 50Mi\n    #   grafanaAgent:\n    #     requests:\n    #       cpu: 20m\n    #       memory: 700Mi\n    #     limits:\n    #       memory: 1400Mi\n\n    # -- Controls whether to install the Grafana Agent Operator and its CRDs.\n    # Note that helm will not install CRDs if this flag is enabled during an upgrade.\n    # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/operations/agent-static-operator/crds\n    installOperator: true\n\n    logs:\n      # -- Controls whether to create resources PodLogs and LogsInstance resources\n      enabled: true\n\n      # -- To disable setting a 'cluster' label in logs, set to 'null'.\n      # To overwrite the 'cluster' label with your own value, set to a non-empty string.\n      # Keep empty string \"\" to have the default value in the 'cluster' label, which is the helm release name for Mimir and the actual cluster name for Enterprise Metrics.\n      clusterLabel: \"\"\n\n      # -- Default destination for logs. The config here is translated to Promtail client\n      # configuration to write logs to this Loki-compatible remote. Optional.\n      remote:\n        # -- Full URL for Loki push endpoint. Usually ends in /loki/api/v1/push\n        url: \"\"\n\n        auth:\n          # -- Used to set X-Scope-OrgID header on requests. Usually not used in combination with username and password.\n          tenantId: \"\"\n\n          # -- Basic authentication username. Optional.\n          username: \"\"\n\n          # -- The value under key passwordSecretKey in this secret will be used as the basic authentication password. Required only if passwordSecretKey is set.\n          passwordSecretName: \"\"\n          # -- The value under this key in passwordSecretName will be used as the basic authentication password. Required only if passwordSecretName is set.\n          passwordSecretKey: \"\"\n\n      # -- Client configurations for the LogsInstance that will scrape Mimir pods. Follows the format of .remote.\n      additionalClientConfigs: []\n\n    metrics:\n      # -- Controls whether to create MetricsInstance resources and ServiceMonitor resources for scraping Kubernetes (when .scrapeK8s.enabled=true).\n      enabled: true\n\n      # -- Default destination for metrics. The config here is translated to remote_write\n      # configuration to push metrics to this Prometheus-compatible remote. Optional.\n      # Note that you need to configure serviceMonitor in order to have some metrics available.\n      #\n      # If you leave the metaMonitoring.grafanaAgent.metrics.remote.url field empty,\n      # then the chart automatically fills in the address of the GEM gateway Service\n      # or the Mimir NGINX Service.\n      #\n      # If you have deployed Mimir, and metaMonitoring.grafanaAgent.metrics.remote.url is not set,\n      # then the metamonitoring metrics are be sent to the Mimir cluster.\n      # You can query these metrics using the HTTP header X-Scope-OrgID: metamonitoring\n      #\n      # If you have deployed GEM, then there are two cases:\n      # * If are using the 'trust' authentication type (mimir.structuredConfig.auth.type: trust),\n      #   then the same instructions apply as for Mimir.\n      #\n      # * If you are using the enterprise authentication type (mimir.structuredConfig.auth.type=enterprise, which is also the default when enterprise.enabled=true),\n      #   then you also need to provide a Secret with the authentication token for the tenant.\n      #   The token should be to an access policy with metrics:read scope.\n      #   To set up the Secret, refer to https://grafana.com/docs/helm-charts/mimir-distributed/latest/run-production-environment-with-helm/monitor-system-health/\n      #   Assuming you are using the GEM authentication model, the Helm chart values should look like the following example.\n      #\n      # remote:\n      #   auth:\n      #     username: metamonitoring\n      #     passwordSecretName: gem-tokens\n      #     passwordSecretKey: metamonitoring\n      remote:\n        # -- Full URL for Prometheus remote-write. Usually ends in /push.\n        # If you leave the url field empty, then the chart automatically fills in the\n        # address of the GEM gateway Service or the Mimir NGINX Service.\n        url: \"\"\n\n        # -- Used to add HTTP headers to remote-write requests.\n        headers: {}\n        auth:\n          # -- Basic authentication username. Optional.\n          username: \"\"\n\n          # -- The value under key passwordSecretKey in this secret will be used as the basic authentication password. Required only if passwordSecretKey is set.\n          passwordSecretName: \"\"\n          # -- The value under this key in passwordSecretName will be used as the basic authentication password. Required only if passwordSecretName is set.\n          passwordSecretKey: \"\"\n\n        # -- Configuration for SigV4 authentication.\n        # sigv4:\n        #   accessKey: abcd\n        #   profile: default\n        #   region: us-east-1\n        #   roleARN: arn:aws:iam::1234:role/1234\n        #   secretKey: abcd\n        sigv4: {}\n\n      # -- Additional remote-write for the MetricsInstance that will scrape Mimir pods. Follows the format of .remote.\n      additionalRemoteWriteConfigs:\n        - url: \"http://mimir-nginx.mimir-test.svc:80/api/v1/push\"\n\n      scrapeK8s:\n        # -- When grafanaAgent.enabled and serviceMonitor.enabled, controls whether to create ServiceMonitors CRs\n        # for cadvisor, kubelet, and kube-state-metrics. The scraped metrics are reduced to those pertaining to\n        # Mimir pods only.\n        enabled: true\n\n        # -- Controls service discovery of kube-state-metrics.\n        kubeStateMetrics:\n          namespace: kube-system\n          labelSelectors:\n            app.kubernetes.io/name: kube-state-metrics\n          service:\n            port: http-metrics\n\n      # -- The scrape interval for all ServiceMonitors.\n      scrapeInterval: 60s\n\n    # -- Sets the namespace of the resources. Leave empty or unset to use the same namespace as the Helm release.\n    namespace: \"\"\n\n    # -- Labels to add to all monitoring.grafana.com custom resources.\n    # Does not affect the ServiceMonitors for kubernetes metrics; use serviceMonitor.labels for that.\n    labels: {}\n\n    # -- Annotations to add to all monitoring.grafana.com custom resources.\n    # Does not affect the ServiceMonitors for kubernetes metrics; use serviceMonitor.annotations for that.\n    annotations: {}\n\n    # -- SecurityContext of Grafana Agent pods. This is different from the SecurityContext that the operator pod runs with.\n    # The operator pod SecurityContext is configured in the grafana-agent-operator.podSecurityContext value.\n    # As of mimir-distributed 4.0.0 the Agent DaemonSet that collects logs needs to run as root and be able to access the\n    # pod logs on each host. Because of that the agent subchart is incompatible with the PodSecurityPolicy of the\n    # mimir-distributed chart and with the Restricted policy of Pod Security Standards https://kubernetes.io/docs/concepts/security/pod-security-standards/\n    podSecurityContext:\n    #  fsGroup: 10001\n    #  runAsGroup: 10001\n    #  runAsNonRoot: true\n    #  runAsUser: 10001\n    #  seccompProfile:\n    #    type: RuntimeDefault\n\n    # -- SecurityContext of Grafana Agent containers. This is different from the SecurityContext that the operator container runs with.\n    # As of mimir-distributed 4.0.0 the agent subchart needs to have root file system write access so that the Agent pods can write temporary files where.\n    # This makes the subchart incompatible with the PodSecurityPolicy of the mimir-distributed chart.\n    containerSecurityContext:\n    #  allowPrivilegeEscalation: false\n    #  runAsUser: 10001\n    #  capabilities:\n    #    drop: [ALL]\n\n    # -- Node selector for Deployment Pods\n    nodeSelector: {}\n    # -- Tolerations for Deployment Pods\n    tolerations: []\n\n    # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints.\n    # More info: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topologyspreadconstraints-field\n    topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: kubernetes.io/hostname\n        whenUnsatisfiable: ScheduleAnyway\n        # minDomains: 1\n        # nodeAffinityPolicy: Honor\n        # nodeTaintsPolicy: Honor\n        # matchLabelKeys:\n        #   - pod-template-hash\n\n# -- Values exposed by the [Grafana agent-operator chart](https://github.com/grafana/helm-charts/blob/main/charts/agent-operator/values.yaml)\ngrafana-agent-operator:\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n    seccompProfile:\n      type: RuntimeDefault\n\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n##############################################################################\n# The values in and after the `enterprise:` key configure the enterprise features\nenterprise:\n  # Enable enterprise features. License must be provided, nginx gateway is not installed, instead\n  # the enterprise gateway is used.\n  enabled: false\n\n  # Whether to generate pre-2.0 Grafana Enterprise Metrics resource labels and selectors, or generate new Kubernetes standard selectors.\n  # Rolling upgrade from version 1.7.x without downtime requires this setting to be true. Fresh installation or upgrade with downtime can set\n  # it to false.\n  legacyLabels: false\n\n  image:\n    # -- Grafana Enterprise Metrics container image repository. Note: for Grafana Mimir use the value 'image.repository'\n    repository: grafana/enterprise-metrics\n    # -- Grafana Enterprise Metrics container image tag. Note: for Grafana Mimir use the value 'image.tag'\n    tag: v2.14.0\n    # Note: pullPolicy and optional pullSecrets are set in toplevel 'image' section, not here\n\n# In order to use Grafana Enterprise Metrics features, you will need to provide the contents of your Grafana Enterprise Metrics\n# license, either by providing the contents of the license.jwt, or the name Kubernetes Secret that contains your license.jwt.\n# To set the license contents, use the flag `--set-file 'license.contents=./license.jwt'`\n# To use your own Kubernetes Secret, `--set license.external=true`.\nlicense:\n  contents: \"NOTAVALIDLICENSE\"\n  external: false\n  secretName: '{{ include \"mimir.resourceName\" (dict \"ctx\" . \"component\" \"license\") }}'\n\n# Settings for the initial admin(istrator) token generator job. Can only be enabled if\n# enterprise.enabled is true - requires license.\ntokengenJob:\n  enable: true\n  extraArgs: {}\n  env: []\n  extraEnvFrom: []\n  annotations: {}\n  initContainers: []\n\n  # -- SecurityContext override for tokengenjob pods\n  securityContext: {}\n\n  # -- The SecurityContext for tokengenjob containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  # -- The name of the PriorityClass for tokenjobgen pods\n  priorityClassName: null\n\n# Settings for the admin_api service providing authentication and authorization service.\n# Can only be enabled if enterprise.enabled is true - requires license.\nadmin_api:\n  replicas: 1\n\n  annotations: {}\n  service:\n    annotations: {}\n    labels: {}\n\n  initContainers: []\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 15%\n\n  podLabels: {}\n  podAnnotations: {}\n\n  nodeSelector: {}\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints:\n    maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: ScheduleAnyway\n    # minDomains: 1\n    # nodeAffinityPolicy: Honor\n    # nodeTaintsPolicy: Honor\n    # matchLabelKeys:\n    #   - pod-template-hash\n\n  # Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n\n  # -- The name of the PriorityClass for admin-api pods\n  priorityClassName: null\n\n  # -- SecurityContext override for admin-api pods\n  securityContext: {}\n\n  # -- The SecurityContext for admin_api containers\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n\n  extraArgs: {}\n\n  persistence:\n    subPath:\n\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n\n  resources:\n    requests:\n      cpu: 10m\n      memory: 32Mi\n\n  terminationGracePeriodSeconds: 60\n\n  tolerations: []\n  extraContainers: []\n  extraVolumes: []\n  extraVolumeMounts: []\n  env: []\n  extraEnvFrom: []\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n\n# -- Cache for admin bucket.\n# If this is disabled, in-memory cache will be set by default.\n# You can use Redis too for cache and set the configuration via structuredConfig.\n# See GEM documentation for Redis configuration option.\nadmin-cache:\n  # -- Specifies whether admin-cache using memcached should be enabled\n  enabled: false\n\n  # -- Total number of admin-cache replicas\n  replicas: 1\n\n  # -- Port of the admin-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to admin-cache for object storage (in MB).\n  allocatedMemory: 64\n\n  # -- Maximum item memory for admin-cache (in MB).\n  maxItemMemory: 1\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for admin-cache pods\n  initContainers: []\n\n  # -- Annotations for the admin-cache pods\n  annotations: {}\n  # -- Node selector for admin-cache pods\n  nodeSelector: {}\n  # -- Affinity for admin-cache pods\n  affinity: {}\n\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: {}\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  #  minDomains: 1\n  #  nodeAffinityPolicy: Honor\n  #  nodeTaintsPolicy: Honor\n  #  matchLabelKeys:\n  #    - pod-template-hash\n\n  # -- Tolerations for admin-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for admin-cache pods\n  priorityClassName: null\n  # -- Labels for admin-cache pods\n  podLabels: {}\n  # -- Annotations for admin-cache pods\n  podAnnotations: {}\n  # -- Management policy for admin-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the admin-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n\n  # -- Stateful admin-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Additional CLI args for admin-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the admin-cache pod.\n  extraContainers: []\n\n  # -- Resource requests and limits for the admin-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\ngraphite:\n  # -- If true, enables graphite querier and graphite write proxy functionality.\n  # Read more in https://grafana.com/docs/enterprise-metrics/latest/graphite/\n  enabled: false\n\n  querier:\n    # Setting it to null will produce a deployment without replicas set, allowing you to use autoscaling with the deployment\n    replicas: 2\n\n    schemasConfiguration:\n      storageSchemas: |-\n        [default]\n        pattern = .*\n        intervals = 0:1s\n        retentions = 10s:8d,10min:1y\n      storageAggregations: |-\n        [default]\n        aggregationMethod = avg\n        pattern = .*\n        xFilesFactor = 0.1\n\n    service:\n      annotations: {}\n      labels: {}\n\n    # -- Resources for graphite-querier pods\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n\n    # -- Additional graphite-querier container arguments, e.g. log level (debug, info, warn, error)\n    extraArgs: {}\n    # -- The name of the PriorityClass for graphite-querier pods\n    priorityClassName: null\n    # -- Labels for graphite-querier pods\n    podLabels: {}\n    # -- Annotations for graphite-querier pods\n    podAnnotations: {}\n\n    nodeSelector: {}\n    affinity:\n      podAntiAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: app.kubernetes.io/component\n                    operator: In\n                    values:\n                      - graphite-querier\n              topologyKey: \"kubernetes.io/hostname\"\n\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: http-metrics\n      initialDelaySeconds: 45\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: http-metrics\n      initialDelaySeconds: 45\n\n    annotations: {}\n    persistence:\n      subPath:\n\n    # -- SecurityContext override for graphite querier pods\n    securityContext: {}\n\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop: [ALL]\n\n    strategy:\n      type: RollingUpdate\n      rollingUpdate:\n        maxUnavailable: 0\n        maxSurge: 15%\n\n    terminationGracePeriodSeconds: 180\n\n    env: []\n    extraEnvFrom: []\n    # -- Jaeger reporter queue size\n    # Set to 'null' to use the Jaeger client's default value\n    jaegerReporterMaxQueueSize: null\n    tolerations: []\n    podDisruptionBudget:\n      maxUnavailable: 1\n    initContainers: []\n    extraContainers: []\n    extraVolumes: []\n    extraVolumeMounts: []\n\n  write_proxy:\n    replicas: 2\n\n    service:\n      annotations: {}\n      labels: {}\n\n    # -- Resources for graphite-write-proxy pods\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n\n    # -- Additional graphite-write-proxy container arguments, e.g. log level (debug, info, warn, error)\n    extraArgs: {}\n    # -- The name of the PriorityClass for graphite-write-proxy pods\n    priorityClassName: null\n    # -- Labels for graphite-write-proxy pods\n    podLabels: {}\n    # -- Annotations for graphite-write-proxy pods\n    podAnnotations: {}\n    # -- Node selector for graphite-write-proxy pods\n    nodeSelector: {}\n    # -- Affinity for graphite-write-proxy pods\n    affinity:\n      podAntiAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: app.kubernetes.io/component\n                    operator: In\n                    values:\n                      - graphite-write-proxy\n              topologyKey: \"kubernetes.io/hostname\"\n\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: http-metrics\n      initialDelaySeconds: 45\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: http-metrics\n      initialDelaySeconds: 45\n\n    annotations: {}\n    persistence:\n      subPath:\n\n    # -- SecurityContext override for graphite write-proxy pods\n    securityContext: {}\n\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop: [ALL]\n\n    strategy:\n      type: RollingUpdate\n      rollingUpdate:\n        maxUnavailable: 0\n        maxSurge: 15%\n\n    # -- Grace period to allow the graphite-write-proxy to shutdown before it is killed\n    terminationGracePeriodSeconds: 180\n    env: []\n    extraEnvFrom: []\n    # -- Jaeger reporter queue size\n    # Set to 'null' to use the Jaeger client's default value\n    jaegerReporterMaxQueueSize: null\n    tolerations: []\n    podDisruptionBudget:\n      maxUnavailable: 1\n    initContainers: []\n    extraContainers: []\n    extraVolumes: []\n    extraVolumeMounts: []\n\n# Graphite's aggregation cache. If you want to know more about it please check\n# https://grafana.com/docs/enterprise-metrics/latest/graphite/graphite_querier/#aggregation-cache\ngr-aggr-cache:\n  # -- Specifies whether memcached based graphite-aggregation-cache should be enabled. Note that the cache will only appear if graphite is also enabled.\n  enabled: true\n\n  # -- Total number of graphite-aggregation-cache replicas\n  replicas: 1\n\n  # -- Port of the graphite-aggregation-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to graphite-aggregation-cache for object storage (in MB).\n  allocatedMemory: 8192\n\n  # -- Maximum item memory for graphite-aggregation-cache (in MB).\n  maxItemMemory: 1\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for graphite-aggregation-cache pods\n  initContainers: []\n\n  # -- Annotations for the graphite-aggregation-cache pods\n  annotations: {}\n  # -- Node selector for graphite-aggregation-cache pods\n  nodeSelector: {}\n  # -- Affinity for graphite-aggregation-cache pods\n  affinity: {}\n  # -- Tolerations for graphite-aggregation-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for graphite-aggregation-cache pods\n  priorityClassName: null\n  # -- Labels for graphite-aggregation-cache pods\n  podLabels: {}\n  # -- Annotations for graphite-aggregation-cache pods\n  podAnnotations: {}\n  # -- Management policy for graphite-aggregation-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the graphite-aggregation-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n\n  # -- Stateful graphite-aggregation-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Additional CLI args for graphite-aggregation-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the graphite-aggregation-cache pod.\n  extraContainers: []\n\n  # -- Resource requests and limits for the graphite-aggregation-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\n# Graphite's metric name cache. If you want to know more about it please check\n# https://grafana.com/docs/enterprise-metrics/latest/graphite/graphite_querier/#metric-name-cache\ngr-metricname-cache:\n  # -- Specifies whether memcached based graphite-metric-name-cache should be enabled. Note that the cache will only appear if graphite is also enabled.\n  enabled: true\n\n  # -- Total number of graphite-metric-name-cache replicas\n  replicas: 1\n\n  # -- Port of the graphite-metric-name-cache service\n  port: 11211\n\n  # -- Amount of memory allocated to graphite-metric-name-cache for object storage (in MB).\n  allocatedMemory: 8192\n\n  # -- Maximum item memory for graphite-metric-name-cache (in MB).\n  maxItemMemory: 1\n\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n\n  # -- Extra init containers for graphite-metric-name-cache pods\n  initContainers: []\n\n  # -- Annotations for the graphite-metric-name-cache pods\n  annotations: {}\n  # -- Node selector for graphite-metric-name-cache pods\n  nodeSelector: {}\n  # -- Affinity for graphite-metric-name-cache pods\n  affinity: {}\n  # -- Tolerations for graphite-metric-name-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget\n  podDisruptionBudget:\n    maxUnavailable: 1\n  # -- The name of the PriorityClass for graphite-metric-name-cache pods\n  priorityClassName: null\n  # -- Labels for graphite-metric-name-cache pods\n  podLabels: {}\n  # -- Annotations for graphite-metric-name-cache pods\n  podAnnotations: {}\n  # -- Management policy for graphite-metric-name-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the graphite-metric-name-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n\n  # -- Stateful graphite-metric-name-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n\n  # -- Additional CLI args for graphite-metric-name-cache\n  extraArgs: {}\n\n  # -- Additional containers to be added to the graphite-metric-name-cache pod.\n  extraContainers: []\n\n  # -- Resource requests and limits for the graphite-metric-name-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n\n# -- Settings for the smoke-test job. This is meant to run as a Helm test hook\n# (`helm test RELEASE`) after installing the chart. It quickly verifies\n# that writing and reading metrics works. Currently not supported for\n# installations using GEM token-based authentication.\nsmoke_test:\n  # -- Controls the backoffLimit on the Kubernetes Job. The Job is marked as failed after that many retries.\n  backoffLimit: 5\n  # The image section has been removed as continuous test is now a module of the regular Mimir image.\n  # See settings for the image at the top image section.\n  tenantId: \"\"\n  extraArgs: {}\n  env: []\n  extraEnvFrom: []\n  annotations: {}\n  initContainers: []\n  # -- The name of the PriorityClass for smoke-test pods\n  priorityClassName: null\n\n# -- Settings for mimir-continuous-test.\n# This continuously writes and reads metrics from Mimir.\n# https://grafana.com/docs/mimir/latest/manage/tools/mimir-continuous-test/\ncontinuous_test:\n  enabled: false\n  # -- Number of replicas to start of continuous test\n  replicas: 1\n  # The image section has been removed as continuous test is now a module of the regular Mimir image.\n  # See settings for the image at the top image section.\n  # -- The endpoints to use for writing to and reading metrics from your instance.\n  # Defaults to the gateway URL, but you may want to test from an external ingress which you can configure here.\n  ## -- Mimir Write Endpoint to use for writing metrics to your instance. Defaults to mimir.gatewayUrl if not set.\n  write:\n  # -- Mimir Read Endpoint to use for querying metrics from your instance. Defaults to mimir.gatewayUrl if not set. Path /prometheus is appended to value.\n  read:\n  # -- Authentication settings of continuous test\n  auth:\n    # -- Type of authentication to use (tenantId, basicAuth, bearerToken)\n    type: tenantId\n    # -- The tenant to use for tenantId or basicAuth authentication type\n    # In case of tenantId authentication, it is injected as the X-Scope-OrgID header on requests.\n    # In case of basicAuth, it is set as the username.\n    tenant: \"mimir-continuous-test\"\n    # -- Password for basicAuth auth (note: can be environment variable from secret attached in extraEnvFrom, e.g. $(PASSWORD))\n    # For GEM, it should contain an access token created for an access policy that allows `metrics:read` and `metrics:write` for the tenant.\n    password: null\n    # -- Bearer token for bearerToken auth (note: can be environment variable from secret attached in extraEnvFrom, e.g. $(TOKEN))\n    bearerToken: null\n  # -- The maximum number of series to write in a single request.\n  numSeries: 1000\n  # -- How far back in the past metrics can be queried at most.\n  maxQueryAge: \"48h\"\n  # -- Interval between test runs\n  runInterval: \"5m\"\n\n  # -- Pod affinity settings for the continuous test replicas\n  affinity: {}\n  # -- Annotations for the continuous test Deployment\n  annotations: {}\n  # -- The SecurityContext for continuous test containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop: [ALL]\n  # -- Extra environment variables for continuous test containers\n  env: []\n  # -- Extra command line arguments for the continuous test container\n  extraArgs: {}\n  # -- Extra environment from secret/configmap for continuous test containers\n  extraEnvFrom: []\n  # -- Jaeger reporter queue size\n  # Set to 'null' to use the Jaeger client's default value\n  jaegerReporterMaxQueueSize: null\n  # -- Extra volumes for the continuous test container\n  extraVolumes: []\n  # -- Extra volume mounts for the continuous test container\n  extraVolumeMounts: []\n  # -- Extra containers for the continuous test Deployment\n  extraContainers: []\n  # -- Extra initContainers for the continuous test Deployment\n  initContainers: []\n  # -- Nodeselector of continuous test replicas\n  nodeSelector: {}\n  # -- The name of the PriorityClass for continuous test pods\n  priorityClassName: null\n  # -- Kubernetes resource requests and limits for continuous test\n  resources:\n    limits:\n      memory: 1Gi\n    requests:\n      cpu: \"1\"\n      memory: 512Mi\n  # -- Security context for the continuous test Deployment\n  securityContext: {}\n  # -- Service for monitoring continuous test\n  service:\n    annotations: {}\n    labels: {}\n  # -- Upgrade strategy for the continuous test Deployment\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n\n  tolerations: []\n  terminationGracePeriodSeconds: 30\n\n# -- Add dynamic manifests via values. Example:\n# extraObjects:\n# - kind: ConfigMap\n#   apiVersion: v1\n#   metadata:\n#     name: extra-cm-{{ .Release.Name }}\n#   data: |\n#     extra.yml: \"does-my-install-need-extra-info: true\"\n# alternatively, you can use strings, which lets you use the mimir defines:\n# extraObjects:\n# - |\n#   kind: ConfigMap\n#   apiVersion: v1\n#   metadata:\n#     name: extra-cm-{{ .Release.Name }}\n#   data: |\n#     extra.yml: \"{{ include some-mimir-define }}\"\nextraObjects: []\n\n"
            ],
            "verify": false,
            "version": "5.5.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.78.2",
                "chart": "kube-prometheus-stack",
                "name": "prometheus",
                "namespace": "monitoring",
                "revision": 1,
                "values": "{\"additionalPrometheusRulesMap\":{},\"alertmanager\":{\"alertmanagerSpec\":{\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPeers\":[],\"affinity\":{},\"alertmanagerConfigMatcherStrategy\":{},\"alertmanagerConfigNamespaceSelector\":{},\"alertmanagerConfigSelector\":{\"matchLabels\":{\"resource\":\"prometheus\"}},\"alertmanagerConfiguration\":{},\"automountServiceAccountToken\":true,\"clusterAdvertiseAddress\":false,\"clusterGossipInterval\":\"\",\"clusterLabel\":\"\",\"clusterPeerTimeout\":\"\",\"clusterPushpullInterval\":\"\",\"configMaps\":[],\"containers\":[],\"externalUrl\":null,\"forceEnableClusterMode\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/alertmanager\",\"sha\":\"\",\"tag\":\"v0.27.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"minReadySeconds\":0,\"nodeSelector\":{},\"paused\":false,\"podAntiAffinity\":\"soft\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"replicas\":1,\"resources\":{},\"retention\":\"120h\",\"routePrefix\":\"/\",\"scheme\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tlsConfig\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useExistingSecret\":false,\"volumeMounts\":[],\"volumes\":[],\"web\":{}},\"annotations\":{},\"apiVersion\":\"v2\",\"config\":{\"global\":{\"resolve_timeout\":\"5m\"},\"inhibit_rules\":[{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = critical\"],\"target_matchers\":[\"severity =~ warning|info\"]},{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = warning\"],\"target_matchers\":[\"severity = info\"]},{\"equal\":[\"namespace\"],\"source_matchers\":[\"alertname = InfoInhibitor\"],\"target_matchers\":[\"severity = info\"]},{\"target_matchers\":[\"alertname = InfoInhibitor\"]}],\"receivers\":[{\"name\":\"null\"}],\"route\":{\"group_by\":[\"namespace\"],\"group_interval\":\"5m\",\"group_wait\":\"30s\",\"receiver\":\"null\",\"repeat_interval\":\"12h\",\"routes\":[{\"matchers\":[\"alertname = \\\"Watchdog\\\"\"],\"receiver\":\"null\"}]},\"templates\":[\"/etc/alertmanager/config/*.tmpl\"]},\"enableFeatures\":[],\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"alertmanager\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"secret\":{\"annotations\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30903,\"port\":9093,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9093,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"enableHttp2\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerSourceRanges\":[],\"nodePort\":30904,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"stringConfig\":\"\",\"templateFiles\":{},\"tplConfig\":false},\"cleanPrometheusOperatorObjectNames\":false,\"commonLabels\":{},\"coreDns\":{\"enabled\":true,\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLabels\":[],\"targetLimit\":0}},\"crds\":{\"enabled\":true},\"customRules\":{},\"defaultRules\":{\"additionalAggregationLabels\":[],\"additionalRuleAnnotations\":{},\"additionalRuleGroupAnnotations\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleGroupLabels\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleLabels\":{},\"annotations\":{},\"appNamespacesTarget\":\".*\",\"create\":true,\"disabled\":{},\"keepFiringFor\":\"\",\"labels\":{},\"node\":{\"fsSelector\":\"fstype!=\\\"\\\"\"},\"rules\":{\"alertmanager\":true,\"configReloaders\":true,\"etcd\":true,\"general\":true,\"k8sContainerCpuUsageSecondsTotal\":true,\"k8sContainerMemoryCache\":true,\"k8sContainerMemoryRss\":true,\"k8sContainerMemorySwap\":true,\"k8sContainerMemoryWorkingSetBytes\":true,\"k8sContainerResource\":true,\"k8sPodOwner\":true,\"kubeApiserverAvailability\":true,\"kubeApiserverBurnrate\":true,\"kubeApiserverHistogram\":true,\"kubeApiserverSlos\":true,\"kubeControllerManager\":true,\"kubePrometheusGeneral\":true,\"kubePrometheusNodeRecording\":true,\"kubeProxy\":true,\"kubeSchedulerAlerting\":true,\"kubeSchedulerRecording\":true,\"kubeStateMetrics\":true,\"kubelet\":true,\"kubernetesApps\":true,\"kubernetesResources\":true,\"kubernetesStorage\":true,\"kubernetesSystem\":true,\"network\":true,\"node\":true,\"nodeExporterAlerting\":true,\"nodeExporterRecording\":true,\"prometheus\":true,\"prometheusOperator\":true,\"windows\":true},\"runbookUrl\":\"https://runbooks.prometheus-operator.dev/runbooks\"},\"extraManifests\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"rbac\":{\"create\":true,\"createAggregateClusterRoles\":false,\"pspAnnotations\":{},\"pspEnabled\":false}},\"grafana\":{\"additionalDataSources\":[],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEditable\":true,\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":false,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"path\":\"/\",\"tls\":[]},\"namespaceOverride\":\"\",\"prune\":false,\"rbac\":{\"pspEnabled\":false},\"service\":{\"ipFamilies\":[],\"ipFamilyPolicy\":\"\",\"portName\":\"http-web\"},\"serviceAccount\":{\"autoMount\":true,\"create\":true},\"serviceMonitor\":{\"enabled\":true,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enableNewTablePanelSyntax\":false,\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"1\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false},\"searchNamespace\":\"ALL\"},\"datasources\":{\"alertmanager\":{\"enabled\":true,\"handleGrafanaManagedAlerts\":false,\"implementation\":\"prometheus\",\"name\":\"Alertmanager\",\"uid\":\"alertmanager\"},\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"exemplarTraceIdDestinations\":{},\"httpMethod\":\"POST\",\"isDefaultDatasource\":true,\"label\":\"grafana_datasource\",\"labelValue\":\"1\",\"name\":\"Prometheus\",\"uid\":\"prometheus\"}}},\"kube-state-metrics\":{\"namespaceOverride\":\"\",\"prometheus\":{\"monitor\":{\"enabled\":true,\"honorLabels\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"create\":true},\"releaseLabel\":true,\"selfMonitor\":{\"enabled\":false}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"jobLabel\":\"component\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[{\"action\":\"drop\",\"regex\":\"apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\",\"sourceLabels\":[\"__name__\",\"le\"]}],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}},\"targetLabels\":[],\"targetLimit\":0},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLabels\":[],\"targetLimit\":0}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"additionalLabels\":{},\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLabels\":[],\"targetLimit\":0}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":2381,\"targetPort\":2381},\"serviceMonitor\":{\"additionalLabels\":{},\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"keyFile\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"http\",\"selector\":{},\"serverName\":\"\",\"targetLabels\":[],\"targetLimit\":0}},\"kubeProxy\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":10249,\"targetPort\":10249},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLabels\":[],\"targetLimit\":0}},\"kubeScheduler\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLabels\":[],\"targetLimit\":0}},\"kubeStateMetrics\":{\"enabled\":true},\"kubeTargetVersionOverride\":\"\",\"kubeVersionOverride\":\"\",\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"additionalLabels\":{},\"attachMetadata\":{\"node\":false},\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[{\"action\":\"drop\",\"regex\":\"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_memory_(mapped_file|swap)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_(file_descriptors|tasks_state|threads_max)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_spec.*\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\".+;\",\"sourceLabels\":[\"id\",\"pod\"]}],\"cAdvisorRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"honorLabels\":true,\"honorTimestamps\":true,\"https\":true,\"insecureSkipVerify\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"sampleLimit\":0,\"targetLabels\":[],\"targetLimit\":0,\"trackTimestampsStaleness\":true}},\"kubernetesServiceMonitors\":{\"enabled\":true},\"nameOverride\":\"\",\"namespaceOverride\":\"\",\"nodeExporter\":{\"enabled\":true,\"forceDeployDashboards\":false,\"operatingSystems\":{\"aix\":{\"enabled\":true},\"darwin\":{\"enabled\":true},\"linux\":{\"enabled\":true}}},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"agentMode\":false,\"annotations\":{},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalAlertRelabelConfigsSecret\":{},\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"automountServiceAccountToken\":true,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enableRemoteWriteReceiver\":false,\"enforcedKeepDroppedTargets\":0,\"enforcedLabelLimit\":false,\"enforcedLabelNameLengthLimit\":false,\"enforcedLabelValueLengthLimit\":false,\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"enforcedTargetLimit\":false,\"evaluationInterval\":\"\",\"excludedFromEnforcement\":[],\"exemplars\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"hostAliases\":[],\"hostNetwork\":false,\"ignoreNamespaceSelectors\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.55.1\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"maximumStartupDurationSeconds\":0,\"minReadySeconds\":0,\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"persistentVolumeClaimRetentionPolicy\":{},\"podAntiAffinity\":\"soft\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorNamespaceSelector\":{},\"podMonitorSelector\":{},\"podMonitorSelectorNilUsesHelmValues\":true,\"portName\":\"http-web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{},\"probeSelectorNilUsesHelmValues\":true,\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"sampleLimit\":false,\"scrapeClasses\":[],\"scrapeConfigNamespaceSelector\":{},\"scrapeConfigSelector\":{},\"scrapeConfigSelectorNilUsesHelmValues\":true,\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceDiscoveryRole\":\"\",\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{},\"serviceMonitorSelectorNilUsesHelmValues\":true,\"shards\":1,\"storageSpec\":{},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"tracingConfig\":{},\"tsdb\":{\"outOfOrderTimeWindow\":\"0s\"},\"version\":\"\",\"volumeMounts\":[],\"volumes\":[],\"walCompression\":true,\"web\":{}},\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"publishNotReadyAddresses\":false,\"reloaderWebPort\":8080,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9090,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"None\",\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"LoadBalancer\"},\"thanosServiceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"tlsConfig\":{}}},\"prometheus-node-exporter\":{\"extraArgs\":[\"--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\",\"--collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"],\"namespaceOverride\":\"\",\"podLabels\":{\"jobLabel\":\"node-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"pspEnabled\":false},\"releaseLabel\":true,\"service\":{\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{\"jobLabel\":\"node-exporter\"},\"portName\":\"http-metrics\"}},\"prometheus-windows-exporter\":{\"config\":\"collectors:\\n  enabled: '[defaults],memory,container'\",\"podLabels\":{\"jobLabel\":\"windows-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"jobLabel\":\"jobLabel\"}},\"releaseLabel\":true},\"prometheusOperator\":{\"admissionWebhooks\":{\"annotations\":{},\"caBundle\":\"\",\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"createSecretJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"deployment\":{\"affinity\":{},\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"dnsConfig\":{},\"enabled\":false,\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/admission-webhook\",\"sha\":\"\",\"tag\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{},\"podLabels\":{},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":10,\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":31080,\"nodePortTls\":31443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":true,\"name\":\"\"},\"strategy\":{},\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[]},\"enabled\":true,\"failurePolicy\":\"\",\"mutatingWebhookConfiguration\":{\"annotations\":{}},\"namespaceSelector\":{\"any\":true},\"objectSelector\":{},\"patch\":{\"affinity\":{},\"annotations\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"registry.k8s.io\",\"repository\":\"ingress-nginx/kube-webhook-certgen\",\"sha\":\"\",\"tag\":\"v20221220-controller-v1.5.1-58-g787ea74b6\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true},\"tolerations\":[],\"ttlSecondsAfterFinished\":60},\"patchWebhookJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"timeoutSeconds\":10,\"validatingWebhookConfiguration\":{\"annotations\":{}}},\"affinity\":{},\"alertmanagerConfigNamespaces\":[],\"alertmanagerInstanceNamespaces\":[],\"alertmanagerInstanceSelector\":\"\",\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"denyNamespaces\":[],\"dnsConfig\":{},\"enabled\":true,\"env\":{\"GOGC\":\"30\"},\"extraArgs\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-operator\",\"sha\":\"\",\"tag\":\"\"},\"kubeletEndpointSliceEnabled\":false,\"kubeletEndpointsEnabled\":true,\"kubeletService\":{\"enabled\":true,\"name\":\"\",\"namespace\":\"kube-system\",\"selector\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"namespaces\":{},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"prometheusConfigReloader\":{\"enableProbe\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-config-reloader\",\"sha\":\"\",\"tag\":\"\"},\"resources\":{}},\"prometheusInstanceNamespaces\":[],\"prometheusInstanceSelector\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"revisionHistoryLimit\":10,\"secretFieldSelector\":\"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30080,\"nodePortTls\":30443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"selfMonitor\":true,\"targetLimit\":0},\"strategy\":{},\"thanosImage\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.37.0\"},\"thanosRulerInstanceNamespaces\":[],\"thanosRulerInstanceSelector\":\"\",\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[],\"verticalPodAutoscaler\":{\"controlledResources\":[],\"enabled\":false,\"maxAllowed\":{},\"minAllowed\":{},\"updatePolicy\":{\"updateMode\":\"Auto\"}}},\"thanosRuler\":{\"annotations\":{},\"enabled\":false,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30905,\"port\":10902,\"targetPort\":10902,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"thanosRulerSpec\":{\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"affinity\":{},\"alertDropLabels\":[],\"alertmanagersConfig\":{\"existingSecret\":{},\"secret\":{}},\"containers\":[],\"evaluationInterval\":\"\",\"externalPrefix\":null,\"externalPrefixNilUsesHelmValues\":true,\"image\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.37.0\"},\"initContainers\":[],\"labels\":{},\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"objectStorageConfig\":{\"existingSecret\":{},\"secret\":{}},\"paused\":false,\"podAntiAffinity\":\"soft\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"web\",\"priorityClassName\":\"\",\"queryConfig\":{\"existingSecret\":{},\"secret\":{}},\"queryEndpoints\":[],\"replicas\":1,\"resources\":{},\"retention\":\"24h\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[],\"web\":{}}},\"windowsMonitoring\":{\"enabled\":false}}",
                "version": "66.5.0"
              }
            ],
            "name": "prometheus",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Default values for kube-prometheus-stack.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of kube-prometheus-stack for `app:` labels\n##\nnameOverride: \"\"\n\n## Override the deployment namespace\n##\nnamespaceOverride: \"\"\n\n## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6\n##\nkubeTargetVersionOverride: \"\"\n\n## Allow kubeVersion to be overridden while creating the ingress\n##\nkubeVersionOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Install Prometheus Operator CRDs\n##\ncrds:\n  enabled: true\n\n## custom Rules to override \"for\" and \"severity\" in defaultRules\n##\ncustomRules: {}\n  # AlertmanagerFailedReload:\n  #   for: 3m\n  # AlertmanagerMembersInconsistent:\n  #   for: 5m\n#   severity: \"warning\"\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    configReloaders: true\n    general: true\n    k8sContainerCpuUsageSecondsTotal: true\n    k8sContainerMemoryCache: true\n    k8sContainerMemoryRss: true\n    k8sContainerMemorySwap: true\n    k8sContainerResource: true\n    k8sContainerMemoryWorkingSetBytes: true\n    k8sPodOwner: true\n    kubeApiserverAvailability: true\n    kubeApiserverBurnrate: true\n    kubeApiserverHistogram: true\n    kubeApiserverSlos: true\n    kubeControllerManager: true\n    kubelet: true\n    kubeProxy: true\n    kubePrometheusGeneral: true\n    kubePrometheusNodeRecording: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    kubeSchedulerAlerting: true\n    kubeSchedulerRecording: true\n    kubeStateMetrics: true\n    network: true\n    node: true\n    nodeExporterAlerting: true\n    nodeExporterRecording: true\n    prometheus: true\n    prometheusOperator: true\n    windows: true\n\n  ## Reduce app namespace alert scope\n  appNamespacesTarget: \".*\"\n\n  ## Set keep_firing_for for all alerts\n  keepFiringFor: \"\"\n\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n  ## Additional labels for PrometheusRule alerts\n  additionalRuleLabels: {}\n\n  ## Additional annotations for PrometheusRule alerts\n  additionalRuleAnnotations: {}\n\n  ## Additional labels for specific PrometheusRule alert groups\n  additionalRuleGroupLabels:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8sContainerCpuUsageSecondsTotal: {}\n    k8sContainerMemoryCache: {}\n    k8sContainerMemoryRss: {}\n    k8sContainerMemorySwap: {}\n    k8sContainerResource: {}\n    k8sPodOwner: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  ## Additional annotations for specific PrometheusRule alerts groups\n  additionalRuleGroupAnnotations:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8sContainerCpuUsageSecondsTotal: {}\n    k8sContainerMemoryCache: {}\n    k8sContainerMemoryRss: {}\n    k8sContainerMemorySwap: {}\n    k8sContainerResource: {}\n    k8sPodOwner: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  additionalAggregationLabels: []\n\n  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.\n  runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\"\n\n  node:\n    fsSelector: 'fstype!=\"\"'\n    # fsSelector: 'fstype=~\"ext[234]|btrfs|xfs|zfs\"'\n\n  ## Disabled PrometheusRule alerts\n  disabled: {}\n  # KubeAPIDown: true\n  # NodeRAIDDegraded: true\n\n## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.\n##\n# additionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRulesMap: {}\n#  rule-name:\n#    groups:\n#    - name: my_group\n#      rules:\n#      - record: my_record\n#        expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n\n    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs\n    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\n    createAggregateClusterRoles: false\n    pspEnabled: false\n    pspAnnotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)\n  ##\n  imageRegistry: \"\"\n\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n  # or\n  # - \"image-pull-secret\"\n\nwindowsMonitoring:\n  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')\n  enabled: false\n\n## Configuration for prometheus-windows-exporter\n## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter\n##\nprometheus-windows-exporter:\n  ## Enable ServiceMonitor and set Kubernetes label to use as a job label\n  ##\n  prometheus:\n    monitor:\n      enabled: true\n      jobLabel: jobLabel\n\n  releaseLabel: true\n\n  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards\n  ##\n  podLabels:\n    jobLabel: windows-exporter\n\n  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards\n  ##\n  config: |-\n    collectors:\n      enabled: '[defaults],memory,container'\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n\n  ## Deploy alertmanager\n  ##\n  enabled: true\n\n  ## Annotations for Alertmanager\n  ##\n  annotations: {}\n\n  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2\n  ##\n  apiVersion: v2\n\n  ## @param alertmanager.enableFeatures Enable access to Alertmanager disabled features.\n  ##\n  enableFeatures: []\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    inhibit_rules:\n      - source_matchers:\n          - 'severity = critical'\n        target_matchers:\n          - 'severity =~ warning|info'\n        equal:\n          - 'namespace'\n          - 'alertname'\n      - source_matchers:\n          - 'severity = warning'\n        target_matchers:\n          - 'severity = info'\n        equal:\n          - 'namespace'\n          - 'alertname'\n      - source_matchers:\n          - 'alertname = InfoInhibitor'\n        target_matchers:\n          - 'severity = info'\n        equal:\n          - 'namespace'\n      - target_matchers:\n          - 'alertname = InfoInhibitor'\n    route:\n      group_by: ['namespace']\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: 'null'\n      routes:\n        - receiver: 'null'\n          matchers:\n            - alertname = \"Watchdog\"\n    receivers:\n      - name: 'null'\n    templates:\n      - '/etc/alertmanager/config/*.tmpl'\n\n  ## Alertmanager configuration directives (as string type, preferred over the config hash map)\n  ## stringConfig will be used only, if tplConfig is true\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  stringConfig: \"\"\n\n  ## Pass the Alertmanager configuration directives through Helm's templating\n  ## engine. If the Alertmanager configuration contains Alertmanager templates,\n  ## they'll need to be properly escaped so that they are not interpreted by\n  ## Helm\n  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function\n  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string\n  ##      https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  tplConfig: false\n\n  ## Alertmanager template files to format alerts\n  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if\n  ## they have a .tmpl file suffix will be loaded. See config.templates above\n  ## to change, add other suffixes. If adding other suffixes, be sure to update\n  ## config.templates above to include those suffixes.\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  ## An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:* {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n  #       {{ end }}\n  #       {{ end }}\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Override ingress to a different defined port on the service\n    # servicePort: 8081\n    ## Override ingress to a different service then the default, this is useful if you need to\n    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)\n    # serviceName: kube-prometheus-stack-alertmanager-0\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n    # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  # -- BETA: Configure the gateway routes for the chart here.\n  # More routes can be added by adding a dictionary key like the 'main' route.\n  # Be aware that this is an early beta of this feature,\n  # kube-prometheus-stack does not guarantee this works and is subject to change.\n  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\n  route:\n    main:\n      # -- Enables or disables the route\n      enabled: false\n\n      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n      apiVersion: gateway.networking.k8s.io/v1\n      # -- Set the route kind\n      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n      kind: HTTPRoute\n\n      annotations: {}\n      labels: {}\n\n      hostnames: []\n      # - my-filter.example.com\n      parentRefs: []\n      # - name: acme-gw\n\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n\n      ## Filters define the filters that are applied to requests that match this rule.\n      filters: []\n\n      ## Additional custom rules that can be added to the route\n      additionalRules: []\n\n  ## Configuration for Alertmanager secret\n  ##\n  secret:\n    annotations: {}\n\n  ## Configuration for creating an Ingress that will map to each Alertmanager replica service\n  ## alertmanager.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for alertmanager per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"alertmanager\"\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for Alertmanager Service to listen on\n    ##\n    port: 9093\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 9093\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30903\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for Alertmanager service\n    ##\n    additionalPorts: []\n    # - name: oauth-proxy\n    #   port: 8081\n    #   targetPort: 8081\n    # - name: oauth-metrics\n    #   port: 8082\n    #   targetPort: 8082\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or 'None'\n    ##\n    sessionAffinity: None\n\n    ## If you want to modify the ClientIP sessionAffinity timeout\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\n    ##\n    sessionAffinityConfig:\n      clientIP:\n        timeoutSeconds: 10800\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a separate Service for each statefulset Alertmanager replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Alertmanager Service per replica to listen on\n    ##\n    port: 9093\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9093\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30904\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a ServiceMonitor for AlertManager\n  ##\n  serviceMonitor:\n    ## If true, a ServiceMonitor will be created for the AlertManager service.\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## enableHttp2: Whether to enable HTTP2.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n    enableHttp2: true\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/alertmanager\n      tag: v0.27.0\n      sha: \"\"\n\n    ## If true then the user will be responsible to provide a secret with alertmanager configuration\n    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used\n    ##\n    useExistingSecret: false\n\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## If false then the user will opt out of automounting API credentials.\n    ##\n    automountServiceAccountToken: true\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for\n    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.\n    ##\n    # configSecret:\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec\n    web: {}\n\n    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.\n    ##\n    alertmanagerConfigSelector:\n      matchLabels:\n        resource: prometheus\n    ## Example which selects all alertmanagerConfig resources\n    ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\"\n    # alertmanagerConfigSelector:\n    #   matchExpressions:\n    #     - key: alertconfig\n    #       operator: In\n    #       values:\n    #         - example-config\n    #         - example-config-2\n    #\n    ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\"\n    # alertmanagerConfigSelector:\n    #   matchLabels:\n    #     role: example-config\n\n    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.\n    ##\n    alertmanagerConfigNamespaceSelector: {}\n    ## Example which selects all namespaces\n    ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchExpressions:\n    #     - key: alertmanagerconfig\n    #       operator: In\n    #       values:\n    #         - example-namespace\n    #         - example-namespace-2\n\n    ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchLabels:\n    #     alertmanagerconfig: enabled\n\n    ## AlermanagerConfig to be used as top level configuration\n    ##\n    alertmanagerConfiguration: {}\n    ## Example with select a global alertmanagerconfig\n    # alertmanagerConfiguration:\n    #   name: global-alertmanager-Configuration\n\n    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:\n    ##\n    alertmanagerConfigMatcherStrategy: {}\n    ## Example with use OnNamespace strategy\n    # alertmanagerConfigMatcherStrategy:\n    #   type: OnNamespace\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n\n    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false\n    ##\n    externalUrl:\n\n    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"soft\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the alertmanager instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: alertmanager\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\n    #   args:\n    #   - --upstream=http://127.0.0.1:9093\n    #   - --http-address=0.0.0.0:8081\n    #   - --metrics-address=0.0.0.0:8082\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   - containerPort: 8082\n    #     name: oauth-metrics\n    #     protocol: TCP\n    #   resources: {}\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n    ## PortName to use for Alert Manager.\n    ##\n    portName: \"http-web\"\n\n    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918\n    ##\n    clusterAdvertiseAddress: false\n\n    ## clusterGossipInterval determines interval between gossip attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterGossipInterval: \"\"\n\n    ## clusterPeerTimeout determines timeout for cluster peering.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPeerTimeout: \"\"\n\n    ## clusterPushpullInterval determines interval between pushpull attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPushpullInterval: \"\"\n\n    ## clusterLabel defines the identifier that uniquely identifies the Alertmanager cluster.\n    clusterLabel: \"\"\n\n    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.\n    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.\n    forceEnableClusterMode: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\n    ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)\n    additionalConfigString: \"\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\n##\ngrafana:\n  enabled: false\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  ## Editable flag for the default dashboards\n  ##\n  defaultDashboardsEditable: true\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: {}\n      # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  # # To make Grafana persistent (Using Statefulset)\n  # #\n  # persistence:\n  #   enabled: true\n  #   type: sts\n  #   storageClassName: \"storageClassName\"\n  #   accessModes:\n  #     - ReadWriteOnce\n  #   size: 20Gi\n  #   finalizers:\n  #     - kubernetes.io/pvc-protection\n\n  serviceAccount:\n    create: true\n    autoMount: true\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n      # Allow discovery in all namespaces for dashboards\n      searchNamespace: ALL\n\n      # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels\n      enableNewTablePanelSyntax: false\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n      isDefaultDatasource: true\n\n      name: Prometheus\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      ## Prometheus request timeout in seconds\n      # timeout: 30\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Set method for HTTP to send query to datasource\n      httpMethod: POST\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations: {}\n        # datasourceUid: Jaeger\n      # traceIdLabelName: trace_id\n      alertmanager:\n        enabled: true\n        name: Alertmanager\n        uid: alertmanager\n        handleGrafanaManagedAlerts: false\n        implementation: prometheus\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   secureJsonData:\n  #       basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090\n  #   version: 1\n\n  # Flag to mark provisioned data sources for deletion if they are no longer configured.\n  # It takes no effect if data sources are already listed in the deleteDatasources section.\n  # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file\n  prune: false\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n    ipFamilies: []\n    ipFamilyPolicy: \"\"\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Flag to disable all the kubernetes component scrapers\n##\nkubernetesServiceMonitors:\n  enabled: true\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.\n    ##\n    attachMetadata:\n      node: false\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## If true, Prometheus use (respect) labels provided by exporter.\n    ##\n    honorLabels: true\n\n    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.\n    ##\n    honorTimestamps: true\n\n    ## If true, defines whether Prometheus tracks staleness of the metrics that have an explicit timestamp present in scraped data. Has no effect if `honorTimestamps` is false.\n    ## We recommend enabling this if you want the best possible accuracy for container_ metrics scraped from cadvisor.\n    ##\n    trackTimestampsStaleness: true\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Skip TLS certificate validation when scraping.\n    ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs\n    ##\n    insecureSkipVerify: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_memory_(mapped_file|swap)'\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_(file_descriptors|tasks_state|threads_max)'\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_spec.*'\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: '.+;'\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: kube-controller-manager\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    enabled: true\n    port: 9153\n    targetPort: 9153\n\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-dns\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-dns\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2381\n    targetPort: 2381\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: etcd\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: true\n\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.23.\n    ##\n    port: null\n    targetPort: null\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: kube-scheduler\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    ## Enable scraping kube-scheduler over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: kube-scheduler\n\n    ## Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    ## Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kube proxy\n##\nkubeProxy:\n  enabled: true\n\n  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  service:\n    enabled: true\n    port: 10249\n    targetPort: 10249\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-proxy\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-proxy\n\n    ## Enable scraping kube-proxy over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  namespaceOverride: \"\"\n  rbac:\n    create: true\n  releaseLabel: true\n  prometheus:\n    monitor:\n      enabled: true\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      # Keep labels from scraped data, overriding server-side labels\n      ##\n      honorLabels: true\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - action: keep\n      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n      #   sourceLabels: [__name__]\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n\n  selfMonitor:\n    enabled: false\n\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n  operatingSystems:\n    linux:\n      enabled: true\n    aix:\n      enabled: true\n    darwin:\n      enabled: true\n\n  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  namespaceOverride: \"\"\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  releaseLabel: true\n  extraArgs:\n    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\n  service:\n    portName: http-metrics\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    labels:\n      jobLabel: node-exporter\n\n  prometheus:\n    monitor:\n      enabled: true\n\n      jobLabel: jobLabel\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - sourceLabels: [__name__]\n      #   separator: ;\n      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n      #   replacement: $1\n      #   action: drop\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n\n      ## Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above.\n      ##\n      # attachMetadata:\n      #   node: false\n\n  rbac:\n    ## If true, create PSPs for node-exporter\n    ##\n    pspEnabled: false\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n\n  ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-operator' by default\n  fullnameOverride: \"\"\n\n  ## Number of old replicasets to retain ##\n  ## The default value is 10, 0 will garbage-collect old replicasets ##\n  revisionHistoryLimit: 10\n\n  ## Strategy of the deployment\n  ##\n  strategy: {}\n\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\n  ##\n  tls:\n    enabled: true\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n    tlsMinVersion: VersionTLS13\n    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n    internalPort: 10250\n\n  ## Liveness probe for the prometheusOperator deployment\n  ##\n  livenessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 0\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n  ## Readiness probe for the prometheusOperator deployment\n  ##\n  readinessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 0\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n\n  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted\n  ## rules from making their way into prometheus and potentially preventing the container from starting\n  admissionWebhooks:\n    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly\n    ## IgnoreOnInstallOnly - If Release.IsInstall returns \"true\", set \"Ignore\" otherwise \"Fail\"\n    failurePolicy: \"\"\n    ## The default timeoutSeconds is 10 and the maximum value is 30.\n    timeoutSeconds: 10\n    enabled: true\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\n    ## If unspecified, system trust roots on the apiserver are used.\n    caBundle: \"\"\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\n    ## certs ahead of time if you wish.\n    ##\n    annotations: {}\n    #   argocd.argoproj.io/hook: PreSync\n    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n\n    namespaceSelector:\n      any: true\n    objectSelector: {}\n\n    mutatingWebhookConfiguration:\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n\n    validatingWebhookConfiguration:\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n\n    deployment:\n      enabled: false\n\n      ## Number of replicas\n      ##\n      replicas: 1\n\n      ## Strategy of the deployment\n      ##\n      strategy: {}\n\n      # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n      podDisruptionBudget: {}\n        # maxUnavailable: 1\n      # minAvailable: 1\n\n      ## Number of old replicasets to retain ##\n      ## The default value is 10, 0 will garbage-collect old replicasets ##\n      revisionHistoryLimit: 10\n\n      ## Prometheus-Operator v0.39.0 and later support TLS natively.\n      ##\n      tls:\n        enabled: true\n        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n        tlsMinVersion: VersionTLS13\n        # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n        internalPort: 10250\n\n      ## Service account for Prometheus Operator Webhook to use.\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n      ##\n      serviceAccount:\n        annotations: {}\n        automountServiceAccountToken: false\n        create: true\n        name: \"\"\n\n      ## Configuration for Prometheus operator Webhook service\n      ##\n      service:\n        annotations: {}\n        labels: {}\n        clusterIP: \"\"\n        ipDualStack:\n          enabled: false\n          ipFamilies: [\"IPv6\", \"IPv4\"]\n          ipFamilyPolicy: \"PreferDualStack\"\n\n        ## Port to expose on each node\n        ## Only used if service.type is 'NodePort'\n        ##\n        nodePort: 31080\n\n        nodePortTls: 31443\n\n        ## Additional ports to open for Prometheus operator Webhook service\n        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n        ##\n        additionalPorts: []\n\n        ## Loadbalancer IP\n        ## Only use if service.type is \"LoadBalancer\"\n        ##\n        loadBalancerIP: \"\"\n        loadBalancerSourceRanges: []\n\n        ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n        ##\n        externalTrafficPolicy: Cluster\n\n        ## Service type\n        ## NodePort, ClusterIP, LoadBalancer\n        ##\n        type: ClusterIP\n\n        ## List of IP addresses at which the Prometheus server service is available\n        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n        ##\n        externalIPs: []\n\n      # ## Labels to add to the operator webhook deployment\n      # ##\n      labels: {}\n\n      ## Annotations to add to the operator webhook deployment\n      ##\n      annotations: {}\n\n      ## Labels to add to the operator webhook pod\n      ##\n      podLabels: {}\n\n      ## Annotations to add to the operator webhook pod\n      ##\n      podAnnotations: {}\n\n      ## Assign a PriorityClassName to pods if set\n      # priorityClassName: \"\"\n\n      ## Define Log Format\n      # Use logfmt (default) or json logging\n      # logFormat: logfmt\n\n      ## Decrease log verbosity to errors only\n      # logLevel: error\n\n      ## Prometheus-operator webhook image\n      ##\n      image:\n        registry: quay.io\n        repository: prometheus-operator/admission-webhook\n        # if not set appVersion field from Chart.yaml is used\n        tag: \"\"\n        sha: \"\"\n        pullPolicy: IfNotPresent\n\n      ## Define Log Format\n      # Use logfmt (default) or json logging\n      # logFormat: logfmt\n\n      ## Decrease log verbosity to errors only\n      # logLevel: error\n\n\n      ## Liveness probe\n      ##\n      livenessProbe:\n        enabled: true\n        failureThreshold: 3\n        initialDelaySeconds: 30\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n\n      ## Readiness probe\n      ##\n      readinessProbe:\n        enabled: true\n        failureThreshold: 3\n        initialDelaySeconds: 5\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n\n      ## Resource limits \u0026 requests\n      ##\n      resources: {}\n      # limits:\n      #   cpu: 200m\n      #   memory: 200Mi\n      # requests:\n      #   cpu: 100m\n      #   memory: 100Mi\n\n      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n      ##\n      hostNetwork: false\n\n      ## Define which Nodes the Pods are scheduled on.\n      ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n      ##\n      nodeSelector: {}\n\n      ## Tolerations for use with node taints\n      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n      ##\n      tolerations: []\n      # - key: \"key\"\n      #   operator: \"Equal\"\n      #   value: \"value\"\n      #   effect: \"NoSchedule\"\n\n      ## Assign custom affinity rules to the prometheus operator\n      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n      ##\n      affinity: {}\n        # nodeAffinity:\n        #   requiredDuringSchedulingIgnoredDuringExecution:\n        #     nodeSelectorTerms:\n        #     - matchExpressions:\n        #       - key: kubernetes.io/e2e-az-name\n        #         operator: In\n        #         values:\n      #         - e2e-az1\n      #         - e2e-az2\n      dnsConfig: {}\n        # nameservers:\n        #   - 1.2.3.4\n        # searches:\n        #   - ns1.svc.cluster-domain.example\n        #   - my.dns.search.suffix\n        # options:\n        #   - name: ndots\n        #     value: \"2\"\n      #   - name: edns0\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n\n      ## Container-specific security context configuration\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      containerSecurityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n\n      ## If false then the user will opt out of automounting API credentials.\n      ##\n      automountServiceAccountToken: true\n\n    patch:\n      enabled: true\n      image:\n        registry: registry.k8s.io\n        repository: ingress-nginx/kube-webhook-certgen\n        tag: v20221220-controller-v1.5.1-58-g787ea74b6\n        sha: \"\"\n        pullPolicy: IfNotPresent\n      resources: {}\n      ## Provide a priority class name to the webhook patching job\n      ##\n      priorityClassName: \"\"\n      ttlSecondsAfterFinished: 60\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n      podAnnotations: {}\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n\n      ## SecurityContext holds pod-level security attributes and common container settings.\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n      ## Service account for Prometheus Operator Webhook Job Patch to use.\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n      ##\n      serviceAccount:\n        create: true\n        annotations: {}\n        automountServiceAccountToken: true\n\n    # Security context for create job container\n    createSecretJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n\n      # Security context for patch job container\n    patchWebhookJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false\n      # self-signed root certificate\n      rootCert:\n        duration: \"\"  # default to be 5y\n      admissionCert:\n        duration: \"\"  # default to be 1y\n      # issuerRef:\n      #   name: \"issuer\"\n      #   kind: \"ClusterIssuer\"\n\n  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).\n  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration\n  ##\n  namespaces: {}\n    # releaseNamespace: true\n    # additional:\n  # - kube-system\n\n  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).\n  ##\n  denyNamespaces: []\n\n  ## Filter namespaces to look for prometheus-operator custom resources\n  ##\n  alertmanagerInstanceNamespaces: []\n  alertmanagerConfigNamespaces: []\n  prometheusInstanceNamespaces: []\n  thanosRulerInstanceNamespaces: []\n\n  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.\n  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)\n  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094\n  ##\n  # clusterDomain: \"cluster.local\"\n\n  networkPolicy:\n    ## Enable creation of NetworkPolicy resources.\n    ##\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   egress:\n\n    ## match labels used in selector\n    # matchLabels: {}\n\n  ## Service account for Prometheus Operator to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    automountServiceAccountToken: true\n    annotations: {}\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30080\n\n    nodePortTls: 30443\n\n    ## Additional ports to open for Prometheus operator service\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n    ##\n    additionalPorts: []\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ## NodePort, ClusterIP, LoadBalancer\n    ##\n    type: ClusterIP\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  # ## Labels to add to the operator deployment\n  # ##\n  labels: {}\n\n  ## Annotations to add to the operator deployment\n  ##\n  annotations: {}\n\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  kubeletService:\n    ## If true, the operator will create and maintain a service for scraping kubelets\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md\n    ##\n    enabled: true\n    namespace: kube-system\n    selector: \"\"\n    ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default\n    name: \"\"\n\n  ## Create Endpoints objects for kubelet targets.\n  kubeletEndpointsEnabled: true\n  ## Create EndpointSlice objects for kubelet targets.\n  kubeletEndpointSliceEnabled: false\n\n  ## Extra arguments to pass to prometheusOperator\n  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/operator.md\n  extraArgs: []\n  #  - --labels=\"cluster=talos-cluster\"\n\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## If true, create a serviceMonitor for prometheus operator\n    ##\n    selfMonitor: true\n\n    ## Labels for ServiceMonitor\n    additionalLabels: {}\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.\n    scrapeTimeout: \"\"\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  ## Operator Environment\n  ##  env:\n  ##    VARIABLE: value\n  env:\n    GOGC: \"30\"\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign custom affinity rules to the prometheus operator\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n  #         - e2e-az2\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n  #     value: \"2\"\n  #   - name: edns0\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n    seccompProfile:\n      type: RuntimeDefault\n\n  ## Container-specific security context configuration\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ##\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n\n  # Enable vertical pod autoscaler support for prometheus-operator\n  verticalPodAutoscaler:\n    enabled: false\n\n    # Recommender responsible for generating recommendation for the object.\n    # List should be empty (then the default recommender will generate the recommendation)\n    # or contain exactly one recommender.\n    # recommenders:\n    # - name: custom-recommender-performance\n\n    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory\n    controlledResources: []\n    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.\n    # controlledValues: RequestsAndLimits\n\n    # Define the max allowed resources for the pod\n    maxAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n    # Define the min allowed resources for the pod\n    minAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n\n    updatePolicy:\n      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction\n      # minReplicas: 1\n      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates\n      # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\".\n      updateMode: Auto\n\n  ## Prometheus-operator image\n  ##\n  image:\n    registry: quay.io\n    repository: prometheus-operator/prometheus-operator\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    sha: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus image to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImage: prometheus/prometheus\n\n  ## Prometheus image registry to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImageRegistry: quay.io\n\n  ## Alertmanager image to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImage: prometheus/alertmanager\n\n  ## Alertmanager image registry to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImageRegistry: quay.io\n\n  ## Prometheus-config-reloader\n  ##\n  prometheusConfigReloader:\n    image:\n      registry: quay.io\n      repository: prometheus-operator/prometheus-config-reloader\n      # if not set appVersion field from Chart.yaml is used\n      tag: \"\"\n      sha: \"\"\n\n    # add prometheus config reloader liveness and readiness probe. Default: false\n    enableProbe: false\n\n    # resource config for prometheusConfigReloader\n    resources: {}\n      # requests:\n      #   cpu: 200m\n      #   memory: 50Mi\n      # limits:\n      #   cpu: 200m\n    #   memory: 50Mi\n\n  ## Thanos side-car image when configured\n  ##\n  thanosImage:\n    registry: quay.io\n    repository: thanos/thanos\n    tag: v0.37.0\n    sha: \"\"\n\n  ## Set a Label Selector to filter watched prometheus and prometheusAgent\n  ##\n  prometheusInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched alertmanager\n  ##\n  alertmanagerInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched thanosRuler\n  thanosRulerInstanceSelector: \"\"\n\n  ## Set a Field Selector to filter watched secrets\n  ##\n  secretFieldSelector: \"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\"\n\n  ## If false then the user will opt out of automounting API credentials.\n  ##\n  automountServiceAccountToken: true\n\n  ## Additional volumes\n  ##\n  extraVolumes: []\n\n  ## Additional volume mounts\n  ##\n  extraVolumeMounts: []\n\n## Deploy a Prometheus instance\n##\nprometheus:\n  enabled: true\n\n  ## Toggle prometheus into agent mode\n  ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md\n  ##\n  agentMode: false\n\n  ## Annotations for Prometheus\n  ##\n  annotations: {}\n\n  ## Configure network policy for the prometheus\n  networkPolicy:\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   endpointSelector:\n    #   egress:\n    #   ingress:\n\n    # egress:\n    # - {}\n    # ingress:\n    # - {}\n    # podSelector:\n    #   matchLabels:\n    #     app: prometheus\n\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  # Service for thanos service discovery on sidecar\n  # Enable this can make Thanos Query can use\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\n  # Thanos sidecar on prometheus nodes\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\n  thanosService:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Service dual stack\n    ##\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## ClusterIP to assign\n    # Default is to make this a headless service (\"None\")\n    clusterIP: \"None\"\n\n    ## Port to expose on each node, if service type is NodePort\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  # ServiceMonitor to scrape Sidecar metrics\n  # Needs thanosService to be enabled as well\n  thanosServiceMonitor:\n    enabled: false\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n\n    ## relabel configs to apply to samples before ingestion.\n    relabelings: []\n\n  # Service for external access to sidecar\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\n  thanosServiceExternal:\n    enabled: false\n    annotations: {}\n    labels: {}\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: LoadBalancer\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for Prometheus Service to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port for Prometheus Reloader to listen on\n    ##\n    reloaderWebPort: 8080\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Additional ports to open for Prometheus service\n    ##\n    additionalPorts: []\n    # additionalPorts:\n    # - name: oauth-proxy\n    #   port: 8081\n    #   targetPort: 8081\n    # - name: oauth-metrics\n    #   port: 8082\n    #   targetPort: 8082\n\n    ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not\n    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec\n    publishNotReadyAddresses: false\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or 'None'\n    ##\n    sessionAffinity: None\n\n    ## If you want to modify the ClientIP sessionAffinity timeout\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\n    ##\n    sessionAffinityConfig:\n      clientIP:\n        timeoutSeconds: 10800\n\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Prometheus Service per replica to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30091\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Service dual stack\n    ##\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  # Ingress exposes thanos sidecar outside the cluster\n  thanosIngress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n    servicePort: 10901\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30901\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n    # - thanos-gateway.domain.com\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Thanos Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanos-gateway-tls\n    #   hosts:\n    #   - thanos-gateway.domain.com\n    #\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Redirect ingress to an additional defined port on the service\n    # servicePort: 8081\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n    #     - prometheus.example.com\n\n  # -- BETA: Configure the gateway routes for the chart here.\n  # More routes can be added by adding a dictionary key like the 'main' route.\n  # Be aware that this is an early beta of this feature,\n  # kube-prometheus-stack does not guarantee this works and is subject to change.\n  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\n  route:\n    main:\n      # -- Enables or disables the route\n      enabled: false\n\n      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n      apiVersion: gateway.networking.k8s.io/v1\n      # -- Set the route kind\n      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n      kind: HTTPRoute\n\n      annotations: {}\n      labels: {}\n\n      hostnames: []\n      # - my-filter.example.com\n      parentRefs: []\n      # - name: acme-gw\n\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n\n      ## Filters define the filters that are applied to requests that match this rule.\n      filters: []\n\n      ## Additional custom rules that can be added to the route\n      additionalRules: []\n\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\n  ## prometheus.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"prometheus\"\n\n  ## Configure additional options for default pod security policy for Prometheus\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  podSecurityPolicy:\n    allowedCapabilities: []\n    allowedHostPaths: []\n    volumes: []\n\n  serviceMonitor:\n    ## If true, create a serviceMonitor for prometheus\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## Statefulset's persistent volume claim retention policy\n    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether\n    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down\n    ## and deleting statefulset, respectively. Requires 1.27.0+.\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    persistentVolumeClaimRetentionPolicy: {}\n    #  whenDeleted: Retain\n    #  whenScaled: Retain\n\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\n    ##\n    ## AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in the pod,\n    ## If the field isn’t set, the operator mounts the service account token by default.\n    ## Warning: be aware that by default, Prometheus requires the service account token for Kubernetes service discovery,\n    ## It is possible to use strategic merge patch to project the service account token into the ‘prometheus’ container.\n    automountServiceAccountToken: true\n\n    disableCompaction: false\n    ## APIServerConfig\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig\n    ##\n    apiserverConfig: {}\n\n    ## Allows setting additional arguments for the Prometheus container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus\n    additionalArgs: []\n\n    ## Interval between consecutive scrapes.\n    ## Defaults to 30s.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\n    ##\n    scrapeInterval: \"\"\n\n    ## Number of seconds to wait for target to respond before erroring\n    ##\n    scrapeTimeout: \"\"\n\n    ## List of scrape classes to expose to scraping objects such as\n    ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.\n    ##\n    scrapeClasses: []\n    # - name: istio-mtls\n    #   default: false\n    #   tlsConfig:\n    #     caFile: /etc/prometheus/secrets/istio.default/root-cert.pem\n    #     certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\n    ## This is disabled by default.\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\n    ##\n    enableAdminAPI: false\n\n    ## Sets version of Prometheus overriding the Prometheus version as derived\n    ## from the image tag. Useful in cases where the tag does not follow semver v2.\n    version: \"\"\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig\n    web: {}\n\n    ## Exemplars related settings that are runtime reloadable.\n    ## It requires to enable the exemplar storage feature to be effective.\n    exemplars: \"\"\n      ## Maximum number of exemplars stored in memory for all series.\n      ## If not set, Prometheus uses its default value.\n      ## A value of zero or less than zero disables the storage.\n    # maxSize: 100000\n\n    # EnableFeatures API enables access to Prometheus disabled features.\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\n    enableFeatures: []\n    # - exemplar-storage\n\n    ## Image of Prometheus.\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/prometheus\n      tag: v2.55.1\n      sha: \"\"\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: prometheus\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n    #   pathPrefix: \"\"\n    #   tlsConfig: {}\n    #   bearerTokenFile: \"\"\n    #   apiVersion: v2\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## enable --web.enable-remote-write-receiver flag on prometheus-server\n    ##\n    enableRemoteWriteReceiver: false\n\n    ## Name of the external label used to denote replica name\n    ##\n    replicaExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote replica name\n    ##\n    replicaExternalLabelNameClear: false\n\n    ## Name of the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelNameClear: false\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.\n    ruleNamespaceSelector: {}\n    ## Example which selects PrometheusRules in namespaces with label \"prometheus\" set to \"somelabel\"\n    # ruleNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ##\n    serviceMonitorNamespaceSelector: {}\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the podmonitors created\n    ##\n    podMonitorSelectorNilUsesHelmValues: true\n\n    ## PodMonitors to be selected for target discovery.\n    ## If {}, select all PodMonitors\n    ##\n    podMonitorSelector: {}\n    ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\"\n    # podMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.\n    podMonitorNamespaceSelector: {}\n    ## Example which selects PodMonitor in namespaces with label \"prometheus\" set to \"somelabel\"\n    # podMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the probes created\n    ##\n    probeSelectorNilUsesHelmValues: true\n\n    ## Probes to be selected for target discovery.\n    ## If {}, select all Probes\n    ##\n    probeSelector: {}\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\n    # probeSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.\n    probeNamespaceSelector: {}\n    ## Example which selects Probe in namespaces with label \"prometheus\" set to \"somelabel\"\n    # probeNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the scrapeConfigs created\n    ##\n    ## If null and scrapeConfigSelector is also null, exclude field from the prometheusSpec\n    ## (keeping downward compatibility with older versions of CRD)\n    ##\n    scrapeConfigSelectorNilUsesHelmValues: true\n\n    ## scrapeConfigs to be selected for target discovery.\n    ## If {}, select all scrapeConfigs\n    ##\n    scrapeConfigSelector: {}\n    ## Example which selects scrapeConfigs with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\n    scrapeConfigNamespaceSelector: {}\n    ## Example which selects scrapeConfig in namespaces with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## Maximum size of metrics\n    ##\n    retentionSize: \"\"\n\n    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration\n    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\n    tsdb:\n      outOfOrderTimeWindow: 0s\n\n    ## Enable compression of the write-ahead log using Snappy.\n    ##\n    walCompression: true\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ##\n    replicas: 1\n\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\n    ## Sharding is done on the content of the `__address__` target meta-label.\n    ##\n    shards: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Log format for Prometheus be configured in\n    ##\n    logFormat: logfmt\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"soft\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the prometheus instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n    ## additionalRemoteRead is appended to remoteRead\n    additionalRemoteRead: []\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n    ## additionalRemoteWrite is appended to remoteWrite\n    additionalRemoteWrite: []\n\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\n    remoteWriteDashboards: false\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    ## Using PersistentVolumeClaim\n    ##\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## Using tmpfs volume\n    ##\n    #  emptyDir:\n    #    medium: Memory\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ## AdditionalScrapeConfigs can be defined as a list or as a templated string.\n    ##\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     targetLabel: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     targetLabel: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n    #\n    ## If scrape config contains a repetitive section, you may want to use a template.\n    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones\n    # additionalScrapeConfigs: |\n    #  - job_name: \"node-exporter\"\n    #    gce_sd_configs:\n    #    {{range $zone := .Values.gcp_zones}}\n    #    - project: \"project1\"\n    #      zone: \"{{$zone}}\"\n    #      port: 9100\n    #    {{end}}\n    #    relabel_configs:\n    #    ...\n\n\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalScrapeConfigs\n    additionalScrapeConfigsSecret: {}\n      # enabled: false\n      # name:\n    # key:\n\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\n    additionalPrometheusSecretsAnnotations: {}\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertManagerConfigs\n    additionalAlertManagerConfigsSecret: {}\n      # name:\n      # key:\n    # optional: false\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertRelabelConfigs\n    additionalAlertRelabelConfigsSecret: {}\n      # name:\n    # key:\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n      # secretProviderClass:\n      #   provider: gcp\n      #   parameters:\n      #     secrets: |\n      #       - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\"\n      #         fileName: \"objstore.yaml\"\n      ## ObjectStorageConfig configures object storage in Thanos.\n      # objectStorageConfig:\n      #   # use existing secret, if configured, objectStorageConfig.secret will not be used\n      #   existingSecret: {}\n      #     # name: \"\"\n      #     # key: \"\"\n      #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,\n      #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set\n      #   # https://thanos.io/tip/thanos/storage.md/#s3\n      #   secret: {}\n      #     # type: S3\n      #     # config:\n      #     #   bucket: \"\"\n      #     #   endpoint: \"\"\n      #     #   region: \"\"\n      #     #   access_key: \"\"\n    #     #   secret_key: \"\"\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ## if using proxy extraContainer update targetPort with proxy container port\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\n    #   args:\n    #   - --upstream=http://127.0.0.1:9090\n    #   - --http-address=0.0.0.0:8081\n    #   - --metrics-address=0.0.0.0:8082\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   - containerPort: 8082\n    #     name: oauth-metrics\n    #     protocol: TCP\n    #   resources: {}\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## PortName to use for Prometheus.\n    ##\n    portName: \"http-web\"\n\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\n    ## on the file system of the Prometheus container e.g. bearer token files.\n    arbitraryFSAccessThroughSMs: false\n\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\n    ## or PodMonitor to true, this overrides honor_labels to false.\n    overrideHonorLabels: false\n\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\n    overrideHonorTimestamps: false\n\n    ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,\n    ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,\n    ## and servicemonitors will be installed in the default service namespace.\n    ## Defaults to false.\n    ignoreNamespaceSelectors: false\n\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\n    ## The label value will always be the namespace of the object that is being created.\n    ## Disabled by default\n    enforcedNamespaceLabel: \"\"\n\n    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\n    ## Deprecated, use `excludedFromEnforcement` instead\n    prometheusRulesExcludedFromEnforce: []\n\n    ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects\n    ## to be excluded from enforcing a namespace label of origin.\n    ## Works only if enforcedNamespaceLabel set to true.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference\n    excludedFromEnforcement: []\n\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\n    queryLogFile: false\n\n    # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.\n    # Set to 'false' to disable global sample_limit. or set to a number to override the default value.\n    sampleLimit: false\n\n    # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.\n    # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets\n    # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.\n    enforcedKeepDroppedTargets: 0\n\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\n    enforcedSampleLimit: false\n\n    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set\n    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall\n    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except\n    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.\n    enforcedTargetLimit: false\n\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelLimit: false\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelNameLengthLimit: false\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this\n    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus\n    ## versions 2.27.0 and newer.\n    enforcedLabelValueLengthLimit: false\n\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\n    ## in Prometheus so it may change in any upcoming release.\n    allowOverlappingBlocks: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n    # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.\n    # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.\n    hostNetwork: false\n\n    # HostAlias holds the mapping between IP and hostnames that will be injected\n    # as an entry in the pod’s hosts file.\n    hostAliases: []\n    #  - ip: 10.10.0.100\n    #    hostnames:\n    #      - a1.app.local\n    #      - b1.app.local\n\n    ## TracingConfig configures tracing in Prometheus.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig\n    tracingConfig: {}\n\n    ## Defines the service discovery role used to discover targets from ServiceMonitor objects and Alertmanager endpoints.\n    ## If set, the value should be either “Endpoints” or “EndpointSlice”. If unset, the operator assumes the “Endpoints” role.\n    serviceDiscoveryRole: \"\"\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\n    ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)\n    additionalConfigString: \"\"\n\n    ## Defines the maximum time that the `prometheus` container's startup probe\n    ## will wait before being considered failed. The startup probe will return\n    ## success after the WAL replay is complete. If set, the value should be\n    ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15\n    ## minutes).\n    maximumStartupDurationSeconds: 0\n\n  additionalRulesForClusterRole: []\n  #  - apiGroups: [ \"\" ]\n  #    resources:\n  #      - nodes/proxy\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  additionalServiceMonitors: []\n    ## Name of the ServiceMonitor to create\n    ##\n    # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## labels to transfer from the kubernetes service to the target\n    ##\n    # targetLabels: []\n\n    ## labels to transfer from the kubernetes pods to the target\n    ##\n    # podTargetLabels: []\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n    ## Example which selects all services to be monitored\n    ## with label \"monitoredby\" with values any of \"example-service-1\" or \"example-service-2\"\n    # matchExpressions:\n    #   - key: \"monitoredby\"\n    #     operator: In\n    #     values:\n    #       - example-service-1\n    #       - example-service-2\n\n    ## label selector for services\n    ##\n    # matchLabels: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n    ## Match any namespace\n    ##\n    # any: false\n\n    ## Explicit list of namespace names to select\n    ##\n    # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n    ## Name of the endpoint's service port\n    ## Mutually exclusive with targetPort\n    # - port: \"\"\n\n    ## Name or number of the endpoint's target port\n    ## Mutually exclusive with port\n    # - targetPort: \"\"\n\n    ## File containing bearer token to be used when scraping targets\n    ##\n    #   bearerTokenFile: \"\"\n\n    ## Interval at which metrics should be scraped\n    ##\n    #   interval: 30s\n\n    ## HTTP path to scrape for metrics\n    ##\n    #   path: /metrics\n\n    ## HTTP scheme to use for scraping\n    ##\n    #   scheme: http\n\n    ## TLS configuration to use when scraping the endpoint\n    ##\n    #   tlsConfig:\n\n    ## Path to the CA file\n    ##\n    # caFile: \"\"\n\n    ## Path to client certificate file\n    ##\n    # certFile: \"\"\n\n    ## Skip certificate verification\n    ##\n    # insecureSkipVerify: false\n\n    ## Path to client key file\n    ##\n    # keyFile: \"\"\n\n    ## Server name used to verify host name\n    ##\n    # serverName: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n  #   action: replace\n\n  additionalPodMonitors: []\n    ## Name of the PodMonitor to create\n    ##\n    # - name: \"\"\n\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the pod endpoint name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for pods to which this PodMonitor applies\n    ##\n    # selector: {}\n    ## Example which selects all Pods to be monitored\n    ## with label \"monitoredby\" with values any of \"example-pod-1\" or \"example-pod-2\"\n    # matchExpressions:\n    #   - key: \"monitoredby\"\n    #     operator: In\n    #     values:\n    #       - example-pod-1\n    #       - example-pod-2\n\n    ## label selector for pods\n    ##\n    # matchLabels: {}\n\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\n    ##\n    # podTargetLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    # sampleLimit: 0\n\n    ## Namespaces from which pods are selected\n    ##\n    # namespaceSelector:\n    ## Match any namespace\n    ##\n    # any: false\n\n    ## Explicit list of namespace names to select\n    ##\n    # matchNames: []\n\n    ## Endpoints of the selected pods to be monitored\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint\n    ##\n  # podMetricsEndpoints: []\n\n## Configuration for thanosRuler\n## ref: https://thanos.io/tip/components/rule.md/\n##\nthanosRuler:\n\n  ## Deploy thanosRuler\n  ##\n  enabled: false\n\n  ## Annotations for ThanosRuler\n  ##\n  annotations: {}\n\n  ## Service account for ThanosRuler to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  ## Configure pod disruption budgets for ThanosRuler\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n    # - thanosruler.domain.com\n\n    ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for ThanosRuler Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanosruler-general-tls\n    #   hosts:\n    #   - thanosruler.example.com\n\n  # -- BETA: Configure the gateway routes for the chart here.\n  # More routes can be added by adding a dictionary key like the 'main' route.\n  # Be aware that this is an early beta of this feature,\n  # kube-prometheus-stack does not guarantee this works and is subject to change.\n  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\n  route:\n    main:\n      # -- Enables or disables the route\n      enabled: false\n\n      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n      apiVersion: gateway.networking.k8s.io/v1\n      # -- Set the route kind\n      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n      kind: HTTPRoute\n\n      annotations: {}\n      labels: {}\n\n      hostnames: []\n      # - my-filter.example.com\n      parentRefs: []\n      # - name: acme-gw\n\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n\n      ## Filters define the filters that are applied to requests that match this rule.\n      filters: []\n\n      ## Additional custom rules that can be added to the route\n      additionalRules: []\n\n  ## Configuration for ThanosRuler service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for ThanosRuler Service to listen on\n    ##\n    port: 10902\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 10902\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30905\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for ThanosRuler service\n    additionalPorts: []\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a ServiceMonitor for the ThanosRuler service\n  ##\n  serviceMonitor:\n    ## If true, create a serviceMonitor for thanosRuler\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting thanosRulerpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec\n  ##\n  thanosRulerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.\n    ##\n    podMetadata: {}\n\n    ## Image of ThanosRuler\n    ##\n    image:\n      registry: quay.io\n      repository: thanos/thanos\n      tag: v0.37.0\n      sha: \"\"\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for ThanosRuler to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 24h\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## Storage is the definition of how storage will be used by the ThanosRuler instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n    ## AlertmanagerConfig define configuration for connecting to alertmanager.\n    ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.\n    alertmanagersConfig:\n      # use existing secret, if configured, alertmanagersConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n      # key: \"\"\n      # will render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set\n      # https://thanos.io/tip/components/rule.md/#alertmanager\n      secret: {}\n        # alertmanagers:\n        # - api_version: v2\n        #   http_config:\n        #     basic_auth:\n        #       username: some_user\n        #       password: some_pass\n        #   static_configs:\n        #     - alertmanager.thanos.io\n        #   scheme: http\n      #   timeout: 10s\n\n    ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.\n    ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.\n    # alertmanagersUrl:\n\n    ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false\n    ##\n    externalPrefix:\n\n    ## If true, http://{{ template \"kube-prometheus-stack.thanosRuler.name\" . }}.{{ template \"kube-prometheus-stack.namespace\" . }}:{{ .Values.thanosRuler.service.port }}\n    ## will be used as value for externalPrefix\n    externalPrefixNilUsesHelmValues: true\n\n    ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## ObjectStorageConfig configures object storage in Thanos\n    objectStorageConfig:\n      # use existing secret, if configured, objectStorageConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n      # key: \"\"\n      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set\n      # https://thanos.io/tip/thanos/storage.md/#s3\n      secret: {}\n        # type: S3\n        # config:\n        #   bucket: \"\"\n        #   endpoint: \"\"\n        #   region: \"\"\n        #   access_key: \"\"\n      #   secret_key: \"\"\n\n    ## Labels by name to drop before sending to alertmanager\n    ## Maps to the --alert.label-drop flag of thanos ruler.\n    alertDropLabels: []\n\n    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.\n    ## Maps to the --query flag of thanos ruler.\n    queryEndpoints: []\n\n    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.\n    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.\n    queryConfig:\n      # use existing secret, if configured, queryConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n      # key: \"\"\n      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set\n      # https://thanos.io/tip/components/rule.md/#query-api\n      secret: {}\n        # - http_config:\n        #     basic_auth:\n        #       username: some_user\n        #       password: some_pass\n        #   static_configs:\n        #     - URL\n        #   scheme: http\n      #   timeout: 10s\n\n    ## Labels configure the external label pairs to ThanosRuler. A default replica\n    ## label `thanos_ruler_replica` will be always added as a label with the value\n    ## of the pod's name and it will be dropped in the alerts.\n    labels: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Allows setting additional arguments for the ThanosRuler container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler\n    ##\n    additionalArgs: []\n      # - name: remote-write.config\n      #   value: |-\n      #     \"remote_write\":\n      #     - \"name\": \"receiver-0\"\n      #       \"remote_timeout\": \"30s\"\n    #       \"url\": \"http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"soft\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the thanosRuler instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: thanos-ruler\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the ThanosRuler UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.\n    ##\n    containers: []\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## PortName to use for ThanosRuler.\n    ##\n    portName: \"web\"\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerwebspec\n    web: {}\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating\n    additionalConfigString: \"\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.\n##\ncleanPrometheusOperatorObjectNames: false\n\n## Extra manifests to deploy as an array\nextraManifests: []\n  # - apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n#     extra-data: \"value\"\n\n"
            ],
            "verify": false,
            "version": "66.5.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "tempo",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "tempo",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "tempo",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.2.3",
                "chart": "tempo",
                "name": "tempo",
                "namespace": "monitoring",
                "revision": 1,
                "values": "{\"datasources\":{\"datasources.yaml\":{\"apiVersion\":1,\"datasources\":[{\"access\":\"proxy\",\"apiVersion\":1,\"basicAuth\":false,\"editable\":false,\"isDefault\":true,\"name\":\"Tempo\",\"orgId\":1,\"type\":\"tempo\",\"uid\":\"tempo\",\"url\":\"http://tempo:3100\",\"version\":1},{\"access\":\"proxy\",\"basicAuth\":false,\"isDefault\":false,\"name\":\"Loki\",\"orgId\":1,\"type\":\"loki\",\"uid\":\"loki\",\"url\":\"http://loki:3200\"}]}},\"env\":{\"GF_AUTH_ANONYMOUS_ENABLED\":true,\"GF_AUTH_ANONYMOUS_ORG_ROLE\":\"Admin\",\"GF_AUTH_DISABLE_LOGIN_FORM\":true}}",
                "version": "1.6.2"
              }
            ],
            "name": "tempo",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "---\nenv:\n  GF_AUTH_ANONYMOUS_ENABLED: true\n  GF_AUTH_ANONYMOUS_ORG_ROLE: \"Admin\"\n  GF_AUTH_DISABLE_LOGIN_FORM: true\n\ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n\n    datasources:\n      - name: Tempo\n        type: tempo\n        access: proxy\n        orgId: 1\n        url: http://tempo:3100\n        basicAuth: false\n        isDefault: true\n        version: 1\n        editable: false\n        apiVersion: 1\n        uid: tempo\n      - name: Loki\n        type: loki\n        uid: loki\n        access: proxy\n        orgId: 1\n        url: http://loki:3200\n        basicAuth: false\n        isDefault: false\n"
            ],
            "verify": false,
            "version": "1.6.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    }
  ],
  "check_results": null
}
